---
title: "Dynamic Shrinkage in Bayesian Structural Time Series and Vector Autoregressive Models"
 
author: "Edoardo Marcelli"

documentclass: book

date: "10 March 2022"

output:
  pdf_document: 
   latex_engine: pdflatex
   extra_dependencies: ["float"]
   number_sections: true
   keep_tex: yes
   
always_allow_html: true   
header-includes:
- \usepackage{babel} 
- \usepackage{longtable}
- \usepackage{amsmath}
- \usepackage{amssymb}
- \usepackage{mathtools}
- \usepackage{graphicx}
- \usepackage{MnSymbol} %allows to insert the QED at the end of the proofs
- \usepackage{hyperref} %set the hyperlink to cite sections, equations...
- \urlstyle{same}
- \usepackage{epigraph}
- \setlength\epigraphrule{0pt}
- \usepackage[toc,page]{appendix}
- \usepackage{comment}
- \usepackage[inline]{enumitem}
- \usepackage{ntheorem}
- \usepackage{lipsum}
- \usepackage{xcolor}
- \theoremstyle{break}
- \newtheorem{thm}{Theorem}% theorem counter resets every \subsection
- \renewcommand{\thethm}{\arabic{thm}}
- \newtheorem{proposition}{Proposition}
- \newtheorem{assumption}{Assumption}
- \newtheorem{lemma}{Lemma}
- \newtheorem{definition}{Definition}
- \theoremstyle{nonumberplain} 
- \newtheorem{proof*}{Proof.}
- \renewcommand{\P}{\mathbb{P}}
- \renewcommand{\baselinestretch}{1.5}
- \newcommand{\E}{\mathbb E}
- \newcommand{\V}{\mathbb V}
- \newcommand{\Q}{\mathbb Q}
- \newcommand{\M}{\mathbb M}
- \newcommand{\R}{\mathbb R}
- \newcommand{\B}{\mathcal B}
- \newcommand{\X}{\mathcal X}
- \newcommand{\Y}{\mathcal Y}
- \newcommand{\Pd}{\mathbb P}
- \newcommand{\N}{\mathcal N}
- \newcommand{\code}[1]{\texttt{#1}}
- \usepackage{algpseudocode}
- \usepackage[ruled, vlined]{algorithm2e}
- \usepackage{float}
- \floatplacement{figure}{}
- \newcounter{algoline}
- \newcommand\Numberline{\refstepcounter{algoline}\nlset{\thealgoline}}
- \AtBeginEnvironment{algorithm}{\setcounter{algoline}{0}}
- \newcommand{\bb}[1]{\boldsymbol{#1}}
- \newcommand{\iid}{\overset{iid}{\sim}}
- \newcommand{\ind}{\overset{ind}{\sim}}
- \usepackage{tikz}
- \usetikzlibrary{decorations.pathreplacing,positioning}
- \usepackage{tabularx}
- \AtBeginDocument{\let\maketitle\relax}
- \usepackage[font={small}]{caption}

bibliography: references.bib
link-citations: yes

nocite: '@*'

fontsize: 12pt

geometry: "left=2.5cm,right=2.5cm,top=3cm,bottom=3cm"

---

\setlength{\abovedisplayskip}{5pt}
\setlength{\belowdisplayskip}{5pt}
\allowdisplaybreaks

\null
    \thispagestyle{empty}
    \addtocounter{page}{-1}
    \newpage

\null
    \thispagestyle{empty}
    \addtocounter{page}{-1}
    \newpage

\par\vspace*{.35\textheight}{\centering \emph{A mia madre Sabrina e mio padre Giuliano}\par}

\null
    \thispagestyle{empty}
    \addtocounter{page}{-1}
    \newpage

\null
    \thispagestyle{empty}
    \addtocounter{page}{-1}
    \newpage


 \frontmatter
  \tableofcontents
  \listofalgorithms
  
```{r message=FALSE, warning=FALSE, include=FALSE}
library(tidyverse)
library(ggplot2)
library(ggpubr)
library(DiagrammeR)
library(kableExtra)
```
  

  \chapter{Introduction}
  
Central banks, statistical institutes, intergovernmental organizations, financial markets and online sources produce large amounts of data everyday. Such immense availability of data has offered many opportunities for macroeconometric modelling, but it has also posed new challenges. The passage from classical econometrics to big data econometrics has been characterized by two phenomena: the inclusion of variable selection strategies inside models and a preference for Bayesian statistics. An emblematic attempt in this direction is the model developed by @scott_varian_2013 which merges Bayesian Structural Time Series (BSTS) with Spike-and-Slab regression. The authors, economists at Google Inc, realized immediately the importance of big data in economic time series analysis and forecasting and they built this flexible model able to accommodate structural time series components, such as trend and seasonality, together with a large set of predictors. However, their model presents two limitations: static regression coefficients and constant variance. Such assumptions are unduly restrictive in time series analysis since they do not allow to fully explore the dynamics of the system under investigation. A wide literature indeed points out that the relationships among economic and financial variables are likely to change over time and therefore they are better described using regression models with time-varying parameters. Nevertheless, while models with stochastic variation in the parameters can be even more prone to overfitting, classical variable selection strategies are designed to induce shrinkage and sparsity in a static framework and not a dynamic one. For this reason, dynamic shrinkage has become an hot-topic in time series analysis and an increasing number of literature is addressing this issue by developing new strategies which are able not only to identify which predictors are truly relevant for the matter in question, but also in which moment in time. Pioneers in this new literature are @rockova_mcalinn_2021, who developed the Dynamic Spike-and-Slab Process Priors that captured our attention for both flexibility and performances. \
In this thesis we present this approach with two major extensions: we improve estimates with a stochastic volatility model for the residual variance and we replace the Forward Filtering Backward Sampling (FFBS) strategy with the precision sampler of @chan_jeliazkov_2009 to boost the computational efficiency. Moreover, we propose a novel Bayesian Structural Time Series model with dynamic shrinkage and stochastic volatility that can be regarded as the natural evolution of BSTS model aforementioned. The proposed model is designed to perform time series analysis and forecasting especially in the field of macroeconomics, however it may find applications also in other research areas. The structural components along with time-varying regression coefficients and the stochastic volatility process allow this model to capture several features which are common in macroeconomic time series such as trend, seasonality, structural breaks and change points. This new class of models will be discussed in details in Chapter \ref{Dynamic Shrinkage in Bayesian Structural Time Series}, after a preliminary review of Bayesian variable selection methods and BSTS (Chapter \ref{Bayesian Variable Selection Methods in Time Series Regression Models}) and a comprehensive analysis of Dynamic Spike-and-Slab Process Priors (Chapter \ref{Dynamic Shrinkage}).\
Then, driven by the enthusiasm of the results obtained with dynamic shrinkage priors, we extended the dynamic variable selection approach to the field of multivariate time series and, in particular, in Vector Autoregressive (VAR) models. Indeed, multivariate time series models suffer even more harshly of the the curse of dimensionality. For this reason, attempts have been made to introduce variable selection strategies in these models. Nevertheless, inducing dynamic shrinkage in Time-Varying VAR models is still a open problem because of the intrinsic complexity of the task.
In Chapter \ref{Dynamic Shrinkage in Multivariate Time Series Models} we provide a possible way to address this problem by exploring the potentiality of Dynamic Spike-and-Slab Process Priors in Time-Varying VAR models.\
Every algorithm described in this thesis has been implement in R. The replication code is publicly available on Github at the personal page of the author \footnote{url: https://github.com/edoardo-marcelli}. The code is organized as a preliminary version of an \code{R}-package (to be possibly submitted to CRAN). All the details about the latter are provided in Chapter \ref{The dynamicshrink package} and in Appendix.

  
  \section{Notation}
  
Before starting with the heart of this thesis, let me introduce the basic notation that will be employed henceforth. A time series will be denoted as $(Y_t)_{t\geq 1}$ or simply $(Y_t)$ where the subscript $t$ denotes the time of observation; capital letter are used for denoting random variables, while lower case letters denote the realizations. Bold upper case letters or upper case Greek letters identify matrices, while bold lower case letters identify vectors. Note, however, that finite sequences will be also denoted with the colon notation, e.g. $y_{1:T} = (y_1 ,..., y_T)$. This notation allows indeed to underline the temporal nature of some objects, which implies that the elements have to be considered in a precise chronological order. In the same manner, when dealing with time-varying matrices, the subscript will be explicated, for example $\bb{y}_{1:T}=(\bb{y}_{1},...,\bb{y}_{T})'$ indicates a $T \times n$ matrix. However, for some very specific cases, the notation might be simplified, for instance we use $\bb{X}$ to define a $T\times p$ matrix of explanatory variables in regression models:
\[ \boldsymbol X = \begin{pmatrix}
  x_{1,1} & x_{1,2} & ... & x_{1,p} \\
  x_{2,1} & x_{2,2} & ... & x_{2,p} \\
  \vdots  & \vdots  & \ddots & \vdots \\
  x_{T,1} & x_{T,2} & ... & x_{T,p}
  \end{pmatrix}\]
Finally, bold upper case Greek letters indicates large matrices, such as block diagonal matrices.  
Objects that do not respect this notation will be accurately introduced in order to make the reading as smoothly as possible.  
  
  

  \mainmatter

 
  \chapter[Bayesian Variable Selection Methods]{Bayesian Variable Selection Methods in Time Series Regression Models}\label{Bayesian Variable Selection Methods in Time Series Regression Models}
  
  \epigraph{\emph{``frustra fit per plura quod potest fieri per pauciora''}}{--- Novacula Occami}
  
  
The emergence of big data in the contemporary era has meant the development of new tools and methodologies to handle and menage efficiently such great sources of information. The aim of variable selection is to retain inside the model only those predictors that significantly contribute at explaining a given phenomenon to a certain degree, removing unwanted noise variables that would bias estimation or lead to overfitting in prediction. Therefore, by selecting only the relevant features we expect not only an higher model's explanatory power but also better out-of-sample predictions. However, such methodologies are not intended to be a substitute to experts' knowledge and they are able to express their potentiality only if combined with a robust theory.\
In this chapter, we decided to focus on the problem of variable selection in linear regression models adopting a Bayesian perspective. This issue however was raised for the first time in the frequentist literature because of an intrinsic weakness of traditional (frequentist) estimators. Consider the classical linear model
\begin{equation*}\label{eq:regmod}
\bb{Y} = \bb{X} \bb{\beta} + \bb{\epsilon},\quad  \bb{\epsilon} \sim \mathcal{N}(\bb{0},\sigma^{2}\bb{I}_{n}).
\end{equation*}
The least squared estimator, $\bb{b}=(\bb{X}'\bb{X})^{-1}\bb{X}'\bb{y}$, requires to invert the matrix $\bb{X}'\bb{X}$, which becomes unattainable when the number of predictors exceed the number of available observations. To overcome this fragility, in the frequentist framework, some techniques have been developed to control for high dimensionality. Leaving aside stepwise procedures that are intrinsically weak and clearly unfeasible when the number of predictors is large, these techniques are mainly based on the penalization of the likelihood and the resolution of optimization problems. However, model selection techniques merely based on likelihood penalization (AIC, BIC etc.) require to compare $2^{p}$ models and they do not provide coefficient's estimates, whereas frequentist shrinking estimators such as the Least Absolute Shrinkage and Selection Operator (LASSO) provides shrunk coefficients' estimates, however the optimization step on which they rely might not be that trivial.\
On the other hand, the Bayesian approach offers appealing methods for variable selection which are straightforward in principle. In principle, the researcher formalizes her information on each model by assigning them a probability law and then a prior distribution on the model-specific parameters; both the information on the parameters and on the models are updated through the Bayes rule as information from the data becomes available. This process is however not without difficulties; the main difficulties arise when it comes to: (i) specify prior distributions for the models' parameters, (ii) specify prior distributions on the models' space and (iii) compute the posterior distributions. For these purposes, classes of informative and uninformative priors provide a wide range of potential solutions for (i) and (ii), whereas recent developments in computational statistics offer powerful tools for (iii). In fact, Bayesian variable selection techniques rely on stochastic simulations rather than optimization problems. We should remark that the Bayesian approach to variable selection is also regarded as a generalization of the frequentist approach rather than an alternative to the latter. Indeed, the prior distribution provides a penalization of the likelihood, which makes Bayesian methods relevant also from a frequentist standpoint. Some of these methods will be reviewed in Section \ref{Hierarchical Models for Variable Selection} of this chapter, after a brief introduction to Bayesian inference in linear regression models (Section \ref{Bayesian Inference in Linear Regression}). Then, all the considerations made for the standard linear regression model will be translated in the framework of State-State Models (SSM).\
SSM are a class of stochastic models that describe the probabilistic dependence between a latent state process and the observable measurements. Originated in the early sixties in the field of control engineering, they have gained popularity in time series analysis thanks to their flexibility. They allow to model both univariate and multivariate time series, also in presence of non-stationarity, structural breaks and irregular patterns.\
Let $(\bb{Y}_{t})_{t \geq 1}$ and $(\bb{\theta}_{t})_{t \geq 0}$ be discrete time processes, where $\bb{Y}_{t}$ and $\bb{\theta}_{t}$ are respectively $n\times 1$ and $p \times 1$ vectors, with $\bb{Y}_t$ and $\bb{\theta}_t$ taking values respectively in spaces $\mathcal{Y}$ and $\Theta$ (these spaces can be multi-dimensional Euclidean spaces, discrete spaces or also less standard), and let $\bb{\psi}$ be a vector of unknown model's parameters, characterized by a prior distribution $\pi(\bb{\psi})$. Then the process $\big{(}(\bb{Y}_{t},\bb{\theta}_{t})\big)_{t\geq 1}$ starting at $\bb{\theta}_{0} \sim \pi_{0}(\cdot)$ is a SSM if 
\begin{itemize}
\item[(A.1)]\ $(\bb{\theta}_{t})|\bb\psi$ is a Markov Chain
\item[(A.2)] \ Conditionally on $(\bb{\theta}_{t})$ and $\bb{\psi}$, the $\bb{Y}_{t}$'s are independent and $\bb{Y}_{t}$ only depends on $\bb{\theta}_{t}$ and $\bb{\psi}$
\end{itemize}
Therefore, for any $t$, the joint distribution of $(\bb{\theta}_{0:t},\bb{y}_{1:t},\bb{\psi})$ is given by
\begin{equation*}
\pi(\bb{\theta}_{0:t},\bb{y}_{1:t},\bb{\psi})=\pi_{0}(\bb{\theta}_{0}|\bb{\psi})\pi(\bb{\psi})\prod_{j=1}^{t}\pi_{j}(\bb{y}_{j}|\bb{\theta}_{j},\bb{\psi})\pi_{j}(\bb{\theta}_{j}|\bb{\theta}_{j-1},\bb{\psi})
\end{equation*}
and it is thus fully specified by the initial distribution $\pi_{0}(\bb{\theta}_0\mid\bb{\psi})$, the parameters' distribution $\pi(\bb{\psi})$, the transition distribution $\pi_{t}(\bb{\theta}_{t}|\bb{\theta}_{t-1},\bb\psi)$ describing the evolution of the latent process, and the emission distribution $\pi_{t}(\bb{y}_{t}|\bb{\theta}_{t},\bb\psi)$ illustrating how data are generated. \
A linear regression model can be thus regarded as a special case of SSM where the latent process is assumed to be static and the observations $\bb{y}_{t}$ are linearly related to the state $\bb{\theta}_{t}$. An in-depth discussion on the ways in which regressors are handled in SSM follows in Section \ref{Bayesian Structural Time Series}, where a recent class of models combining SSM and Bayesian variable selection methods is introduced.
  
  \section{Bayesian Inference in Linear Regression} \label{Bayesian Inference in Linear Regression}
  
  Regression models are meant to describe the relationship between a random variable $Y$ and a set of explanatory variables $\boldsymbol x=(x_{1},...,x_{p})$. The classic linear regression model is
\begin{equation}\label{eq:mod1}
Y_{1:T}'=\boldsymbol{X}\boldsymbol{\beta}+\bb{\epsilon}, \ \ \ \bb{\epsilon}\mid\Sigma\sim\mathcal{N}(\bb{0},\Sigma)
\end{equation}
This formulation provides a full specification of the joint probability of the vector $Y_{1:T}=(Y_{1},...,Y_{T})$ given the matrix regressors $\boldsymbol X = (\boldsymbol x_{1}',...,\boldsymbol x_{T}')'$ along with the model's parameters $\boldsymbol \beta$ and $\Sigma$. The covariance matrix $\Sigma$ can be specified using any symmetric positive-definite matrix, however considering $\Sigma=\sigma^{2}\boldsymbol{I}_{T}$ preserves the i.i.d errors property.
In a Bayesian perspective, the unknown parameters are random quantities and model (\ref{eq:mod1}) expresses the conditional distribution $Y_{1:T}' \mid (\bb{\beta}, \Sigma) \sim \mathcal{N}(\bb{X}\bb{\beta}, \Sigma)$. The model is thus completed by assigning a prior distribution on $(\bb{\beta}, \Sigma)$, and inference is solved by computing their posterior distribution given the data. However, the posterior distribution can be analytically involved; and a useful way to proceed is by using conjugate priors. Considering both $(\boldsymbol \beta,\Sigma)$ unknown, the conjugate model is a Normal-Gamma with parameters $(\beta_{0},\Omega_0,n_{0},d_{0})$, which means assuming
\begin{align*}
\boldsymbol{\beta}\mid\sigma^2 \sim & \mathcal{N}(\boldsymbol{\beta}_0,\sigma^{2}\Lambda_{0})\\
\frac{1}{\sigma^{2}}\sim & \mathcal{G}(n_{0},d_{0})
\end{align*}
where $\mathcal{G}(\alpha,\beta)$ is a Gamma distribution with mean $\alpha/\beta$ and variance $\alpha/\beta^{2}$. Therefore, the posterior of $(\bb{\beta},\Sigma)$ given $y_{1:T}$ is still a Normal-Gamma with updated parameters $(\tilde{\bb{\beta}},\Lambda,n_{T},d_{T})$ where
\begin{align*}
\tilde{\bb{\beta}}= & \bb{\beta}_{0}+\Lambda_{0}\bb{X}'(\bb{X}\Lambda_{0}\bb{X}'+\bb{I})^{-1}(y_{1:T}'-\bb{X}\bb{\beta}_{0}),\\
\Lambda = & \Lambda_{0}-\Lambda_{0}\bb{X}'(\bb{X}\Lambda_{0}\bb{X}'+\bb{I})^{-1}\bb{X}\Lambda_{0},\\
n_{T}= & n_{0}+\frac{T}{2},\\
d_{T}= & d_{0}+\frac{1}{2}(\bb{\beta}_{0}\Lambda_{0}^{-1}\bb{\beta}_{0}+y_{1:T}y_{1:T}'-\tilde{\bb{\beta}}'\Lambda\tilde{\bb{\beta}})
\end{align*}
Another useful result that is worth to be mentioned since it will be resumed multiple time in this thesis concerns the updating rule of a non-conjugate model provided by theorem (8.1) of @KC_2014. They consider the following prior densities
\begin{align*}
\boldsymbol{\beta} \sim & \mathcal{N}(\boldsymbol{\beta}_{0},\Lambda_{0})\\
\frac{1}{\sigma^{2}}\sim & \mathcal{G}(n_{0},d_{0})
\end{align*}
and
\begin{align*}
\boldsymbol{\beta} | \sigma^{2},y_{1:T} \sim & \mathcal{N}(\tilde{\bb{\beta}},\Lambda)\\
\frac{1}{\sigma^{2}}|\boldsymbol{\beta},y_{1:T} \sim & \mathcal{G}\bigg(n_{0}+\frac{T}{2},d_{0}+\frac{(y_{1:T}'-\boldsymbol{X}\boldsymbol{\beta})'(y_{1:T}'-\boldsymbol{X}\boldsymbol{\beta})}{2}\bigg)
\end{align*}
where $\tilde{\bb{\beta}}=\Lambda(\bb{X}'y_{1:T}'/\sigma^{2}+\Lambda_{0}^{-1}\bb{\beta_{0}})$ and $\Lambda=(\bb{X}'\bb{X}/\sigma^{2}+\Lambda^{-1}_{0})^{-1}$. This result is important since it provides the foundation of the precision sampler of @chan_jeliazkov_2009. The latter is a fast posterior sampling strategy for Dynamic Linear Models that can be used in alternative to the FFBS. The precision sampler is illustrated in details in Section \ref{Sparse Matrix}.\
So far, all the regression coefficients were treated as influential, however one could reasonably questions the relevance of some predictors. This belief can be taken into account in the Bayesian approach through a different specification of the density priors as illustrated in the next paragraphs.

  \section{Hierarchical Models for Variable Selection} \label{Hierarchical Models for Variable Selection}
  
Bayesian variable selection methods are usually built on hierarchical mixture priors. With reference to the linear regression framework described in the previous section, this translates in introducing auxiliary binary variables $\gamma_{j}$ taking values $\gamma_{j}=1$ if
the $j$-th coefficient is significantly different from zero and $\gamma_{j}=0$ otherwise. Therefore, the vector $\bb{\gamma}=(\gamma_1,...,\gamma_p)'$ indicates which covariates enter the regression model. For example, $\bb{\gamma}=(0,1,0,...,0)'$ identifies a model including only $x_{2}$. We denote with $\bb{X}_{\gamma}\bb{\beta}_{\gamma}$ the model associated with a generic $\bb\gamma$. The specification of such hierarchical model is completed by setting the priors $\pi(\bb{\gamma})$, $\pi(\bb{\beta}_{\gamma}|\bb{\gamma})$ and by specifying a likelihood $\pi(y_{1:T}|\bb{\beta}_{\gamma},\bb{\gamma})$. Every strategy here presented starts from this setup and considers different prior specifications and posterior inference strategies. For brevity, we decide to focus on only one technique for each class of methods. For a more complete overview, we refer to @OS_2009, @R_2013 and @fan2010reversible.
  
  \subsection{Spike-and-Slab Priors}\label{Indicator Model Selection}
  
A popular approach in Bayesian analysis to generate sparsity consists in using mixture density priors of the class \emph{Spike-and-Slab}. The name attached to these priors derives from their peculiarity of combining a diffuse density for $\beta_{j}|\gamma_{j}=1$ and a concentrated distribution with mean zero on $\beta_{j}|\gamma_{j}=0$. In the original formulation of @MB_1988, the idea was to assign to every coefficients a mixture prior with a Dirac measure on zero and a uniform density elsewhere; while in more recent versions the point mass prior is replaced by a continuous distribution concentrated on zero since it facilitates computations. The Stochastic Search Variable Selection (SSVS) by @GM_1993 belong to this latter class. This strategy will be relevant in the developments of the thesis, therefore we analyse it in details. The approach uses a scale mixture of two normal distributions for the model's coefficients. Globally, the hierarchical setup is the following
\begin{align}
Y_{t}|\bb{x}_{t},\bb{\beta},\sigma^{2} \overset{ind}{\sim} & \mathcal{N}(\bb{x}_{t}'\bb{\beta},\sigma^{2}), \quad t=1,...,T, \nonumber \\
\beta_{j}|\gamma_{j}\sim &  (1-\gamma_{j})\mathcal{N}(0,\lambda_{j})+\gamma_{j}\mathcal{N}(0,c_{j}^{2}\lambda_{j}), \ \ \ j=1,...,p,\label{eq:eq2ssvs} \\ 
\sigma^{2}|\bb{\gamma}\sim &  \mathcal{IG}\bigg(\frac{n_{\gamma}}{2},\frac{d_{\gamma}}{2}\bigg), \label{eq:ssvs2.3}    \\
P(\gamma_{j}=1)= & 1-P(\gamma_{j}=0)=\omega_{j} \label{eq:ssvs3}
\end{align}
where $\mathcal{IG}(a,b)$ is an Inverse-Gamma distribution with mean $\frac{b}{a-1}$ and variance $\frac{b^{2}}{(a-1)^{2}(a-2)}$. Thus, by setting a (positive) small value for $\lambda_{j}$ when $\gamma_{j}=0$ then $\beta_{j}$ would be shrunk to zero. On the other hand, fixing $c_{j}$ large (and at least greater than $1$) when $\gamma_{j}=1$ then the prior gives high probability to $\beta_{j}$ being substantially far from zero. According to this interpretation, $\omega_{j}$ can be regarded as the prior probability of $x_{j}$ being included in the model.
```{r myfig1, echo=FALSE, fig.align='center', fig.cap="Spike (solid line) and Slab (dashed line) distributions with hyperparameters: $\\lambda=0.1$ and $c = 5, 10, 50$.", fig.pos='h', fig.width=10, message=FALSE, warning=FALSE, fig.height=3, results=F}

ssplot<-function(c,lambda){
  require(ggplot2)
  require(ggpubr)
vec<-seq(-2.5,2.5, by=0.001)
spike<-dnorm(vec,0,lambda)
slab<-dnorm(vec,0,c*lambda)
data.spike.slab<-data.frame(vec,spike,slab)

ggplot(data.spike.slab,aes(x=vec))+
  geom_line(aes(y=spike),linetype="solid")+
  geom_line(aes(y=slab),linetype="dashed")+
  labs(x="x",y="")+
  theme_bw()
  
}

plot.one<-ssplot(5,0.1)
plot.two<-ssplot(10,0.1)
plot.three<-ssplot(50,0.1)
#plot.four<-ssplot(100,0.1)

ggarrange(plot.one,plot.two,plot.three,nrow=1)


```
Equation (\ref{eq:eq2ssvs}) gives the conditional distribution of $\beta_j$;  the joint conditional distribution of $\beta_j$ can be rewritten in a general multivariate form as 
\begin{equation*} \label{eq:ssvsmulti}
\bb{\beta}|\bb{\gamma}\sim\mathcal{N}(\bb{0},\bb{D}_{\gamma}\bb{R}_{\gamma},\bb{D}_{\gamma})
\end{equation*}
where $\bb{D}_{\gamma} \equiv diag(\alpha_{1}\lambda_{1},...,\alpha_{p}\lambda_{p})$ with $\alpha_{j}=1$ if $\gamma_{j}=0$ and $\alpha_{j}=c_{j}$ if $\gamma_{j}=1$, and $\bb{R}_{\gamma}$ is prior correlation matrix when the model is characterized by gamma. A convenient specification for the latter is $\bb{R}_{\gamma}=\bb{I}$ if the $\beta_j$ can be considered independent or the @zellner_1986 \emph{g-prior} that assumes $\bb{R}_{\gamma}^{-1}=g(\bb{X}'\bb{X})/n$.
Regarding the priors on the residual variance $\sigma^{2}$ and on $\bb{\gamma}$, it is reasonable to assume that $\sigma^2$ would decrease when the number of active coefficient increases. Thus, the parameters of the Inverse-Gamma distribution in equation (\ref{eq:ssvs2.3}) can be set in function of the expected dimensionality.
For instance, interpreting $n_\gamma$ as the prior sample size and $d_\gamma/(n_\gamma-1)$ as the prior estimate of $\sigma^2$. One may let $d_\gamma/(n_\gamma-1)$ be a decreasing function of $|\bb{\gamma}|$. Finally, @GM_1993 recognize the difficulty in choosing an informative prior for $\bb{\gamma}$ especially when $p$ is large, thus they suggest the following specification which assumes independent indicators marginally distributed as in equation (\ref{eq:ssvs3}),
\begin{equation*}
\pi(\bb{\gamma})=\prod_{j=1}^{p}\omega_{j}^{\gamma_{j}}(1-\omega_{j})^{1-\gamma_{j}}
\end{equation*}
This prior basically implies that the inclusion probability of each regressor is independent of the inclusion of the others. Despite its simplicity, this prior choice is very effective since it facilitates Gibbs sampling; @GM_1993 found it to work well in various situations. Note also that, considering an equal probability of each regressor to enter the model and thus setting $\omega_{j}=\frac{1}{2}$, the prior becomes a uniform $\pi(\bb{\gamma})\equiv 2^{-p}$. \
Obviously, the priors must be chosen with care, and they must strike a balance between reliability and functionality. The prior distributions in SSVS are precisely configured to produce closed form full conditional distributions, allowing for rapid and efficient simulations using Gibbs sampling, for example.\
Alternatively to equation (\ref{eq:ssvsmulti}), @GM_1997 propose a conjugate design for $(\bb{\beta},\sigma^{2})$, i.e. $\bb{\beta}|\sigma^{2},\bb{\gamma}\sim\mathcal{N}(\bb{0},\sigma^{2}\bb{D}_{\gamma}\bb{R}_{\gamma},\bb{D}_{\gamma})$. This formulation simplifies analytical calculations since $(\bb{\beta},\sigma^{2})$ can be integrated out from the full posterior $\pi(\bb{\beta},\sigma^{2},\bb{\gamma}|y_{1:T})$ and thus it enables to easily obtain the posterior $\pi(\bb\gamma|y_{1:T})$, which is known up to a proportionality constant, and it can be carried analytically for small $p$ or numerically otherwise with MCMC methods. @GM_1997 show that this conjugate design facilitates MCMC exploration.

  \subsection{Bayesian Regularization}

Rather than using auxiliary variables to generate spike and slab densities, shrinking towards zero can alternatively be achieved by directly assigning a continuous prior density to the model's parameters. Such priors are usually exponential density priors of the form $\pi(\beta_{j}|\lambda_{j})\propto\exp(-\lambda_{j}|\beta_{j}|^{i})$, with $i>0$ and where $\lambda_{j}$ is a function of the scale parameter. For example, @T_1996 uses a double-exponential (Laplace) prior with $i=1$ and assumes that the regression coefficients are i.i.d. distributed according to 
\[\pi(\beta_{j}|\lambda)=\frac{\lambda}{2}\exp(-\lambda|\beta_{j}|)\]
for $j=1,...,p$. Assuming a Laplace rather than a Normal distribution has the effect of increasing the probability mass around zero, resulting in a more severe shrinking towards zero. @T_1996 reconciles this Bayesian approach to variable selection with the frequentist LASSO by showing that the latter can be regarded as the mode of the posterior distribution of $\beta$. However, while in the frequentist LASSO the regression coefficients are obtained by solving a constrained optimization problem, in the Bayesian version we rely on 
simulation algorithms, primarily MCMC, approximating the posterior distribution which is usually analytically intractable. This implies that contrary to the frequentist LASSO that produces sparse models by forcing coefficients to zero, in the Bayesian LASSO the point estimates are never exactly equal to zero.\
The approach of @T_1996 was interestingly developed into the Bayesian LASSO by @PC_2008. In this case the regression coefficients are modeled with a conditional Laplace prior distribution of the type 
\[\pi(\bb{\beta}|\sigma^{2})=\prod_{j=1}^{p}\frac{\lambda}{2\sqrt{\sigma^2}}\exp\bigg(-\frac{\lambda|\beta_{j}|}{\sqrt{\sigma^2}}\bigg)\]
with an improper density $\pi(\sigma^{2})\propto \frac{1}{\sigma^{2}}$ on the variance or an Inverse-Gamma prior that maintains the conjugacy. The effect of conditioning on $\sigma^2$ has significant consequences in terms of convergence of the MCMC strategy and more meaningful point estimates since it guarantees an unimodal full posterior density. Interestingly [@AM_1974], a scale mixture of Normal densities with an exponential mixing density can be used to represent the Laplace distribution:
\[
\frac{\lambda}{2}\exp(-\lambda|\beta|)=\int_0^\infty \frac{1}{\sqrt{2\pi s}}\exp\bigg(-\frac{\beta^2}{2s}\bigg)\frac{\lambda}{2}\exp\bigg(-\frac{\lambda^2\beta}{2}\bigg)ds
\]
Therefore, the full model can be represented with this hierarchical scheme
\begin{align*}
\bb \beta|\sigma^{2},\tau^{2}_{1},...,\tau^{2}_{p}\sim & \mathcal{N}(\bb{0},\sigma^{2}\bb{D}_{\tau}), \\
\bb{D}_{\tau}= & diag(\tau_{1}^{2},...,\tau_{p}^{2}),\\
\sigma^{2},\tau^{2}_{1},...,\tau^{2}_{p}\sim &  \pi(\sigma^{2})\prod_{j=1}^{p}\frac{\lambda^{2}}{2}\exp\bigg(-\lambda^{2}\tau^{2}_{j}/2\bigg),\\
\sigma^{2},\tau^{2}_{1},...,\tau^{2}_{p}> & 0
\end{align*}
which can be simplified by assuming $\tau^{2}_{1},...,\tau^{2}_{p}=\tau^{2}$. The Bayesian LASSO parameter, $\lambda$, can be estimated in several ways; @PC_2008 suggest empirical Bayes via marginal maximum likelihood or the use of hyperpriors. We refer to the original article for further details.

  \subsection{Reversible Jump MCMC}
The approaches mentioned so far presume that the collection of predictors is fixed and that individual coefficients are given shrinkage priors. Alternatively, one can wonder if the number of explanatory variables in the model is appropriate, or if it is too high or too low. The Bayesian way to model such uncertainty is straightforward and it consists of treating also $N_{p}$ as a random variable and assigning a prior to it. In this way the degree of sparseness depends on two factors: the coefficients shrunk towards zero and the dimension of the regression matrix. The computational challenges of this approach can be addressed by using reversible jump MCMC  algorithms (@G_1995) which allow to simultaneously  explore spaces of different dimensions. The idea is to generate a Markov Chain in order to simulate from $\pi(\bb{\beta}_{\gamma},\bb{\gamma}|y_{1:T},\bb{X})\propto\pi(y_{1:T}|\bb{X},\bb{\beta})\pi(\bb{\beta}|\bb{\gamma})\pi(\bb{\gamma})$. This is commonly achieved through a Metropolis-Hastings algorithm. The difficulty stands in the fact that in order to explore different spaces we need a strategy that allow the transition from state $(\bb{\beta}_{\gamma},\bb{\gamma})$ to the state $(\bb{\beta}_{\gamma}^{*},\bb{\gamma}^{*})$ where $N_{p} \neq N_{p^*}$. Such "jump" from one dimension to an other is possible through the concept of \emph{dimension matching}. The idea is the following. Let's say for instance that the jump occurs from $(\bb{\beta}_{\gamma},\bb{\gamma})$ to $(\bb{\beta}_{\gamma^{*}}^{*},\bb{\gamma}^{*})$ where $N_{p}<N_{p*}$. To make this jump possible a random vector $\bb{u}$ with density $h_{\gamma \to \gamma^*}(\bb{u})$ and length $(N_{p^*}-N_{p})$ is generated from a known density. The aim of this vector is to fill the dimensional gap between $\bb{\gamma}$ and $\bb{\gamma}^{*}$. In other words, considering a one-to-one mapping function $g_{\gamma \to \gamma^*}:\mathbb{R}^{N_{p}}\times\mathbb{R}^{N_{p^*}-N_{p}}\to\mathbb{R}^{N_{p^*}}$, the relationship between the current and the new state is given by $\bb{\beta}^{*}_{\gamma^{*}}=g_{\gamma \to \gamma^*}(\bb{\beta}_{\gamma},\bb{u})$.
Once we generate a random candidate state $(\bb{\beta}_{\gamma}^{*},\bb{\gamma}^{*})$, its acceptance probability must take into account also the change in dimensions, thus it is necessary to compute the Jacobian of $g$:
\begin{equation*}\label{eq:jumpMCMC}
\alpha((\bb{\beta}_{\gamma},\bb{\gamma}),(\bb{\beta}_{\gamma}^{*},\bb{\gamma}^{*}))=\min\bigg\{1,\frac{\pi(\bb{\beta}_{\gamma}^{*},\bb{\gamma}^{*}|y_{1:T},\bb{X})q(\bb\gamma^{*}\to\bb{\gamma})}{\pi(\bb{\beta}_{\gamma},\bb{\gamma}|y_{1:T},\bb{X})q(\bb{\gamma} \to\bb{\gamma}^{*})h_{\gamma\to\gamma^{*}}(\bb{u})}\bigg|\frac{\partial g_{\gamma\to\gamma^{*}}(\bb{\beta}_{\gamma},\bb{u})}{\partial (\bb{\beta}_{\gamma},\bb{u})}\bigg|\bigg\}
\end{equation*}

where $q(\bb\gamma^* \to \bb\gamma)$ is the the probability of proposing a transaction from $(\bb{\beta}_{\gamma},\bb{\gamma})$ to $(\bb{\beta}_{\gamma}^{*},\bb{\gamma}^{*})$. In the same way, the acceptance ratio of the reverse move proposal (from $\bb{\gamma}^* \to \bb{\gamma}$) is given by $\alpha((\bb{\beta}_{\gamma},\bb{\gamma}),(\bb{\beta}_{\gamma}^{*},\bb{\gamma}^{*}))$.
The mechanism just introduced, however, requires a wise evaluation of the functions $h$ and $g$, since their choice could affect the performances of the simulation strategy. \
Moreover, one may relax the assumptions on the size of $\bb{u}$. In this case, the dimensionality can still be matched by letting the length of $\bb{u}$ to be equal to $l_{p}$ such that $N_{p}+l_{p}=N_{p^*}+l_{p^*}$. However, now $\bb{u}^{*}$ cannot be generated by the inverse function of $h_{\gamma \to \gamma^*}$ since it is no more available in deterministic terms. Therefore, $\bb{u}^*$ will be generated from a new density $h_{\gamma^* \to \gamma}=(\bb{u}^*)$. Finally, in addition to the mapping $\bb{\beta}^{*}_{\gamma^{*}}=g_{\gamma \to \gamma^*}(\bb{\beta}_{\gamma},\bb{u})$ is added a reverse mapping $\bb{\beta}_{\gamma}=g_{\gamma^*\to\gamma}(\bb{\beta}^{*}_{\gamma^*},\bb{u}^*)$. The new acceptance probability is thus
\begin{equation*}\label{eq:jumpMCMC1}
\alpha((\bb{\beta}_{\gamma},\bb{\gamma}),(\bb{\beta}_{\gamma}^{*},\bb{\gamma}^{*}))=\min\bigg\{1,\frac{\pi(\bb{\beta}_{\gamma}^{*},\bb{\gamma}^{*}|y_{1:T},\bb{X})q(\bb\gamma^{*}\to\bb{\gamma})h_{\gamma^{*}\to\gamma}(\bb{u^*})}{\pi(\bb{\beta}_{\gamma},\bb{\gamma}|y_{1:T},\bb{X})q(\bb{\gamma} \to\bb{\gamma}^{*})h_{\gamma\to\gamma^{*}}(\bb{u})}\bigg|\frac{\partial g_{\gamma\to\gamma^{*}}(\bb{\beta}_{\gamma},\bb{u})}{\partial (\bb{\beta}_{\gamma},\bb{u})}\bigg|\bigg\}.
\end{equation*} 

In general, Reversible Jump MCMC have proved to perform well for variable selection and they allow for a rapid mixing time. However, we do not explore this approach further. See @G_1995 for furthers details and examples.
  
  \section{Bayesian Structural Time Series} \label{Bayesian Structural Time Series}
  
After @varian_choi_2009, the awareness of scientists about the effectiveness of web data (and in particular Google Trends) in improving forecasting increased. However, because of the high dimensionality, dealing with such great amount of data poses new modelling challenges. In order to meet the demand for an ensemble method able to average over different combinations of predictors in time series analysis, Steven L. Scott and Hal Varian, respectively Senior Economist Analyst and Chief Economist at Google Inc, developed a new methodology that combines State-Space Models and Bayesian Variable Selection Methods. The authors refer to this new class of models as \emph{Bayesian Structural Time Series} (BSTS). The latter is based on three pillars:
\begin{itemize}
\itemsep-0.5em 
  \item Structural Time Series Models
  \item Spike and Slab Regression
  \item Markov Chain Monte Carlo methods
\end{itemize}
We develop the discussion in the next three sections.

\subsection{Structural Time Series Models}\label{Structural Time Series Models}

Structural Time Series Models can be regarded as a subclass of Dynamic Linear Models (DLM) where the state process describes the evolution of some unobserved time series features, precisely the trend, the seasonal and, possibly, the cycle components. 
Intuitively, they remind of the classical time series decomposition, that describes a time series as composed by some systematic components (trend, seasonality, cycle) and a random  component (noise). However, while classical decomposition is just a descriptive tool,  Structural Time Series Models are probabilistic models, and allow prediction and a proper quantification of uncertainty - assuming that we are able to identify the components from the noisy background. Formulating the problem in the form of general multivariate DLM, the observable $\mathbb{R}^{n}$-valued time series $(\bb{Y}_{t})_{t\geq 1}$ is related to the latent $\mathbb{R}^{p}$-valued state process $(\bb{\theta}_{t})_{t\geq 0}$ through the following system of equation, for each $t\geq1$,
\begin{align} \label{eq:DLM1}
\bb{Y}_{t} = & \bb{F}_{t}\boldsymbol{\theta}_{t}+\bb{\epsilon}_{t}, \ \ \ \bb{\epsilon}_{t}\sim\mathcal{N}_{n}(\bb{0},\Sigma_{\epsilon,t}) \\
\boldsymbol{\theta}_{t} = & \bb{G}_{t}\boldsymbol{\theta}_{t-1}+\bb{\eta}_{t}, \ \ \ \bb{\eta}_{t}\sim\mathcal{N}_{p}(\bb{0},\Sigma_{\eta,t}) \label{eq:DLM2}
\end{align}
with $\bb{\theta}_{0}\sim\mathcal{N}_{p}(\bb{m}_{0},\bb{C}_{0})$. $\bb{F}_{t}$ and $\bb{G}_{t}$ are respectively a $n \times p$ and a $p \times p$. The disturbances, $(\bb{\epsilon}_{t})$ and $(\bb{\eta}_{t})$, are assumed to be two sequences of serially uncorrelated Gaussian random vectors with mean zero and $p \times p$ covariance matrices $\Sigma_{\epsilon,t}$ and $\Sigma_{\eta,t}$. Moreover,it is assumed that $(\bb{\epsilon}_{t}) \perp\!\!\!\perp (\bb{\eta}_{t})\perp\!\!\!\perp \bb{\theta}_{0}$. Equation (\ref{eq:DLM1}) s refereed to as the observation equation and equation (\ref{eq:DLM2}) as the state equation.
State-space models naturally arise in problems of fintering, where interest is in extracting the signal, $(\bb{\theta}_{t})_{t \geq 1}$, from an incomplete, potentially noisy, set of observations $(\bb{y}_{t})_{t\geq 1}$. Very briefly, this is possible through a recursive procedure that exploits the Markovian structure of the state process and the assumption of conditionally independent observations (see (A.1) and (A.2)) in order to compute the filter and predictive densities at any time $t$ starting from $\bb{\theta}_{0}\sim p_{0}$. The process of filtering can be summarized in three steps. Let $(\bb Y_{t},\bb \theta_{t})_{t \geq 1}$ be a discrete time stochastic process satisfying (A.1) and (A.2), and $\bb{\psi}$ a vector of model's parameters, for $t\geq 1$ compute
\begin{enumerate}[label=(\roman*)]
\item The one-step-ahead predictive distribution for the states using the filtering density at time $t-1$, $\pi(\bb \theta_{t-1}|\bb y_{1:t-1})$: \[\pi(\bb \theta_{t}| \bb y_{1:t-1})=\int\int \pi(\bb \theta_{t}|\bb \theta_{t-1},\bb{\psi}) \pi(\bb \theta_{t-1}|\bb y_{1:t-1},\bb{\psi})\pi(\bb{\psi}|\bb{y}_{1:t-1}) \ d\bb{\theta}_{t-1}\ d\bb{\psi} \]
\item The one-step-ahead predictive distribution for the observations using the density computed in step (i): \[\pi(\bb y_{t}|\bb y_{1:t-1})=\int\int \pi(\bb y_{t}|\bb \theta_{t},\bb{\psi}) \pi(\bb \theta_{t}|\bb y_{1:t-1},\bb{\psi})\pi(\bb{\psi}|\bb{y}_{1:t-1})  \ d\bb{\theta_{t}}\  d\bb{\psi}\]
\item The filtered distribution at time $t$ using the densities computed at step(i) and step (ii):  \[\pi(\bb \theta_{t}|\bb y_{1:t})=\frac{\pi(\bb y_{t}|\bb \theta_{t})\pi(\bb \theta_{t}|\bb y_{1:t-1})}{\pi(\bb y_{t}|\bb y_{1:t-1})}\]
\end{enumerate}
Such densities are available in closed form and they are easy to compute in linear Gaussian State-Space Models only when model's parameters are known. In this latter case, the joint distribution for $(\bb \theta_{0},\bb \theta_{1},...,\bb \theta_{T},\bb Y_{1},...,\bb Y_{T})$ is a multivariate Normal distribution, which implies that also the marginal and conditional distributions are normally distributed. Therefore they are characterized by the first two moments, which are obtained by the celebrated Kalman Filter which is based on the process described below. Let $\bb{\theta}_{0}\sim \mathcal{N}(\bb m_{0},\bb C_{0})$, compute for $t=1,...,T$
\begin{equation}\label{eq:Kalman}
\begin{matrix}
\bb{a}_{t}=\E(\bb \theta_{t}|\bb y_{1:t-1})=\bb{G}_{t}\bb{m}_{t-1}, & \bb{R}_{t}=\V(\bb \theta_{t}|\bb y_{1:t-1})=\bb{G}_{t}\bb{C}_{t-1}\bb{G}_{t}'+\Sigma_{\eta,t}, \\
\bb{f}_{t}=\E(\bb y_{t}|\bb y_{1:t-1})=\bb{F}_{t}\bb{a}_{t}, & \bb{Q}_{t}=\V(\bb y_{t}|\bb y_{1:t-1})=\bb{F}_{t}\bb{R}_{t}\bb{F}_{t}'+\Sigma_{\epsilon,t}, \\
\bb{m}_{t}=\E(\bb \theta_{t}|\bb y_{1:t})=\bb{a}_{t}+\bb{A}_{t}\bb{e}_{t}, & \bb{C}_{t}=\V(\bb \theta_{t}|\bb y_{1:t})=\bb{R}_{t}-\bb{A}_{t}\bb{Q}_{t}\bb{A}_{t}' \\
\end{matrix}
\end{equation}
where $\bb{A}_{t}=\bb{R}_{t}\bb{F}_{t}'\bb{Q}_{t}^{-1}$ and $\bb{e}_{t}=\bb{y}_{t}-\bb{f_t}$. The problem of smoothing, that is, the computation of the distribution of $\bb{\theta}_{0:T} \mid \bb{y}_{1:T}$ is similarly solved by Kalman smoothing. Given the filtering distribution at time $T$, we trace the states' history up T through backward recursive formulae. Let $\bb \theta_{t+1}|\bb y_{1:T}\sim\mathcal{N}(\bb s_{t+1},\bb S_{t+1})$, then $\bb \theta_{t}|\bb y_{1:T}\sim\mathcal{N}(\bb s_{t},\bb S_{t})$
\begin{equation}\label{eq:Smooth}
\begin{matrix}
\bb{s}_{t}={m}_{t}+\bb{B}_{t}(\bb{s}_{t+1}-\bb{a}_{t+1}), & \bb{S}_{t}=\bb{C}_{t}-\bb{B}_{t}(\bb{R}_{t+1}-\bb{S}_{t+1})\bb{B}_{t}'
\end{matrix}
\end{equation}
where $\bb{B}_{t}=\bb{C}_{t}\bb{G}_{t+1}'\bb{R}_{t+1}^{-1}$. Jointly, Kalman Filter (also known as \emph{forward step}) and Kalman Smoother (or \emph{backward step}) provide the posterior distribution of the states assuming that the matrices of the model are known. Unfortunately, the model's parameters are usually unknown. Nevertheless, the process described in (\ref{eq:Kalman}) and (\ref{eq:Smooth}) to obtain moments can be used in Markov Chain Monte Carlo methods to compute the full conditional distribution $p(\bb{\theta}_{t}|\bb{y}_{1:t},\bb{\psi})$. Further details on MCMC are provided in Section \ref{MCMC methods for posterior inference}. \
Let us now consider the BSTS model's specification. We restrict our attention to univariate time series, as in @scott_varian_2013, but multivariate extensions are available (see @ning2021mbsts). As already mentioned, the time series structural components enter the model through the state vector or, in other words, $\boldsymbol{\theta}_{t}=(\mu_{t},\delta_{t},\tau_{t})'$ where $\mu_{t}$ is the level of the series at time $t$, $\delta_{t}$ is its the time-varying growth and $\tau_{t}$ is the seasonal factor. A convenient way to deal with seasonality is to consider the seasonal factors as the dynamic coefficients of a set of $S$ dummy variables and constrain them to have zero expectation over a full cycle of $S$ seasons. Finally, augmenting the observation equation with a regression component, we obtain a Bayesian Structural Time Series model
  \begin{align} \label{eq:eqbsts}
  Y_{t} & =\mu_{t}+\tau_{t}+\boldsymbol{x}'_{t}\boldsymbol{\beta}+\epsilon_{t}\\
  \mu_{t} & = \mu_{t-1}+\delta_{t-1}+u_{t} \nonumber \\
  \delta_{t} & = \delta_{t-1}+v_{t} \nonumber \\
  \tau_{t} & =-\sum_{s=1}^{S-1}\tau_{t-s}+w_{t} \nonumber
  \end{align}
With reference to equation (\ref{eq:DLM2}), $\bb{\eta}_{t}=(u_{t},v_{t},w_{t})'$ and $\bb \eta_{t} \sim N_{3}(\bb 0,\Sigma_{\eta})$ where $\Sigma_{\eta,t}=diag(\sigma^{2}_{u},\sigma^{2}_{v},\sigma^{2}_{w})$, while $\Sigma_{\epsilon,t}=\sigma^{2}_{\epsilon}$ in equation (\ref{eq:DLM1}), for $t \geq 1$. The unknown parameters of this model are $(\bb \beta,\sigma^{2}_{\epsilon},\sigma^{2}_{u},\sigma^{2}_{v},\sigma^{2}_{w})$. We assign a prior to the unknown parameters, that is updated as new observations become available. More insights on this are provided in Section \ref{MCMC methods for posterior inference}. In the next paragraph (Section \ref{Static Shrinkage in Bayesian Structural Time Series}) we focus instead on the regression part and the problem of Bayesian variable selection. 
  
  \subsection{Static Shrinkage in Bayesian Structural Time Series}\label{Static Shrinkage in Bayesian Structural Time Series}
  
A static regression component is frequently included in Bayesian Structural Time Series.
The latter can be incorporated into the baseline state space model in a variety of ways. The method envisaged by @scott_varian_2013 is conceptually the simplest and operationally the most efficient. It consists in appending a constant $1$ to the state vector $\bb \theta_{t}$ and $\boldsymbol{x}'_{t}\boldsymbol{\beta}$ to $Z_{t}$ in equation (\ref{eq:eqbsts}). In this way the dimension of the state vector increases just by one, regardless the number of predictors, and since the computational complexity of the Kalman filter is linear in the length of the data and quadratic in the dimension of the state vector, the computational savings are substantial. Considering that BSTS have been developed to deal with internet data, the number of predictors entering our model might be potentially huge and, in particular, much larger than the number of available observations (in other words $p>>T$). Therefore, estimation issues are likely to arise. To cope with this, spike-and-slab regression is used to introduce sparsity among the regression coefficients. Especially when using web data, it is indeed safe to assume that many predictors would not play any role in the regression and thus they must be shrunk to zero. For this purpose, @scott_varian_2013 rely on the SSVS we discussed in Section \ref{Indicator Model Selection}. Let $\gamma_{j}$ be an auxiliary binary indicator assuming two values
\begin{align*}
\gamma_{j} & = 1 \ \ \ \text{if} \ \ \ \beta_{j} \neq 0 \\
\gamma_{j} & = 0  \ \ \ \text{if} \ \ \ \beta_{j}=0
\end{align*}
where $\beta_{j}$ is the coefficient of the $j$-th predictor. Let $\boldsymbol\beta_{\gamma}$ be the subset of active coefficients, i.e $\beta_{j} \neq 0$. A spike-and-slab prior can be written 
\begin{equation*}
\pi(\bb \beta,\bb \gamma,\sigma^{2}_{\epsilon})=\pi(\bb \beta_{\gamma}|\bb \gamma,\sigma^{2}_{\epsilon})\pi(\sigma^{2}_{\epsilon}|\bb \gamma)\pi(\bb \gamma)
\end{equation*}
where $\pi(\bb \gamma)$ is the spike and, in practice, it usually coincides with a Bernoulli prior
\begin{equation*}
\bb \gamma \sim \prod_{j=1}^{p}\pi_{j}^{\gamma_{j}}(1-\pi_{j})^{1-\gamma_{j}}
\end{equation*}
which can be simplified by assuming that the probability of the indicator to be $0$ or $1$ is the same for each predictor or, in other words, by imposing $\pi_{j}=\pi$. Moreover, there are multiple ways to set the hyperparameter's values. Coherently with the Bayesian approach, a simple strategy is to fix $\pi$ according to the "expected model size". Therefore the idea is to make a guess on how many coefficients we expect to be active, $p_{0}$, and then set $\pi=\frac{p_{0}}{p}$. On the other hand, a convenient way to model $\bb \beta$ and $\sigma^{2}_{\epsilon}$ is through a conjugate prior law, namely the Normal-Gamma prior distribution
\begin{align} \label{eq:bsts2}
\bb \beta_{\gamma}|\bb \gamma,\sigma^{2}_{\epsilon} & \sim \mathcal{N}(\bb b_{\gamma},\sigma^{2}_{\epsilon}(\Omega_{\gamma}^{-1})^{-1}) \\
\frac{1}{\sigma_{\epsilon}^{2}}|\bb \gamma & \sim \mathcal{G}\bigg(\frac{n_{0}}{2},\frac{d_{0}}{2}\bigg)  \label{eq:bsts3}
\end{align}
where $\bb b_{\gamma}$ is a vector of prior guesses for non null predictors, $\Omega$ is a symmetric matrix and $\Omega_{\gamma}$ denote the rows and columns of $\Omega$ for which $\gamma_{j}=1$. Furthermore, $n_{0}$ represents the prior sample size and $d_{0}$ is the prior sum of squares. They can be set properly by making a guess on the number of observations worth of weight and the expected $R^{2}$. Therefore, equations (\ref{eq:bsts2}) and (\ref{eq:bsts3}) represent the slab density, which can be more or less informative according to how the hyperparameters have been set.  Finally, for effective shrinkage, a possible prior distribution for $\bb \beta_{\gamma}|\bb \gamma,\sigma^{2}_{\epsilon}$ is the "g-prior" proposed by @zellner_1986. The latter is characterized by a null vector $\bb b_{\gamma}$ and $\Omega^{-1}=g\bb{X}'\bb{X}/n$. \
Let $y^{\star}_{t}=y_{t}-\bb F_{t}^{\star}\bb\theta_{t}$ where $\bb F_{t}^{\star}$ is the observation matrix of equation (\ref{eq:DLM1}) with the regression component $\bb{\beta}'\bb{x}_{t}$ set to zero and let $y_{1:T}^{\star}=(y_{1}^{\star},...,y_{t}^{\star})$. Thanks to the conjugate nature of the model, posterior inference can be carried out. The Bayesian updating process of a Normal-Gamma prior leads to a Normal-Gamma posterior distribution with parameters:
\begin{align*}
\bb \beta_{\gamma}|\bb \gamma,\sigma^{2}_{\epsilon},y_{1:T}^{\star} & \sim \mathcal{N}( \tilde{\bb{\beta}}_{\gamma},\sigma^{2}_{\epsilon}(\Lambda_{\gamma}^{-1})^{-1}) \\
\frac{1}{\sigma_{\epsilon}^{2}}|\bb{\gamma},y_{1:T}^{\star} & \sim \mathcal{G}\bigg(\frac{n_T}{2},\frac{d_T}{2}\bigg)
\end{align*}
where
\begin{align*}
\Lambda_{\gamma}^{-1} & =(\bb{X}'\bb{X})_{\gamma}+\Omega_{\gamma}^{-1} \\
n_{T} & = n_{0} + T \\
\tilde{\bb{\beta}_{\gamma}} & =(\Lambda_{\gamma}^{-1})^{-1}(\bb{X}_{\gamma}'y_{1:T}^{\star}+\Omega_{\gamma}^{-1}\bb{b}_{\gamma}) \\
d_{T} & =d_{0}+{y_{1:T}^{\star}}' y_{1:T}^{\star}+ \bb{b}_{\gamma}'\Omega_{\gamma}^{-1}\bb{b}_{\gamma}-\tilde{\bb{\beta}_{\gamma}}'\Lambda_{\gamma}^{-1}\tilde{\bb{\beta}_{\gamma}}
\end{align*}
Moreover, it is possible to marginalize out $\bb \beta_{\gamma}$ and $\sigma^{2}_{\epsilon}$ from the full posterior distribution $\pi(\bb \beta_{\gamma},\sigma^{2}_{\epsilon},\bb \gamma)$ and obtain the marginal posterior distribution of the indicator 
\begin{equation*}
\bb \gamma|y_{1:T}^{\star} \sim C(y_{1:T}^{\star})\frac{|\Omega^{-1}_{\gamma}|^{\frac{1}{2}}\pi(\bb \gamma)}{|\Lambda^{-1}_{\gamma}|^{\frac{1}{2}}d_{T}^{\frac{n_{T}}{2}-1}}
\end{equation*}
where $C(y_{1:T}^{\star})$ is a normalizing constant that depends on $\bb{y}^\star$ but not on $\bb \gamma$. This equation can be used to draw from the posterior distribution of every $\gamma_{j}$ given $\bb \gamma_{-j}$, which represents all the elements of $\bb \gamma$ except $\gamma_{j}$. However, the MCMC algorithms used to fit the model is such that it does not require $C(y_{1:T}^{\star})$ to be computed explicitly. 
  
\subsection{MCMC methods for posterior inference}\label{MCMC methods for posterior inference}

Let $\bb \psi=(\bb \beta,\sigma^{2}_{\epsilon},\sigma^{2}_{u},\sigma^{2}_{v},\sigma^{2}_{w})$ be the vector of model parameters and $y_{t+h}$ the $h$-step ahead forecast, the Bayesian approach to parameter learning and forecasting is based on providing their respective posterior distribution starting from an assigned prior distribution. The prior densities of $(\bb \beta,\sigma^{2}_{\epsilon})$ have already been described in the previous paragraph; we also assign an Inverse-Gamma prior to $(\sigma^{2}_{u},\sigma^{2}_{v},\sigma^{2}_{w})$ so that the update rule is analytically available. Since model's parameters are unknown, and here treated as random quantities, Kalman Filter and Kalman Smoother cannot be directly used to obtain a full description of these densities, but an approximate solution can be obtained through Markov Chain Monte Carlo (MCMC) methods. The algorithm below is the Gibbs sampling presented in @scott_varian_2013.
\begin{algorithm}
\caption{Parameter learning in BSTS} \label{alg:BSTS}
\begin{center}
\emph{Step 1: Sample states and States' variances}\\
\end{center}
\nl Sample $\theta_{0:T}$ from $p(\theta_{0:T}|\bb \beta,\sigma^{2}_{\epsilon},\Sigma_{\eta},y_{1:T})$ using the method proposed by Durbin and Koopman (2002)\;
\nl Sample indipendently the diagonal elements of $\Sigma_{\eta}$ from $p(\sigma^{2}_{i}|\theta_{0:T},\bb \beta,\sigma^{2}_{\epsilon},y_{1:T})$ for $i \in \{u,v,w\}$\;
\begin{center}
\emph{Step 2: SSVS}\\
\end{center}
\nl Sample the coefficient vector $\boldsymbol{\beta}$ from $p(\boldsymbol{\beta}|\theta_{0:T},\sigma^{2}_{\epsilon},\boldsymbol{\gamma},y_{1:T})$\;
\nl Sample $\sigma^{2}_{\epsilon}$ from $p(\sigma^{2}_{\epsilon}|\theta_{0:T},\boldsymbol{\beta},\boldsymbol{\gamma},y_{1:T})$\;
\nl Sample the indicator vector $\boldsymbol\gamma$ componentwise by sampling consecutively from $p(\gamma_{j}|\theta_{0:T},\boldsymbol\beta,\boldsymbol\gamma_{-j},y_{1:T})$
\end{algorithm}
The first step of Algorithm \ref{alg:BSTS} can be carried out in a variety of way. In @scott_varian_2013, the authors propose the Simulation Smoother developed by @Durbin_K2002. A brief description of this method is reported separately in Algorithm \ref{alg:DK2002}. Some of the other existing strategies that are used in Chapter \ref{Dynamic Shrinkage} are the Forward Filter Backward Sampling (FFBS) algorithm by @CK_1994, @fruhwirth-schnatter_1994, @S_1994 and the more recent sparse matrix method of @chan_jeliazkov_2009.
\begin{algorithm}
\caption{Simulation Smoother by Durbin and Koopman (2002)} \label{alg:DK2002}
\nl Draw $\bb{\xi}^{+}\sim \mathcal{N}(\bb 0,\bb{\Sigma}_{\xi})$, where $\bb{\xi}=(\bb{\epsilon}_{1}',\bb{\eta}_{1}',...,\bb{\epsilon}_{T}',\bb{\eta}_{T}')'$ \\ and $\bb{\Sigma}_{\xi}=diag(\Sigma_{\epsilon,1},\Sigma_{\eta,1},...,\Sigma_{\epsilon,T},\Sigma_{\eta,T})$ \;
\nl Draw $\bb\theta_{0}\sim\mathcal{N}(\bb{m}_{0},\bb{C}_{0})$\;
\nl Compute $\bb{\theta}^{+}_{t}$ and $\bb{y}^{+}_{t}$, using equation (\ref{eq:DLM2}) and \ref{eq:DLM1} for $t=1,...,T$\;
\nl Apply Kalman Filter and Kalman Smoother to $\bb{y}_{1:T}$ and $\bb{y}_{1:T}^{+}$, respectively the original and the simulated time series, and compute $\bb{m}_{t}$ and $\bb{m}_{t}^{+}$ for $t=0,...,T$\;
\nl Compute the mean correction $\tilde{\bb{\theta}}_{t}$=$\bb{m}_{t}+(\bb{\theta}^{+}_{t}-\bb{m}_{t}^{+})$ for $t=0,...,T$ and obtained the corrected trajectory of the latent states $\tilde{\bb{\theta}}_{0:T}$\;
\end{algorithm}
Forecasting and parameter learning are two interconnected issue. Let $\bb{\tilde{y}}=(y_{T+1},...,y_{T+h})$ the random vector of future observations, forecasting coincides with making inference on the posterior density $\pi(\bb{\tilde{y}}|y_{1:T})$. Such distributions is easy to sample once we have knowledge of the parameters, by integrating out the latter:
\begin{equation} \label{eq:postforecast}
\pi(\bb{\tilde{y}}|y_{1:T})=\int_{\Psi} \pi(\bb{\tilde{y}}|\bb{\psi},y_{1:T})\pi(\bb{\psi}|y_{1:T}) \ d\bb{\psi}
\end{equation}
where $\pi(\bb{\tilde{y}}|\bb \psi,y_{1:T})$ is obtain through the following recursion. For $h=1,2,...$
\begin{enumerate}[label=(\roman*)]
\item $\pi(\bb \theta_{t+h}|\bb \psi,y_{1:T})=\int \pi(\bb \theta_{t+h}|\bb \psi,\bb \theta_{t+h-1})\pi(\bb \theta_{t+h-1}|\bb \psi,y_{1:T}) d\bb \theta_{t+h-1}$
\item $\pi(y_{t+h}|\bb \psi, y_{1:T})=\int \pi(y_{t+h}|\bb \psi,\bb{\theta}_{t+h})\pi(\bb{\theta}_{t+k}|\bb \psi,y_{1:T}) d\bb{\theta}_{t+h}$
\end{enumerate}
The draws generated from the density of equation (\ref{eq:postforecast}) can be used to compute quantities of interest such as the expected values, the median, the variance and the quantiles. Moreover, an interesting feature of SSVS algorithm is that each draw $(\bb{\theta}_{0:T}^{(i)},\bb{\psi}^{(i)})$, $i=1,...,N$, where $N$ is the number of draws, depends on a different $\bb{\gamma}^{(i)}$ or, in other words, on a different model. Therefore the posterior predictive distribution can be thought as a weighted average of model-specific predictive distributions where the weights are given by $\pi(\bb{\gamma}|y_{1:T})$. This mechanism is better known as Bayesian Model Averaging.\
Finally, the intrinsic flexibility of Bayesian Structural Time Series models allows to insert a dynamic regression component into the system of equation (\ref{eq:eqbsts}). Although @scott_varian_2013 do not explore this further, the insight is to enlarge the state vector of as many dimensions as the number of dynamic coefficients, which are assumed to have this evolution
\begin{equation*}
\beta_{i,t}=\beta_{i,t-1}+\xi_{t}, \ \ \ \xi_{t}\sim \mathcal{N}\bigg(0,\frac{\sigma_{i}^{2}}{\V(\bb{X}_{i})}\bigg) 
\end{equation*}
for $i^{th}$ coefficient. Thus, each coefficient is supposed to evolve independently as a random walk and its variance is given by $\sigma^{2}_{i}$ scaled by the variance of the $i^{th}$ column of $\bb X$. Moreover, a Gamma prior is assigned to the precision
\begin{equation*}
\frac{1}{\sigma^{2}}\sim \mathcal{G}(a,b) 
\end{equation*}
and two independent Gamma priors are also assigned to the hyperparameters $\sqrt{a/b}$ and $a$.\
The authors do not discourage the use of dynamic regressors, however they recommend to be parsimonious with them. As remarked in Section \ref{Static Shrinkage in Bayesian Structural Time Series}, the computational complexity of Kalman Filter is quadratic in the size of the state vector, hence the introduction of many dynamic regressors would dramatically increases the computational efforts. Moreover, no shrinking methods has been envisaged so far. Therefore, the inclusion of a set of dynamic regressors (that is meant to be large in a big data context) may lead to a slow and inefficient algorithm, entailing overfitting and inaccurate states' estimates. In Chapter \ref{Dynamic Shrinkage in Bayesian Structural Time Series} we propose an innovative approach to overcome these problems by introducing sparsity in the Bayesian Structural Time Series framework.
 
\chapter[Dynamic Shrinkage]{Dynamic Shrinkage with Spike-and-Slab Process Priors}\label{Dynamic Shrinkage}
  
In time-series analysis, the assumption of static regression coefficients may be overly limiting. Moreover, the lack of dynamic variable selection methods posed a significant barrier to using large sets of predictors in dynamic regression models. Recently, a growing literature has been addressing this issue (see Kalli and Griffin 2014, Nakajima and West 2013, Belmonte, Koop,
and Korobilis 2014, Bitto and Frühwirth-Schnatter 2018, and others). In this chapter we want to illustrate the dynamic variable selection approach of @rockova_mcalinn_2021 is a significant contribution in this research area. Before we continue, a small premise on what is dynamic sparsity is necessary. Let $\bb{\beta}_{1:T}=(\bb{\beta}_{1},...,\bb{\beta}_{T})$ be the matrix of regression coefficients. Sparsity can be induced: (a) \emph{horizontally}, when the $j^{th}$ predictor is not persistently relevant or, in other words, the $j^{th}$ coefficient $\beta_{1:T, j}=(\beta_{1,j}, \ldots, \beta_{T,j})$ presents intermittent zeros, (b) \emph{vertically}, when at period $t$ only a subset of coefficients $\bb{\beta}_t=(\beta_{t,1}, \ldots, \beta_{t,p})$ is active. With the term \emph{dynamic sparsity} we refer to a two-pronged issue, which is given by the combination of horizontal and vertical sparsity.\
Therefore, consider the following large Time-Varying Parameter (TVP) regression model characterized by a latent process having a Markovian structure
\begin{align} \label{eq:eq211}
  y_{t}= & \boldsymbol{x'}_{t}\boldsymbol{\beta}_{t}+\epsilon_{t}, \ \ \ \epsilon_{t}\sim \mathcal{N}(0,\sigma^{2}_{\epsilon,t}) \\
  \boldsymbol{\beta}_{t} = & f( \boldsymbol{\beta}_{t-1})+ \boldsymbol{\xi}_{t}, \ \ \ \boldsymbol{\xi}_{t}\sim \mathcal{N}(\boldsymbol{0},\boldsymbol{\Lambda}_{t}) \nonumber
\end{align}
for $t=1,...,T$, and assume that only a small fraction of the total set of explanatory variables is truly relevant and that their importance changes over time. Assuming $f$ to be a linear function, this TVP model coincides with the DLM illustrated in Section \ref{Structural Time Series Models} with $\bb{F}_{t}=\bb{x}_{t}$ and $\bb{\theta}_{t}$ being a vector of time-varying regression coefficients $\boldsymbol{\beta}_{t}=(\beta_{t,1},...,\beta_{t,p})'$. It is the Markovian structure of the state process that makes possible to estimate the $T\times p$ coefficients of equation (\ref{eq:eq211}) with just $T$ observations. Standard filtering approaches, on the other hand, become less accurate at providing state estimations as $p$ grows larger, dissolving the signal across redundant predictors. Therefore, the necessity of a dynamic variable selection method arises. \emph{Dynamic Spike-and-Slab (DSS) priors} (@rockova_mcalinn_2021) offer a solution to this problem. Before we proceed, it is worth to make a remark about the trade-off between modeling flexibility in the (conditional) mean and in the residuals, and possible poor identifiability or overfitting.  Time-varying regression coefficients allow to capture non linear features of the data; thus, the model might actually succeed in capturing the predictable dependence through the (flexible) conditional mean, so that "what is left" are indeed i.i.d. residuals. However, there are situations where some form of  structural dependence still remains. In particular, model mispecification might imply time-varying residual variance despite the large number of predictors. Also, especially in macroeconomics, idiosyncratic shocks are of interest and they are usually time persistent, for this reason in the macroeconomic literature they are usually modeled using a stochastic volatility model. The gain of switching from a constant variance model to a time-varying volatility model for macroeconomic forecasting have been empirically shown by @CR_2015.

\section{Dynamic Spike-and-Slab priors}
Dynamic Spike-and-Slab priors (DSS) can be regarded as an extension of the Spike-and-Slab approach, illustrated in paragraph \ref{Hierarchical Models for Variable Selection}, to the framework of Mixture Autoregressive (MAR) processes [@WL_2000], where the mixing weights are allowed to change over time. We present this approach in a TVP regression model where every coefficients $\beta_{t,j}$ are independently and identically distributed as a DSS process prior. In this case we can simplify the notation by suppressing the subscript $j$. \
Let us start with the conditional specification of the DSS prior. Given $\beta_{t-1}$ and an auxiliary binary indicator $\gamma_{t} \in \{0,1\}$ that denotes whether the variable fall in the spike $\psi_{0}(\beta_{t}|\lambda_{0})$ or in the slab $\psi_{1}(\beta_{t}|\mu_{t},\lambda_{1})$ distribution, then $\beta_{t}$ has a mixture density
\begin{equation} \label{eq:eq2.1}
\pi(\beta_{t}|\gamma_{t},\beta_{t-1})=(1-\gamma_{t})\psi_{0}(\beta_{t}|\lambda_{0})+\gamma_{t}\psi_{1}(\beta_{t}|\mu_{t},\lambda_{1})
\end{equation}
where
\begin{equation}\label{eq:eq2.2}
\mu_{t}=\phi_{0}+\phi_{1}(\beta_{t-1}-\phi_{0}) \ \ \ \text{with} \ \ \ |\phi_{1}|<1
\end{equation}
and
\begin{equation}\label{eq:eq2.3}
P(\gamma_{t}=1|\beta_{t-1})=\omega_{t}
\end{equation}
In order to achieve variable selection, the spike and slab distributions have to be choosen such that the first is concentrated on zero ($\lambda_0$ is small) while the latter is more diffuse ($\lambda_1 >\lambda_0$) and it may eventually have a different mean. Note that this hierarchical setup (equations (\ref{eq:eq2.1})--(\ref{eq:eq2.3})) introduces two important innovations with respect to the one described in Section \ref{Indicator Model Selection}.

Firstly, instead of centering both the spike and the slab densities at zero, the latter depends on the previous value of the coefficients through $\mu_{t}$. This feature enables to regard the mixture prior of equation (\ref{eq:eq2.1}) as a \emph{multiple shrinkage prior} (George 1986a and George
1986b) which has two gravitational pulls, $0$ and $\mu_{t}$, and only the slab density depends on $\beta_{t-1}$. Therefore, with Dynamic Spike-and-Slab priors we are assuming that the regression coefficients can be classified into two groups: irrelevant coefficients, which fall in the spike and are anchored to zero, and active coefficients, which follow an autoregressive process. In this thesis we conveniently assume that the spike and slab distributions are Gaussian, i.e. $\psi_{0}(\beta_{t}|\lambda_{0}) \equiv \mathcal{N}(0,\lambda_{0})$ and $\psi_{1}(\beta_{t}|\mu_{t},\lambda_{1})\equiv \mathcal{N}(\mu_{t},\lambda_{1})$. Therefore, when the coefficient is active it follows a stationary Gaussian autoregressive process of order one:
\begin{equation}\label{eq:betaevol}
\beta_{t}=\phi_{0}+\phi_{1}(\beta_{t-1}-\phi_{0})+\xi_{t}, \quad \xi_{t}\overset{iid}{\sim}\mathcal{N}(0,\lambda_{1})
\end{equation}
with $|\phi_1|<1$, which is characterized by the following Gaussian stationary distribution
\begin{equation}
\psi_{1}^{ST}(\beta_{t}|\lambda_{1},\phi_{0},\phi_{1})\equiv \psi_{1}\bigg(\beta_{t}|\phi_{0},\frac{\lambda_{1}}{1-\phi_{1}^{2}}\bigg).
\end{equation}

The second advancement is the use of time-varying mixing weights. The sequence of probabilities $(\omega_t)_{t \geq 1}$ is indeed able to evolve smoothly over time. The way the conditional inclusion probabilities are modeled by @rockova_mcalinn_2021 allows to make them marginally stable, that is, their marginal distribution is constant over time, while also including all the relevant information. The rule is
\begin{equation}\label{eq:eq2.8}
\omega_{t}\equiv \omega{(\beta_{t-1})}=\frac{\Omega \; \psi_{1}^{ST}(\beta_{t-1}|\lambda_{1}\phi_{0},\phi_{1})}{\Omega \; \psi_{1}^{ST}(\beta_{t-1}|\lambda_{1}\phi_{0},\phi_{1})+(1-\Omega)\; \psi_{0}(\beta_{t-1},\lambda_{0})}
\end{equation}
where $\Omega \in (0,1)$ is a scalar hyperparameter whose role is to weight the chances to fall in the spike or in the slab distribution.  $\omega_{t}$ is therefore the conditional inclusion probability of  $\beta_{t-1}$ to belong to the stationary slab distribution or to the stationary spike distribution. The intuition is the following: when $|\beta_{t-1}|$ is large, then $\omega(\beta_{t-1})$ approaches one, pointing out that $\beta_{t}$ is likely to belong to the slab distribution. In the same manner, a small coefficient $|\beta_{t-1}|$ means a small mixing weight $\omega(\beta_{t-1})$ and thus it suggests that $\beta_{t}$ is in the spike. 

Together, equations (\ref{eq:eq2.1})--(\ref{eq:eq2.3}) and (\ref{eq:eq2.8}) define a Dynamic Spike-and-Slab (DSS) process with hyperparameters $(\Omega,\lambda_{0},\lambda_{1},\phi_{0},\phi_{1})$
\begin{equation}
(\beta_{t})\sim DSS(\Omega,\lambda_{0},\lambda_{1},\phi_{0},\phi_{1})
\end{equation}
Again, we remark that this formulation entails an interesting consequence which consists in having stable marginal distributions. Theorem 1 in @rockova_mcalinn_2021 proves that if $(\beta_{t})\sim DSS(\Omega,\lambda_{0},\lambda_{1},\phi_{0},\phi_{1})$ with $|\phi_1|<1$, then the probability law of $(\beta_{t})$ is has marginal distributions
\begin{equation}\label{eq:eq5.5}
\pi^{ST}(\beta_{t}|\Omega,\lambda_{0},\lambda_{1},\phi_{0},\phi_{1}) = \Omega \; \psi_{1}^{ST}(\beta_{t}|\lambda_{1},\phi_{0},\phi_{1})+(1-\Omega)\; \psi_{0}(\beta_{t}|\lambda_{0})
\end{equation}
for $t\geq 1$.\
In fact, the mechanism that induces shrinkage counts two steps. After establishing whether the coefficients belong to the the \emph{stationary} slab distribution, then it should be assessed whether the coefficients belong also to the \emph{conditional} slab density. In other words, it is necessary to compute $p^{\star}_t(\beta_t)=\mathbb{P}(\gamma_t=1\mid \beta_t,\beta_{t-1},\omega_t)$. As shown in @rockova_mcalinn_2021, the posterior inclusion probability results to be
\begin{equation}\label{eq:pstar}
p_{t}^{\star}(\beta_{t})\equiv \frac{\omega_{t}\psi_{1}(\beta_{t}|\mu_{t},\lambda_{1})}{\omega_{t}\psi_{1}(\beta_{t}|\mu_{t},\lambda_{1})+(1-\omega_{t})\psi_{0}(\beta_{t}|\beta_{0})}
\end{equation}
Therefore, if $\beta_{t-1}$ is large, then $\omega_t(\beta_{t-1})$ tends to one, signalling that $\beta_{t}$ is likely to belong to the slab. However, depending on the value of $\beta_{t}$ estimated, there are two possible scenarios. If the latter is large in absolute value then also $p^{\star}_t(\beta_t)$ is close to one, which signals that there are no more doubts about $\beta_{t}$ being active and to belong to an autoregressive path. On the other hand, if $|\beta_{t}|$ is small,
$p^{\star}_t(\beta_t)$ will be close to zero. In the latter case, the predictor is considered no more important and its coefficient is shrunk to zero. Clearly, shrinkage towards zero is even more accentuated if also $|\beta_{t-1}|$ is small, anchoring the coefficient to the spike. The full information transmission mechanism is described in the flowchart on the next page.

Assuming Gaussian spike and slab distributions thus leads to shrinkage towards $\mu_{t}$ or zero. Alternatively, rather than shrinking around those values, one may prefer to bring the coefficients to these exact values. This is possible within the Dynamic Spike-and-Slab framework by assuming $\psi_{0}(\beta_{t}|\lambda_{0})$ and $\psi_{1}(\beta_{t}|\mu_{t},\lambda_{1})$ to be two Laplace priors centered respectively in $0$ and $\mu_{t}$, i.e. $\psi_{0}(\beta|\lambda_{0})\equiv \frac{\lambda_{0}}{2}\exp\{-|\beta|\lambda_{0}\}$ and $\psi_{1}(\beta|\mu_{t},\lambda_{1})\equiv \frac{\lambda_{1}}{2}\exp\{-|\beta-\mu_{t}|\lambda_{1}\}$. However, while using a Laplace spike does not introduce further complexities to the model, modelling the slab density as a Laplace density entails a whole range of considerations. We discuss them in Appendix.

Operationally, once the full system has been specified, inference can be carried out using Markov Chain Monte Carlo methods. In the next sections we illustrate some useful algorithms to efficiently simulate from the posterior distributions and to compute Maximum A Posteriori (MAP) estimates.

\begin{tikzpicture}[>=stealth]
  \node[draw] (dWdt) {$|\beta_{t-1}|$};
  \node[draw] [below left=1cm and 3cm of dWdt] (large) {large};
  \node[draw] [below right=1cm and 3cm of dWdt] (small) {small};
  \node[draw] [below=of large] (uno) {$\omega(\beta_{t-1})\to1$}; 
  \node[draw] [below=of small] (zero) {$\omega(\beta_{t-1})\to0$};
  \node[draw] [below= of uno,text width=4cm] (slab) {$\beta_{t}$ is more likely to belong to the slab};
  \node[draw] [below= of zero,text width=4cm] (spike) {$\beta_{t}$ is more likely to belong to the spike};
  \node[draw] [below left=1cm and -2cm of slab] (larget) {$|\beta_{t}|$ is indeed large};
  \node[draw] [below right=1cm and -2cm of slab] (smallt) {$|\beta_{t}|$ is instead small};
  \node[draw] [below left=1cm and -2cm of spike] (larget1) {$|\beta_{t}|$ is instead large};
  \node[draw] [below right=1cm and -2cm of spike] (smallt1) {$|\beta_{t}|$ is indeed small};
  \node[draw] [below=of larget] (unot) {$p^{\star}(\beta_{t})\to1$}; 
  \node[draw] [below=of smallt] (zerot) {$p^{\star}(\beta_{t})< 1$};
  \node[draw] [below=of larget1] (unot1) {$p^{\star}(\beta_{t})>0$}; 
  \node[draw] [below=of smallt1] (zerot1) {$p^{\star}(\beta_{t})\to0$};
  \node[draw] [below=of unot,text width=3cm] (cslab) {$\beta_{t}$ is an active coefficient ($\gamma_{t}=1$)}; 
  \node[draw] [below=of zerot,text width=3cm] (cspike) {$\beta_{t}$ is negligible and it steps to the spike with a certain probability}; 
  \node[draw] [below=of unot1,text width=3cm] (cslab1) {$\beta_{t}$ has a certain (small) probability to step in the slab}; 
  \node[draw] [below=of zerot1,text width=3cm] (cspike1) {$\beta_{t}$ is anchored to zero ($\gamma_{t}=0$)}; 
  
  \draw[->] (dWdt) -- (large);
  \draw[->] (dWdt) -- (small);
  \draw[->] (large) -- (uno);
  \draw[->] (small) -- (zero);
  \draw[->] (uno) -- (slab);
  \draw[->] (zero) -- (spike);
  \draw[->] (slab) -- (larget);
  \draw[->] (slab) -- (smallt);
  \draw[->] (spike) -- (larget1);
  \draw[->] (spike) -- (smallt1);
  \draw[->] (larget) -- (unot);
  \draw[->] (smallt) -- (zerot);
  \draw[->] (larget1) -- (unot1);
  \draw[->] (smallt1) -- (zerot1);
  \draw[->] (unot) -- (cslab);
  \draw[->] (zerot) -- (cspike);
   \draw[->] (unot1) -- (cslab1);
  \draw[->] (zerot1) -- (cspike1);
  
\end{tikzpicture}  

\section{Dynamic Stochastic Search Variable Selection}\label{Dynamic Stochastic Search Variable Selection}
  
Efficient posterior simulation of parameters of Time-Varying Parameter regression models with Dynamic Spike-and-Slab process priors is made possible thanks to a MCMC algorithm developed by @rockova_mcalinn_2021. The latter consists of a Gibbs sampler which develops in three stages:
\begin{enumerate}
\item Simulate the regression coefficients from $\pi(\bb{\beta}_{0:T}|\bb{\gamma}_{0:T},\sigma^{2}_{\epsilon,0:T},y_{1:T})$ using Forward Filter Backward Sampling (FFBS), as illustrated in \emph{Step: 1} of Algorithm \ref{alg:DSSVS-original},
\item Simulate the inclusion indicators from $\pi(\bb{\gamma}_{0:T}|\bb{\beta}_{0:T}\sigma^{2}_{\epsilon,0:T},y_{1:T})$ using results of Equations \ref{eq:eq2.8} and \ref{eq:pstar} to compute conditional mixing weights and posterior inclusion probabilities,
\item Simulate the residual variances from $\pi(\sigma^{2}_{\epsilon,0:T}|\bb{\gamma}_{0:T},\bb{\beta}_{0:T},y_{1:T})$ using a FFBS strategy as the one illustrated in \emph{Step: 3} of Algorithm \ref{alg:DSSVS-original}.
\end{enumerate}
Since this strategy reminds the SSVS of @GM_1993, then the procedure takes the label \emph{Dynamic Stochastic Search Variable Selection} (or Dynamic SSVS). Here we illustrate the original version of the Dynamic SSVS developed by the authors, which constitute the starting point for the algorithms developed in the next sections. In particular, we refer to the Gaussian Spike-and-Slab case introduced previously. Nevertheless, the procedure described here can be rearranged easily to include a Laplacian spike instead of a Gaussian one. The Gaussian specification allow a straightforward and elegant representation of the latent process, which is the following 
\begin{equation}
\bb{\beta}_{t}=\bb{H}_{t}+\bb{G}_{t}(\bb{\beta_{t-1}}-\bb{H}_{t})+\bb{\xi}_{t}, \ \ \ \bb{\xi}_{t}\sim\mathcal{N}(\bb{0},\Lambda_{t})
\end{equation}
where $\bb{H}_{t}=\phi_{0}\bb{\gamma}_{t}'$, $\bb{G}_{t}=diag\{\gamma_{t,j}\phi_{1}\}_{j=1}^{p}$, $\Lambda_{t}=diag\{\gamma_{t,j}\lambda_{1}+(1-\gamma_{t,j})\lambda_{0}\}_{j=1}^{p}$ and the initial state vector $\bb{\beta}_{0}\sim\mathcal{N}(\bb{m}_{0},\bb{C}_{0})$ with $\bb{m}_{0}=\phi_{0}\bb{\gamma}_{0}$ and $\bb{C}_{0}=diag\{\gamma_{0,j}\lambda_{1}/(1-\phi_{1}^{2})+(1-\gamma_{0,j})\lambda_{0}\}_{j=1}^{p}$. This model's hyperparameters are $(\Omega,\lambda_{0},\lambda_{1},\phi_{0},\phi_{1})$, which can be calibrated or, eventually, treated as random variables by placing a prior density to them. In this thesis we fix a priori $\phi_1$ and $\phi_0=0$, and we usually assume that $\phi_0=0$ and $\phi_1$ ranges between $0.98$ and $0.9$. This of course entails great simplifications, but seems to work well in all cases we explored. On the other hand, in its original version the author suggest to treat $\phi_1$ as a Beta random variable with support $[0.8,1)$. We see that the gain due to the latter specification and the drawbacks in terms of implementation of adding a Metropolis-Hasting step in the Gibbs sampling scheme compensate each other, that's why we opted for we opted for a predetermined choice. The hyperparameters $\lambda_0$ and $\lambda_1$ are set a-priori also in the original model @rockova_mcalinn_2021.\
The residual variances are modeled through discount factors. This simple strategy, developed by @WH_1997, allows temporal fluctuations of the model's variance while preserving conjugacy. Briefly, consider the precision sequence $\nu_{t}=\frac{1}{\sigma^{2}_{\epsilon,t}}$ for $t=1,...,T$. The latter is assumed to follow a stochastic evolution affected by independent random shocks $c_{t}/\delta$, thus
\begin{equation}\label{eq:discountfactor}
\nu_{t}=\frac{c_{t}}{\delta}\nu_{t-1}
\end{equation}
where $\delta$ is the discount factor. It takes values in $(0,1]$ and it can be regarded as the decay of precision from period $t-1$ to $t$. Therefore, the precision in every period depends on the previous precision and on the new information that becomes recursively available. The dynamic of equation (\ref{eq:discountfactor}) is completed by assuming $c_{t}\sim\mathcal{B}(\frac{\delta n_{t-1}}{2},\frac{(1-\delta)n_{t-1}}{2})$ and $n_{t}=\delta n_{t-1}+1$. This implies $\E(c_{t}|y_{1:t-1})=\delta$ or, equivalently, that $\E(c_{t}/\delta|y_{1:t-1})=1$. Assigning a Gamma prior on $\nu_{0}\sim\mathcal{G}(n_{0},d_{0})$, one can solve the updating process and solutions in closed form. For $t>0$, the one-step-ahead predictive distribution is
\begin{equation*}
\nu_{t}|y_{1:t-1},\bb{\beta}_{1:t-1}\sim\mathcal{G}\bigg(\frac{\delta n_{t-1}}{2},\frac{\delta d_{t-1}}{2}\bigg) 
\end{equation*}
which updates into
\begin{equation}\label{eq:factmod}
\nu_{t}|y_{1:t},\bb{\beta}_{1:t}\sim\mathcal{G}\bigg(\frac{n_{t}}{2},\frac{ d_{t}}{2}\bigg) 
\end{equation}
with $n_{t}=\delta n_{t-1}+1$ and $d_{t}=\delta d_{t-1}+r^{2}_{t}$, where $r_{t}=y_{t}-\bb{x}_{t}'\bb{\beta}_{t}$. Posterior sampling of model's precision can be carried out via a FFBS algorithm (more details in West and Harrison, 1997). Basically, the filtering steps follow the updating rule just mentioned, while the backward sampling consists in drawing from $\nu_{T}\sim\mathcal{G}(n_{T}/2,d_{T}/2)$, and then drawing from $\eta_{t}\sim\mathcal{G}((1-\delta)n_{t} / 2,d_{t}/2)$ and setting $\nu_{t}=\eta_{t}+\delta\nu_{t+1}$ for $t=T-1,...,0$. This mechanism is outlined in \emph{Step 3} of Algorithm \ref{alg:DSSVS-original}. Note that the discount factor strategy for variances estimation can be reconciled to the standard Bayesian estimation strategy for the unknown model's variance. Consider for example
\begin{align*}
  y_{t}|\boldsymbol{x'}_{t}\boldsymbol{\beta}_{t},\sigma^{2}_{\epsilon} & \ind \mathcal{N}(\boldsymbol{x'}_{t}\boldsymbol{\beta}_{t},\sigma^{2}_{\epsilon}) \\
  \frac{1}{\sigma^{2}_{\epsilon}} \sim & \mathcal{G}\bigg(\frac{n}{2},\frac{d}{2}\bigg)
  \end{align*}
Then we have
\begin{align*}
\pi(\nu|\bb{\beta}_{0:T},\bb{\gamma}_{0:T},y_{1:T})& \propto \pi(\nu,\bb{\beta}_{0:T},\bb{\gamma}_{0:T},y_{1:T})\\
&\propto  \pi(y_{1:T}|\nu,\bb{\beta}_{0:T},\bb{\gamma}_{0:T})\pi(\bb{\beta}_{0:T}|\nu,\bb{\gamma}_{0:T})\pi(\bb{\gamma}_{0:T}|\nu)\pi(\nu)\\
&\propto  \pi(y_{1:T}|\nu,\bb{\beta}_{0:T})\pi(\nu)\\
&\propto \prod_{t=1}^{T}\pi(y_{t}|\nu,\bb{\beta}_{t})\pi(\nu)\\
&\propto \pi(\nu)(\nu)^{T/2}\exp\bigg\{-\frac{\nu}{2}\sum_{t=1}^{T}(y_{t}-\bb{x}_{t}'\bb{\beta}_{t})^{2}\bigg\}\\
&\propto \nu^{\frac{n}{2}+T/2+1}\exp\bigg\{-\nu\bigg[\frac{d}{2}+\frac{1}{2}\sum_{t=1}^{T}(y_{t}-\bb{x}_{t}'\bb{\beta}_{t})^{2}\bigg]\bigg\}.
\end{align*}
Therefore, the full conditional posterior distribution is 
\begin{equation*}
\nu|\bb{\beta}_{0:T},\bb{\gamma}_{0:T},y_{1:T}\sim\mathcal{G}\bigg(\frac{n+T}{2},\frac{d}{2}+\frac{1}{2}\sum_{t=1}^{T}(y_{t}-\bb{x}_{t}'\bb{\beta}_{t})^{2}\bigg)
\end{equation*}
which is equivalent to the one obtained using the discount factor model at time $T$ for $\delta=1$. Consequently, no further challenges are required to step from a stochastic to a constant volatility model.

The Dynamic SSVS illustrated in this section provides satisfactory results, however there is room for improvements. Trials performed on simulated data show indeed that misspecification concerning residual variances may seriously affect coefficients' estimates. The reason lies on an artificial bias we detected in the signal-to-noise ratio, which is decreasing in $\sigma^{2}_{\epsilon,t}$. We label this phenomenon the "Spike trap". In fact, some irrelevant predictors may activate after some periods whereas other predictors might leave and re-enter the model multiple times as time progresses. However, the Dynamic Spike-and-Slab approach is basically distrustful and it allows steps from the spike to the slab distribution only if there is enough evidence supporting this change. This is legitimate and it aims at avoiding abrupt changes from the spike to the slab densities. However, in certain cases some coefficients may be forced to the spike even when it is incorrect. On the other hand, the discounted factor model for the observational variance is an estimation strategy that heavily relies on cumulated residuals through $d_{t}=d_{t-1}+r_{t}^{2}$. However, if the system fails to recognize some active coefficients in the first iterations of the Markov Chain, then the residuals $r_{t}=y_{t}-\bb{x}_{t}'\bb{\beta}_{t}$ become larger and thus the variances inflate. In other words, the algorithm mistakes the fluctuations of the observations for variations of the volatility process rather than changes of the regression coefficients. Eventually, such a bias can exacerbate in a vicious cycle whereby: the system erroneously assign a variable to the spike, then the estimated residual variances inflate and the signal-to-noise ratio decreases. This leads the algorithm to become even more distrustful of the observations and it continues to assign those coefficients to the spike. 

Nevertheless, this undesirable mechanism can be fixed. In particular, our proposal is the following. Since the first iterations are the most critical we recommend to fix the variance at a very low level for the first loops in such a way that the algorithm strives to understand which variable can produce the fluctuation observed and then continue with the MCMC as described. Even though this solution is quite heuristic, we notice that forcing $\sigma^{2}_{\epsilon,t}=0.25$ for $t=1,...,T$ for just the first 10 loops improves significantly the estimates. In addition, we developed an alternative Dynamic SSVS which is based on another strategy for estimating the volatility process and that we found out to return better performances in terms of accuracy of the estimates and running time. More details about this method are provided in the next paragraph. 


\begin{algorithm}
\begin{footnotesize}
\caption{Dynamic SSVS by Rockova and McAlinn (2021)} \label{alg:DSSVS-original}
\nl \textbf{Initialize} $\gamma_{j,t}$ and $\sigma^{2}_{\epsilon,t}$ for $0 \leq t \leq T$ and $1 \leq j \leq p$ and set $n_{0}$ and $d_{0}$\;
\begin{center}
\emph{Step 1: Sample Regression Coefficients}\\
\end{center}
\nl \For{$1 \leq t \leq T$}{ 
Compute $\boldsymbol{a}_{t}=\boldsymbol{H}_{t}+\boldsymbol{G}_{t}(\boldsymbol{m}_{t-1}-\boldsymbol{H}_{t})$\;
Compute $\boldsymbol{R}_{t}=\boldsymbol{G}_{t}\boldsymbol{C}_{t-1}\boldsymbol{G}'_{t}+\boldsymbol{\Lambda}_{t}$\;
Compute $f_{t}=\boldsymbol{x}'_{t}\boldsymbol{a}_{t}$ and $e_{t}=y_{t}-f_{t}$\;
Compute $q_{t}=\boldsymbol{x}'_{t}\boldsymbol{R}_{t}\boldsymbol{x}_{t}+\sigma^{2}_{\epsilon,t}$\;
Compute $\boldsymbol{m}_{t}=\boldsymbol{a}_{t}+\boldsymbol{A}_{t}e_{t}$ and $\boldsymbol{C}_{t}=\boldsymbol{R}_{t}-\boldsymbol{A}_{t}\boldsymbol{A}'_{t}q_{t}$ with $\boldsymbol{A}_{t}=\boldsymbol{R}_{t}\boldsymbol{x}_{t}/q_{t}$
}
Draw $\boldsymbol{\beta}_{t}\sim \mathcal{N}(\boldsymbol{m}_{T},\boldsymbol{C}_{t})$\; \For{$t = T-1,...,0$}{  
Compute $\boldsymbol{a}_{T}(t-T)=\boldsymbol{m}_{t}+\boldsymbol{B}_{t}(\boldsymbol{\beta}_{t+1}-\boldsymbol{a}_{t+1})$\;
Compute $\boldsymbol{R}_{t}(t-T)=\boldsymbol{C}_{t}-\boldsymbol{B}_{t}\boldsymbol{R}_{t+1}\boldsymbol{B}'_{t}$ where $\boldsymbol{B}_{t}=\boldsymbol{C}_{t}\boldsymbol{G}'_{t+1}\boldsymbol{R}_{t+1}^{-1}$\;
Draw $\boldsymbol{\beta}_{t}\sim \mathcal{N}(\boldsymbol{a}_{T}(t-T),\boldsymbol{R}_{t}(t-T))$ \;
}
\begin{center}
\emph{Step 2: Sample Indicators}\\
\end{center}
\nl \For{$j=1,...,p$}{
\For{$1 \leq t \leq T$}{
Compute $\omega_{j,t}=\omega(\beta_{j,t-1})$ from \;
Compute $p^{\star}_{t,j}=p^{\star}_{t,j}(\beta_{j,t})$ from \;
Draw $\gamma_{j,t} \sim Bernoulli(p^{\star}_{j,t}(\beta_{j,t}))$\;
}
Compute $p^{\star}_{j,0}=\omega(\beta_{j,0})$\;
Draw $\gamma_{j,0} \sim Bernoulli(p^{\star}_{j,0}(\beta_{j,0}))$\;
}
\begin{center}
\emph{Step 3: Sample Volatility}\\
\end{center}
\nl \For{$t = 1,...,T$}{
Compute $n_{t}=\delta n_{t-1}+1$ and $d_{t}=\delta d_{t-1}+r^{2}_{t}$, where $r_{t}=y_{t}-\bb{x}_{t}'\bb{\beta}_{t}$\;
Draw $\nu_{T}\sim\mathcal{G}(n_{T}/2,d_{T}/2)$\;
}
\For{$t=T-1,...,0$}{
Draw $\eta_{t}\sim\mathcal{G}((1-\delta)n_{t} / 2,d_{t}/2)$\;
Set $\nu_{t}=\eta_{t}+\delta\nu_{t+1}$\;
Compute $\sigma^{2}_{\epsilon,t}=\frac{1}{\nu_{t}}$\;
}
\end{footnotesize}
\end{algorithm}


  \subsection{Introducing stochastic volatility with Dynamic Spike-and-Slab process priors}\label{A new proposal for the volatility process}
  
Interestingly, the nature of the MCMC described in Algorithm \ref{alg:DSSVS-original} allows for a great flexibility in the way the volatility can be modeled. Indeed, observational variances are sampled only in \emph{Step 3} and thus our focus will be on the latter, leaving the first two steps unchanged. Our proposal is to replace the discount factor model for the variance with a Stochastic Volatility (SV) model. In the discount factor model, the precision is thought as affected by a random impulse which is modeled in order to maintain the stability of the Gamma function. On the other hand a stochastic volatility model assumes the volatility to be a latent stationary autoregressive process of unknown parameters $(\alpha_{0},\alpha_{1},\sigma^{2}_{\zeta})$, with $|\alpha_{1}|<1$, which are usually considered as random variables. These class of models has been widely explored and a great amount of literature has been produced. Given its popularity in the financial econometrics literature we believe that a stochastic volatility model is of interest and it is particularly appropriate for the applications discussed in this thesis. In details, the specification for the variance here considered is the canonical one reported in the influential article of @KSC_1998
\begin{align*}
\sigma^{2}_{\epsilon,t}=&\exp^{h_{t}},\\
h_{t}= & \alpha_{0}+\alpha_{1}h_{t-1}+\zeta_{t}, \ \ \ \zeta_{t}\iid\mathcal{N}(0,\sigma^{2}_{\zeta})
\end{align*}
Therefore, the new hierarchical setup is
  \begin{equation}
  \begin{aligned}\label{eq:setup}
  y_{t}|\boldsymbol{x'}_{t}\boldsymbol{\beta}_{t},h_{t} & \ind \mathcal{N}(\boldsymbol{x'}_{t}\boldsymbol{\beta}_{t},\exp{h_{t}}) \\
  \beta_{t,j}|\gamma_{t,j},\beta_{t-1,j} & \ind \mathcal{N}(\gamma_{t,j}\mu_{t,j},\gamma_{t,j}\lambda_{1}+(1-\gamma_{t,j})\lambda_{0})\\
  \gamma_{t,j}|\beta_{t-1,j}\ind & Bernoulli(\omega(\beta_{t-1,j}))\\
  \beta_{0,j}\iid & \mathcal{N}(m_{0},C_{0}) \\
  h_{t}|h_{t-1},\alpha_{0},\alpha_{1},\sigma^{2}_{\zeta}\sim & \mathcal{N}(\alpha_{0}+\alpha_{1}(h_{t-1}-\alpha_0),\sigma^{2}_{\zeta})\\
  h_{0}|\alpha_{0},\alpha_{1},\sigma^{2}_{\zeta}\sim & \mathcal{N}\bigg(\alpha_{0},\frac{\sigma^{2}_{\zeta}}{1-\alpha_{1}}\bigg)
  \end{aligned}
  \end{equation}
Note that by taking the residuals, the resulting system is 
\begin{equation}
   \begin{aligned}\label{eq:centeredpar}
r_{t} & =e^{\frac{h_{t}}{2}}\epsilon_{t}, \ \ \ \epsilon_{t}\overset{iid}{\sim}\mathcal{N}(0,1) \\
h_{t} & =\alpha_{0}+\alpha_{1}(h_{t-1}-\alpha_0)+\zeta_{t}, \ \ \ \zeta_{t}\overset{iid}{\sim}\mathcal{N}(0,1)
\end{aligned}
\end{equation}
which is a standard stochastic volatility model.\
Over the years, several methods have been developed to estimate this model. Here, for pragmatism and efficiency, we use the MCMC method developed of @KASTNER2014408. The steps described henceforth replace the steps illustrated in \emph{Step 3} of the original DSS algorithm. The latter considers the following prior specifications for the unknown stochastic volatility model's parameters:
\begin{align*}
\alpha_{0}\sim & \mathcal{N}(a_{\alpha},A_{\alpha}),\\
(\alpha_{1}+1)/2\sim & \mathcal{B}(a_{0},b_{0}),\\
\pm\sqrt{\sigma^{2}_{\zeta}}\sim & \mathcal{N}(0,A_{\sigma_{\zeta}})
\end{align*}
A recommended choice for the hyperparameters in our problem is too set them in such a way that the stochastic volatility process would not show abrupt changes. For example we can guess: $\alpha_{0}\sim \mathcal{N}(-2,10)$, $(\alpha_{1}+1)/2\sim \mathcal{B}(20,1.5)$ such that $\E(\alpha_1)=0.86$ and $\V(\alpha_1)=0.11$, and $A_{\sigma_{\zeta}}=1$ or even less. Draws from the posterior distributions are obtained using a fast simulation procedure provided by the \code{stochvol} package [@SV_2016]. The rapidity of the algorithm is guaranteed by sampling volatilities jointly “all without a loop” (AWOL), a techinque which is exhaustively described in @MCCAUSLAND2011199, and by using the “ancillarity-sufficiency interweaving strategy” (ASIS) developed by @YM_2011. In a nutshell, the ASIS strategy is based on a simple intuition, i.e. every SV models can be written in a \emph{centered parametrization (C)} form, which is the one of equation (\ref{eq:centeredpar}), or alternatively in a \emph{non-centered parametrization (NC)} form, that is 
\begin{equation}
   \begin{aligned}\label{eq:centeredparr}
r_{t} & \sim \mathcal{N}(0,e^{\mu}e^{\sigma \tilde{h}_{t}}),\\
\tilde{h}_{t} & =\alpha_{1}\tilde{h}_{t-1}+\sigma_{\zeta}\zeta_{t}, \ \ \ \zeta_{t}\overset{iid}{\sim}\mathcal{N}(0,1)
\end{aligned}
\end{equation}
where $\tilde{h}_{t}=(h_{t}-\alpha_{0})/\sigma_{\zeta}$. It is immediate to notice that $h_{1:T}$ is a sufficient statistic for $\alpha_0$ and $\alpha_1$ in C, while $\tilde{h}_{1:T}$ in NC is ancillary for the same parameters. Therefore, sampling can be improved by interwaving between the two parametrizations. @YM_2011 link this results to the Basu's theorem, which demonstrates independence among complete sufficient and ancillary statistics. Operationally, the strategy consists in sampling $\alpha_{0},\alpha_{1},\sigma^{2}_{\zeta}$ twice: once in C and once in NC. Therefore, let $\tilde{r}_{t}=\log{r_{t}}^{2}$ and let's approximate $\tilde{\epsilon}=\log\epsilon^{2}_{t}$ with a mixture of normal distributions such that $\log\epsilon^{2}_{t}|i_{t}\iid\mathcal{N}(m_{i_t},s_{i_t}^{2})$, where $i_{t} \in \{1,...,10\}$ is a mixture component indicator at period $t$, then consider the following linear and conditionally Gaussian approximation of a SV model:
\[
\tilde{r}_{t} = m_{i_t}+h_{t}+\tilde{\epsilon}_t, \quad \tilde{\epsilon}_{t}\iid\mathcal{N}(0,s_{i_t}^2)
\]
A MCMC algorithm to simulate from
\begin{align*}
\tilde{r}_{t} & =  m_{i_t}+h_{t}+\tilde{\epsilon}_t, \quad \tilde{\epsilon}_{t}\iid\mathcal{N}(0,s_{i_t}^2),\\
h_{t} & = \alpha_{0}+\alpha_{1}(h_{t-1}-\alpha_0)+\zeta_{t}, \ \ \ \zeta_{t}\overset{iid}{\sim}\mathcal{N}(0,1),
\end{align*}
can be performed in this way:
\begin{enumerate}
\item Sample $h_{1:T}$ AWOL from $h_{1:T}\mid r_t,i_t,\alpha_0,\alpha_1,\sigma^2_\zeta \sim \mathcal{N}(\Xi^{-1} \bb{c},\Xi^{-1})$ in C, where 
\[
\Xi = \begin{pmatrix}
  \frac{1}{s^{2}_{i_1}}+\frac{1}{\sigma^2_\zeta} & \frac{-\alpha_1}{\sigma^{2}_{\zeta}} & 0 & ... & 0 \\
  \frac{-\alpha_1}{\sigma^{2}_{\zeta}} & \frac{1}{s^{2}_{i_1}}+\frac{1+\alpha_1^2}{\sigma^2_\zeta} & \frac{-\alpha_1}{\sigma^{2}_{\zeta}} & \ddots & \vdots \\
  0  &  \frac{-\alpha_1}{\sigma^{2}_{\zeta}} & \ddots & \ddots & 0\\
  \vdots & \ddots & \ddots & \frac{1}{s^{2}_{i_{T-1}}}+\frac{1+\alpha_1}{\sigma^2_\zeta} & \frac{-\alpha_1}{\sigma^{2}_{\zeta}}\\
  0 & \cdots & 0 & \frac{-\alpha_1}{\sigma^{2}_{\zeta}} & \frac{1}{s^{2}_{i_{T}}}+\frac{1}{\sigma^2_\zeta}
  \end{pmatrix}
\]
and 
\[
\bb{c}= \begin{pmatrix} 
\frac{1}{s^{2}_{i_1}}(\tilde{r}_{1}-m_{i_1})+\frac{\alpha_{0}(1-\alpha_1)}{\sigma^{2}_{\zeta}}\\
\frac{1}{s^{2}_{i_2}}(\tilde{r}_{2}-m_{i_2})+\frac{\alpha_{0}(1-\alpha_1)^2}{\sigma^{2}_{\zeta}}\\
\vdots \\
\frac{1}{s^{2}_{i_{T-1}}}(\tilde{r}_{T-1}-m_{i_{T-1}})+\frac{\alpha_{0}(1-\alpha_1)^2}{\sigma^{2}_{\zeta}}\\
\frac{1}{s^{2}_{i_T}}(\tilde{r}_{2}-m_{i_T})+\frac{\alpha_{0}(1-\alpha_1)}{\sigma^{2}_{\zeta}}
\end{pmatrix}
\]
and sample $h_0$ from $\pi(h_{0}\mid h_{1},\alpha_1,\sigma^{2}_{\zeta})$.
\item Sample $\alpha_0,\alpha_1,\sigma^2_\zeta$. In C, it is possible to draw the parameters jointly from $\pi(\alpha_0,\alpha_1,\sigma^2_\zeta\mid h_{1:T})$ using a single Metropolis-Hastings step. Let $\rho=(1-\alpha_1)\alpha_0$ and $\bb{\psi}=(\alpha_0,\alpha_1,\sigma^2_\zeta)$, the proposal density can be wisely chose as
\[
\pi_{aux}(\bb{\psi}^{(\text{new})}\mid h_{0:T})=\pi_{aux}(\rho^{(\text{new})},\alpha_1^{(\text{new})}\mid h_{0:T},{\sigma^{2}_{\zeta}}^{(\text{new})})\pi_{aux}({\sigma^{2}_{\zeta}}^{(\text{new})}\mid h_{0:T})
\]
and $\pi_{aux}(\sigma^2_{\zeta})\propto \sigma^{-1}_{\zeta}$ and $\pi_{aux}(\rho,\alpha_1\mid \sigma^{2}_{\zeta})\sim \mathcal{N}(\bb{0},\sigma^{2}\bb{B}_{0})$ with $\bb{B}_{0}=diag(B_{0}^{11},B_{0}^{22})$. Therefore, the posteriors are 
\[
\rho,\alpha_1\mid h_{0:T},\sigma^{2}_{\zeta}\sim\mathcal{N}(\bb{b}_{T},\sigma^{2}_{\zeta}\bb{B}_{T})
\]
with $\bb{B}_{T}=(\bb{H}'\bb{H}+\bb{B}_{0}^{-1})^{-1}$ and $\bb{b}_{T}=\bb{B}_{T}\bb{H}'h_{1:T}'$, where $\bb{H}$ is a $T \times 2$ matrix $\bb{H}=(\bb{1}_{T}',h_{0:T-1}')$, and 
\[
\sigma^{2}_{\zeta}\mid h_{0:T}\sim \mathcal{IG}(c_{T},C_{T})
\]
with $c_{T}=(T-1)/2$ and $C_T=\frac{1}{2}(\sum_{t=1}^{T}h_{t}^{2}-\bb{b}_{T}'\bb{H}'h_{1:T}')$. The acceptance probability is $\min(1,R)$ where
\[
R = \frac{\pi(h_0\mid \bb\psi^{\text{(new)}})\pi(\rho^{\text{(new)}}\mid \alpha_1^{\text{(new)}})\pi({\sigma^{2}_{\zeta}}^{\text{(new)}})}{\pi(h_0\mid \bb\psi^{\text{(old)}})\pi(\rho^{\text{(old)}}\mid \alpha_1^{\text{(old)}})\pi({\sigma^{2}_{\zeta}}^{\text{(old)}})} \times \frac{\pi_{aux}(\alpha_1^{\text{(old)}},\rho^{\text{(old)}}\mid {\sigma^{2}_{\zeta}}^{\text{(old)}})}{\pi_{aux}(\alpha_1^{\text{(new)}},\rho^{\text{(new)}}\mid {\sigma^{2}_{\zeta}}^{\text{(new)}})}.
\]

Alternatively, one can use a 2-block sampler that draw from $\pi(\sigma^2_\zeta\mid h_{1:T},\alpha_0,\alpha_1)$ and  $\pi(\alpha_0,\alpha_1\mid h_{1:T},\sigma^2_\zeta)$, or sample them individually from their full conditional distributions. 
\item Shift to NC using the transformation $\tilde{h}_{t}=(h_{t}-\alpha_0)/\alpha_1$ for $t=1,...,T$.
\item Draw again $\alpha_0,\alpha_1,\sigma^2_\zeta$. In NC, use Metropolis-Hastings for sampling from $\pi(\alpha_1\mid\tilde{h}_{1:T})$ and then sample $\alpha_0$ and $\sigma^2_\zeta$ jointly from $\pi(\alpha_0,\sigma^2_\zeta\mid r_{1:T},\tilde{h}_{1:T},i_{1:T})$. In details, to sample $\alpha_1$, which is the only parameter in the state equation, one can use an improper auxiliary prior $\pi_{aux}(\alpha_1)\propto c$, with posterior
\[
\alpha_1\mid\tilde{h}_{0:T}\sim\mathcal{N}\bigg(\frac{\sum_{t=0}^{T-1}\tilde{h}_{t}\tilde{h}_{t+1}}{\sum_{t=0}^{T-1}\tilde{h}_{t}^{2}},\frac{1}{\sum_{t=0}^{T-1}\tilde{h}_{t}^{2}}\bigg)
\]
and acceptance probability $\min(1,R)$, where 
\[
R = \pi(\tilde{h}_{0}\mid\alpha_{1}^{(\text{new})})\pi(\alpha_1^{(\text{new})})/\pi(\tilde{h}_{0}\mid \alpha_1^{(\text{old})})\pi(\alpha_1^{\text{(old)}}).
\]
Then, samples of $\alpha_0$ and $\sigma^2_\zeta$ are obtained starting from the regression model
\[
\hat{\bb{r}}=\bb{H}\begin{pmatrix}\alpha_0 \\ \sigma^2_\zeta \end{pmatrix} + \bb{\epsilon}, 
\]
where $\bb{\epsilon}\sim \mathcal{N}(\bb{0},\bb{I}_{T})$, 
\[
\hat{\bb{r}} = \begin{pmatrix} 
(\tilde{r}_{1}-m_{i_1})/s_{i_1} \\ 
\vdots \\
(\tilde{r}_{T}-m_{i_T})/s_{i_T}
\end{pmatrix}
\]
and 
\[
\bb{H} = \begin{pmatrix} 
\tilde{h}_{1}/s_{i_1} &  1/s_{i_1} \\ 
\vdots \\
\tilde{h}_{T}/s_{i_T} &  1/s_{i_T}
\end{pmatrix}.
\]

The posterior distribution $\pi(\alpha_0,\sigma^{2}_{\zeta}\mid r_{1:T},\tilde{h}_{0:T},i_{1:T})$ is thus $\mathcal{N}(\bb{b}_{T},\bb{B}_{T})$ with $\bb{B}_{T}=(\bb{H}'\bb{H}+\bb{B}_{0}^{-1})^{-1}$ and $\bb{b}_{T}=\bb{B}_{T}(\bb{B}^{-1}_{0}\bb{\beta}_{0}+\bb{H}'\bb{r})$, where $\bb{b}_{0}=(b_{\alpha_0},0)'$ and $\bb{B}_{0}=diag(B_{\alpha_0},B_{\sigma_\zeta})$.\
Alternatively, one can sample $(\alpha_0,\sigma^2_\zeta)$ individually form $\pi(\alpha_0\mid r_{1:T},\tilde{h}_{1:T},i_{1:T},\sigma^2_\zeta)$ and $\pi(\sigma^2_\zeta\mid r_{1:T},\tilde{h}_{1:T},i_{1:T},\alpha_0)$.
\item Return to C by computing $h_{t}=\alpha_{0}+\sigma_\zeta \tilde{h}_t$ for $t=1,...,T$
\item Draw $i_t$ from $\pi(i_{t}\mid r_{1:T},h_{1:T})$ in C. Given that $\tilde{r}_{t}-h_{t}=\epsilon^{*}_{t}$, with $\epsilon^{*}_{t}\sim\mathcal{N}(m_{i_t},s^{2}_{i_t})$, then the posterior probability of $i_{1:T}\mid r_{1:T},h_{0:T}$ is 
\[
\mathbb{P}(i_{t}=k|r_{1:T},h_{0:T}) \propto \mathbb{P}(i_t=k)\frac{1}{s_k}\exp\bigg\{-\frac{(\epsilon^{*}-m_{k})^2}{2s_{k}^2}\bigg\}
\]
for $k \in \{1,...,10\}$ and $t \in \{1,...,T\}$, where $\mathbb{P}(i_t=k)$ are the mixture weights of the $k$th component.
\end{enumerate}
So far, we described the procedure starting in C, however it is also possible to start in NC, then move to C and finally return back to NC. For more details on the AWOL-ASIS strategy, the reference is @KASTNER2014408.


  \subsection{Speeding up MCMC with a precision sampler}\label{Sparse Matrix}
  
In order to speed up the Dynamic SSVS, we also propose an alternative estimation and simulation technique for the regression coefficients. As already mentioned in Chapter 1, the computational complexity of the Kalman filter is linear in the length of the data and quadratic in the dimension of the state vector. This turns out to be a downside in the context we are considering which is characterized by several latent processes, as many as the predictors inside the model. The AWOL method to sample from the posterior distributions of the SV model allows to slightly reduce the running time, however the major computational effort comes from the FFBS step for the regression coefficients. Therefore, we decide to try a different approach for sampling the $\bb{\beta}_{0:T}$, namely the precision sampler of @chan_jeliazkov_2009. The latter is based on sparse matrix routines and it has been developed for multivariate time series. We describe it using the notation of the authors which will be resumed in Chapter 4. In particular we follow @Chan_2018. In fact, the Gibbs sampler proposed here extends the original precision sampler, in taking into account the dynamic shrinkage expressed through the DSS process prior. Let $\boldsymbol{y} = (\boldsymbol{y}_{1}',...,\boldsymbol{y}_{T}')'$, where $\boldsymbol{y}_{t}=(y_{t,1},...,y_{t,n})'$, and $\boldsymbol{\theta} = (\boldsymbol{\theta}_{1}',...,\boldsymbol{\theta}_{T}')'$, where in the TVP Regression model $\boldsymbol{\theta}=\boldsymbol{\beta}$ and $\bb{\theta}_{t}=(\theta_{t,1},...,\theta_{t,p})'$
The Gibbs sampling follows the scheme \ref{alg:sparsemat}.
\begingroup
\LinesNumbered
\begin{algorithm}[t]
\caption{Dynamic Shrinkage in the Precision Sampler of Chan and Jeliazkov (2009) } \label{alg:sparsemat}
 Sample from $\pi(\boldsymbol{\theta}|\boldsymbol{y},\boldsymbol{h},\boldsymbol{\gamma},\boldsymbol{\theta}_{0})$ \;
 Sample from $\pi(\boldsymbol{\theta}_{0}|\boldsymbol{y},\boldsymbol{h},\boldsymbol{\gamma},\boldsymbol{\theta})$ \;
 Sample individually and independently from $\pi(\gamma_{j,t}|\boldsymbol{y},\boldsymbol{h},\boldsymbol{\theta},\boldsymbol{\theta}_{0},\gamma_{-j,t})$ \;
 Compute $\boldsymbol{\Lambda}=diag\{\boldsymbol{\gamma}\lambda_{1}+(\boldsymbol{1}-\boldsymbol{\gamma})\lambda_{0}\}$\;
 Compute $\boldsymbol{D}$ as in equation (\ref{eq:D})\;
 Sample from $\pi(\boldsymbol{h}|\boldsymbol{y},\boldsymbol{\theta},\boldsymbol{\gamma},\boldsymbol{\theta}_{0},\bb{\Psi})$ \;
\end{algorithm}
\endgroup
Note: in Algorithm \ref{alg:sparsemat}, the sampling of $\bb{\Psi}=\{\alpha_{0,i},\alpha_{1,i},\sigma^{2}_{\zeta,i}\}_{i=1}^{n}$ is implicit, but it consists of the same steps described in Section \ref{A new proposal for the volatility process}. Because of the large number of parameters to be estimated in multivariate time series, it seems a reasonable simplifying assumption to use a random walk for the volatility, i.e. $\bb{h}_{t}=\bb{h}_{t-1}+\bb{\zeta}_{t}$ with $\bb{\zeta}_{t}\sim\mathcal{N}(\bb{0},\Sigma_{\zeta})$ and $\Sigma_{\zeta}=diag\{\sigma^{2}_{i}\}_{i=1}^{n}$. Let's focus on each step.
\begin{itemize}
\item Step 1; Let us recall the state space representation of a TVP-VAR model 
\[\underset{(T \times n)\times1}{\boldsymbol{y}}=\underset{(T\times n)\times p}{\boldsymbol{X}}\ \ \underset{p\times 1}{\boldsymbol{\theta}} + \underset{(T \times n)\times1}{\boldsymbol{\epsilon}}, \ \ \ \boldsymbol{\epsilon} \sim \mathcal{N}\bigg(\underset{(T \times n)\times1}{\boldsymbol{0}},\underset{(T \times n)\times (T\times n)}{\boldsymbol{\Sigma}}\bigg)\]
where $\boldsymbol{\epsilon} = (\boldsymbol{\epsilon}_{1}',...,\boldsymbol{\epsilon}_{T}')'$, $\boldsymbol{\Sigma} = diag(\Sigma_{\epsilon,1},...,\Sigma_{\epsilon,T})$ and $\boldsymbol{X} = diag(\boldsymbol{X}_{1},...,\boldsymbol{X}_{T})$, and the latent process of the stochastic coefficients evolves as 
\[\boldsymbol{\theta}_{t}=\boldsymbol{G}_{t}\boldsymbol{\theta}_{t-1}+\boldsymbol{\eta}_{t}, \ \ \ \boldsymbol{\eta}_{t} \sim \mathcal{N}(\boldsymbol{0},\Lambda_{t})\]
where in the DSS scheme $\boldsymbol{G}_{t}=diag\{\gamma_{j,t}\phi_{1}\}_{j=1}^{p}$ and $\Lambda_{t}=diag\{\gamma_{j,t}\lambda_{1}+(1-\gamma_{j,t})\lambda_{0}\}_{j=1}^{p}$. \
Define the matrix 
\begin{equation} \label{eq:D}
\boldsymbol{D} = \begin{pmatrix}
\boldsymbol{I}_{k} & 0 & ... & 0\\
-\boldsymbol{G}_{1} & \boldsymbol{I}_{k} & ... & 0\\
... & ... & ... & 0 \\
0 & ... & -\boldsymbol{G}_{T} & \boldsymbol{I}_{k}
\end{pmatrix}
\end{equation}
Therefore we can write
\[  \boldsymbol{D}\boldsymbol{\theta} = \boldsymbol{\tilde{\alpha}}_{0}+\boldsymbol{\xi}, \ \ \ \boldsymbol{\xi} \sim \mathcal{N}(\boldsymbol{0},\boldsymbol{S_\theta}) \]
where $\boldsymbol{\tilde{\alpha}}_{0}=(\boldsymbol{\theta}'_{0},\boldsymbol{0},...\boldsymbol{0})'$ and $\boldsymbol{S_\theta}=diag(\Lambda_{1},...,\Lambda_{T})$.
Equivalently we write
\[\boldsymbol{\theta} | \boldsymbol{\theta}_{0},\boldsymbol{\gamma} \sim \mathcal{N}(\boldsymbol{D}^{-1}\boldsymbol{\tilde{\alpha}}_{0},(\boldsymbol{D}'\boldsymbol{S_\theta}^{-1}\boldsymbol{D})^{-1})\]
and we label $\bb{\alpha}_{0}=\boldsymbol{D}^{-1}\boldsymbol{\tilde{\alpha}}_{0}$.
Thanks to Corollary 8.1 of Theorem 8.1 of Kroese and Chan (2014), that we mentioned in Section \ref{Bayesian Inference in Linear Regression}, we can sample from 
\[\boldsymbol{\theta} | \boldsymbol{y},\boldsymbol{h},\boldsymbol{\gamma},\boldsymbol{\theta}_{0},\boldsymbol{h}_{0} \sim \mathcal{N}(\hat{\boldsymbol{\theta}},\boldsymbol{K}^{-1}_{\boldsymbol{\theta}})\]
where $\hat{\boldsymbol{\theta}}=\boldsymbol{K}^{-1}_{\boldsymbol{\theta}}\boldsymbol{d}_{\boldsymbol{\theta}}$, $\boldsymbol{K_\theta}=\boldsymbol{D}'\boldsymbol{S_\theta}^{-1}\boldsymbol{D}+\boldsymbol{X}'\boldsymbol{\Sigma_\epsilon}^{-1}\boldsymbol{X}$ and $\boldsymbol{d_\theta}=\boldsymbol{D}'\boldsymbol{S_\theta}^{-1}\boldsymbol{D}\boldsymbol{\alpha}_{0}+\boldsymbol{X}'\boldsymbol{\Sigma_{\epsilon}}^{-1}\boldsymbol{y}$.
\item Step 2; Sample $\boldsymbol{\theta}_{0}$ from the full conditional distribution 
\[
(\boldsymbol{\theta}_{0}|\boldsymbol{y},\boldsymbol{\theta},\boldsymbol{h},\boldsymbol{\Lambda}_{0})\sim\mathcal{N}(\hat{\boldsymbol{\theta}_{0}},\boldsymbol{K_{\theta_{0}}}^{-1})
\]
where $\boldsymbol{K_{\theta_{0}}}=\bb{C}_{0}^{-1}+\Lambda^{-1}_{0}$ and $\hat{\boldsymbol{\theta}_{0}}=\boldsymbol{K_{\theta_{0}}}^{-1}(\bb{C}_{0}^{-1}\bb{m}_{0}+\Lambda_{0}^{-1}\boldsymbol{\theta}_{1})$, with $\bb{m}_{0}$ and $C_{0}$ respectively the prior mean and the prior variance of the state vector.
\item Step 3; Sample individually and independently the indicators $\gamma_{j,t}$ from their full conditional distribution as in Step 2 of Algorithm \ref{alg:DSSVS-original}.
\item Step 6; Compute $\bb{r}=\bb{y}-\bb{X}\bb{\theta}$ and use the residuals $\bb{r}_{i}=(r_{1,i},...,r_{T,i})'$ (for $i=1,...,n$) to perform the AWOL-ASIS strategy of Kastner (2016).
\end{itemize}
In alternative to @KASTNER2014408, the authors of the precision sampler use the computational strategy of @KSC_1998. In this case it is also necessary to sample $\boldsymbol{h}_{0}$ from the full conditional distribution 
\[
(\boldsymbol{h}_{0}|\boldsymbol{y},\boldsymbol{h},\boldsymbol{\theta},\boldsymbol{\Lambda}_{0},\Sigma_{\zeta})\sim\mathcal{N}(\hat{\boldsymbol{h}_{0}},\boldsymbol{K_{h_{0}}}^{-1})
\]
where $\boldsymbol{K_{h_{0}}}=\boldsymbol{V}^{-1}_{h}+\Sigma_{\zeta}^{-1}$ and $\hat{\boldsymbol{h}_{0}}=\boldsymbol{K_{h_{0}}}^{-1}(\boldsymbol{V}^{-1}_{h}\boldsymbol{a}_h+\Sigma_{\zeta}^{-1}\boldsymbol{h}_{1})$, and $\bb{a}_{h}$ and $\bb{V}_{h}$ are the prior mean and variance of the initial volatility state.
  
  \section{Dynamic Expectation-Maximization Variable Selection}\label{Dynamic EMVS}
  
A valuable alternative to simulation algorithms come from @rockova_mcalinn_2021. They propose an innovative optimization approach to variable selection based on an Expectation-Maximization (EM) algorithm which is labelled by the authors Dynamic Expectation-Maximization Variable Selection or, more briefly, Dynamic EMVS. This method aims at providing the Maximum A Posteriori (MAP) estimate $\bb{\hat{\beta}}_{1:T}=\arg\max\pi(\bb{\beta}_{1:T}|y_{1:T})$ by reducing this challenging maximization problem into a sequence of simpler maximization steps. Each iteration is indeed decomposed into two stages. In the first step, known as E-Step, the conditional expectations of the unknown model's parameters is computed given the observations and the other parameters of interest. The second step, or M-step, consists in maximizing the joint conditional expectation of the model's parameters given the observations, where the other unknown model's parameters are treated as missing values and are replaced by their conditional expectation. In our specific case, these two passages become:
\begin{center}
\begin{itemize}
\item \emph{E-step:} Compute $\E(\bb{\gamma}_{0:T},\sigma^2_{\epsilon,1:T}\mid \bb{\beta}^{(m)}_{0:T},y_{1:T})$, where $\bb{\beta}_{0:T}^{(m)}$ indicates the regression coefficients sequence at the $m^{th}$ iteration.
\item \emph{M-step:} Maximize $\E_{\bb{\gamma}_{0:T},\sigma^2_{\epsilon,1:T} \mid \cdot }\log\pi(\bb{\beta}_{0:T},\bb{\gamma}_{0:T},\sigma^{2}_{\epsilon,1:T}\mid y_{1:T})$ with respect to $\bb{\beta}_{0:T}$.
\end{itemize}
\end{center}
In details, since $\bb{\gamma}_{0:T}$ and $\sigma^2_{\epsilon,1:T}$ are conditionally independent, we compute 
$\E(\bb{\gamma}_{0:T}\mid\bb{\beta}_{0:T},y_{1:T})$ and $\E(\sigma^2_{\epsilon,1:T}\mid\bb{\beta}_{0:T},y_{1:T})$. The first boils down in computing $p^{\star}_{t,j} = \mathbb{P}(\gamma_{t,j}=1 \mid \beta_{t,j}^{(m)},\beta_{t-1,j}^{(m)},\Omega)$ for $t=0,...,T$ and $j=1,...,p$. The latter depends on the specification of the variances used in the model. For example, considering the discount factor model for the volatility of equation (\ref{eq:discountfactor}), it can be shown [@WH_1997] that
\[
\E(\nu_{t}\mid\bb{\beta}^{(m)}_{0:T},y_{1:T})=(1-\delta)n_{t}/d_{t}+\delta\E(\nu_{t+1}\mid\bb{\beta}^{(m)}_{0:T},y_{1:T})
\]
for $1\leq t <T$, and $\E(\nu_{t}\mid\bb{\beta}^{(m)}_{0:T},y_{1:T})=n_{T}/d_{T}$. On the other hand, the non-linearity introduced by specifying the volatility as a log-Normal AR(1) process does not allow to obtain first moments in closed form. For this reason, we developed a novel approach characterized by particle filtering and smoothing in the E-step. At the cost of a slight increase in the running time, the new algorithm provides with better volatility estimates. More details of this methods are provided in the next section.
\
The M-step requires some considerations too. Here, we describe the Gaussian Spike-and-Slab case; for the Laplace case we refer to @rockova_mcalinn_2021. The initial state vector, $\bb{\beta}_{0}$, is estimated with the whole state sequence $\bb{\beta}_{1:T}$ and, according to the DSS specification, it is assumed to have a stationary distribution
\begin{equation}
\pi(\bb{\beta}_{0}|\bb{\gamma}_{0})=\prod_{j=1}^{p}[\gamma_{0,j}\psi_{1}^{ST}(\beta_{0,j}|\lambda_{1},\phi_{0},\phi_{1})+(1-\gamma_{0,j})\psi_{0}(\beta_{0,j}|\lambda_{0})]
\end{equation}
where $\gamma_{0,j}| \Omega\sim Bernoulli(\Omega)$ for $j=1,...,p$. Notice that the Markov structure of the models parameters and their independence allow the following factorization
\begin{equation}
\pi(\bb{\beta}_{0:T},\bb{\gamma}_{0:T},\sigma^2_{\epsilon,1:T})=\pi(\bb{\beta}_{0}|\bb{\gamma}_{0})\pi(\bb{\gamma}_{0})\prod_{t=1}^{T}\bigg[\pi(\sigma^2_{\epsilon,t}|\sigma^2_{\epsilon,t-1})\prod_{j=1}^{p}\pi(\beta_{t,j}|\gamma_{t,j},\beta_{t-1,j})\pi(\gamma_{t,j}|\beta_{t-1,j})\bigg]
\end{equation}
Then we can write 
\begin{equation}
   \begin{aligned}\label{eq:objfun}
 & \log \pi(\bb{\beta}_{0:T},\bb{\gamma}_{0:T},\sigma^2_{\epsilon,1:T}|y_{1:T})
 \\  & = C+\sum_{t=1}^{T}\sum_{j=1}^{p}[\gamma_{t,j}\log \theta_{t,j}+(1-\gamma_{t,j})\log(1-\theta_{t,j})] 
\\ & - \sum_{t=1}^{T}\bigg\{\frac{(y_{t}-\bb{x}_{t}'\bb{\beta}_{t})^{2}}{2\sigma^{2}_{\epsilon,t}}+\sum_{j=1}^{p}\bigg[\gamma_{t,j}\frac{(\beta_{t,j}-\phi_{1}\beta_{t-1,j})^{2}}{2\lambda_{1}}+(1-\gamma_{t,j})\frac{\beta_{t,j}^{2}}{2\lambda_{0}}\bigg]+\log\pi(\sigma^2_{\epsilon,t}|\sigma^2_{\epsilon,t-1})\bigg\} 
\\ & - \sum_{j=1}^{p}\bigg\{\gamma_{0,j}\frac{\beta_{0,j}^{2}(1-\phi_{1}^{2})}{2\lambda_{1}}+(1-\gamma_{0,j})\frac{\beta_{0,j}^{2}}{2\lambda_{0}}-\gamma_{0,j}\log\Omega-(1-\gamma_{0,j})\log(1-\Omega)\bigg\}
\end{aligned}
\end{equation}
where $C$ incorporates all the constant components. Let $\nu_t = \frac{1}{\sigma^{2}_{\epsilon,t}}$, the objective function we want maximize in the M-step is $\E_{\bb{\gamma}_{0:T},\nu_{1:T} \mid \cdot }\log\pi(\bb{\beta}_{0:T},\bb{\gamma}_{0:T},\nu_{1:T}\mid y_{1:T})=Q(\bb{\Xi}\mid y_{1:T})$. Therefore, we replace $(\bb{\gamma}_{0:T},\nu_{1:T})$ in equation (\ref{eq:objfun}) with their conditional expectations $(\bb{p}^{\star}_{0:T},\nu^{\star}_{1:T})$ computed in the E-step. Then, we find the maximum of $Q(\bb{\Xi}\mid y_{1:T})$ with respect to $\bb{\beta}_{0:T}$. Below we report the first order conditions:
\begin{align*}
\frac{\partial Q(\bb{\Xi} \mid y_{1:t})}{\partial \bb{\beta}_{t}} = 0 & \iff \bb{\beta}_{t} = \bb{D}_{t}^{-1}\bigg\{\nu_{t}^{\star}y_{t}\bb{x}_{t}+\frac{\phi_1}{\lambda_1}\bb{\beta}_{t-1}\odot\bb{p}^{\star}_{t}+\frac{\phi_1}{\lambda_{1}}\bb{\beta}_{t+1}\odot\bb{p}^{\star}_{t+1}\bigg\}\\
\frac{\partial Q(\bb{\Xi} \mid y_{1:t})}{\partial \bb{\beta}_{0}} = 0 & \iff
\bb{\beta}_{0}=\bb{D}_{0}^{-1}\frac{\phi_1}{\lambda_1}\bb{\beta_1}\odot\bb{p_{1}^{\star}}
\end{align*}
where
\[
\bb{D}_t=\nu_t^{\star}\bb{x}_{t}\bb{x}_{t}'+diag\bigg\{\frac{p^{\star}_{t,j}}{\lambda_1}+\frac{1-p^{\star}_{t,j}}{\lambda_{0}}+\frac{\phi^{2}_{1}p^{\star}_{t+1,j}}{\lambda_{1}}\bigg\}_{j=1}^{p}
\]
and 
\[
\bb{D}_0=diag\bigg\{\frac{(1-\phi^2_1)p^{\star}_{0,j}}{\lambda_1}+\frac{1-p^{\star}_{0,j}}{\lambda_0}+\frac{\phi^{2}_1 p^{\star}_{1,j}}{\lambda_1}\bigg\}_{j=1}^{p}.
\]
The notation $\odot$ stands for the element-wise vector multiplication.\
The passages so far illustrated are resumed in algorithm (\ref{alg:DEMVS}). Note that the inversion of $\bb{D}_{t}$ is facilitated by the Woodburry formula [@Woodburry]. \
Labelling $\Delta = diag\bigg\{\frac{p^{\star}_{t,j}}{\lambda_1}+\frac{1-p^{\star}_{t,j}}{\lambda_{0}}+\frac{\phi^{2}_{1}p^{\star}_{t+1,j}}{\lambda_{1}}\bigg\}_{j=1}^{p}$, then it yields
\[
\bb{D}^{-1}_{t}=\Delta^{-1}_{t}-\nu_{t}^{\star}\Delta^{-1}_{t}\frac{\bb{x}_{t}\bb{x}_{t}'}{1+\nu_{t}^{\star}\bb{x}_{t}'\Delta^{-1}_{t}\bb{x}_{t}}\Delta_{t}^{-1}
\]
Thanks to this shortcut, the overall running time of the algorithm reduces drastically.

\begin{algorithm}
\caption{Dynamic EMVS by Rockova and McAllin (2021)} \label{alg:DEMVS}
\nl \textbf{Initialize} $\beta_{t,j}$ for $t=0,...,T$ and $j=1,...,p$\;
\begin{center}
\emph{E-Step}\\
\end{center}
\nl \For{$j=1,...,p$}{
\For{$1 \leq t \leq T$}{
Compute $\omega_{t,j}=\omega(\beta_{t-1,j})$ from \;
Compute $p^{\star}_{t,j}=p^{\star}_{t,j}(\beta_{t,j})$ from \;
}
Compute $p^{\star}_{0,j}=\omega(\beta_{0,t})$ from \;
}
\nl \For{$t = 1,...,T$}{
Compute $n_{t}=\delta n_{t-1}+1$ and $d_{t}=\delta d_{t-1}+r^{2}_{t}$, where $r_{t}=y_{t}-\bb{x}_{t}'\bb{\beta}_{t}$\;
}
Set $\nu^{\star}_{T}=n_{T}/d_{T}$\;
\For{$t=T-1,...,0$}{
Set $\nu^{\star}_{t}=(1-\delta)n_{t}/d_{t}+\delta \nu^{\star}_{t+1}$\;
}
\begin{center}
\emph{M-Step}\\
\end{center}
\nl \For{$t=1,...,T$}{
Compute $\bb{D}_t=\nu_t^{\star}\bb{x}_{t}\bb{x}_{t}'+diag\bigg\{\frac{p^{\star}}{\lambda_1}+\frac{1-p^{\star}}{\lambda_{0}}+\mathbb{I}(t<T)\frac{\phi^{2}_{1}p^{\star}_{t+1,j}}{\lambda_{1}}\bigg\}_{j=1}^{p}$\;
Compute $\bb{\beta}_{t} = \bb{D}_{t}^{-1}\bigg\{\nu_{t}^{\star}y_{t}\bb{x}_{t}+\frac{\phi_1}{\lambda_1}\bb{\beta}_{t-1}\odot\bb{p}^{\star}_{t}+\mathbb{I}(t<T)\frac{\phi_1}{\lambda_{1}}\bb{\beta}_{t+1}\odot\bb{p}^{\star}_{t+1}\bigg\}$\;
Compute $\bb{D}_0=diag\bigg\{\frac{(1-\phi^2_1)p^{\star}_{0,j}}{\lambda_1}+\frac{1-p^{\star}_{0,j}}{\lambda_0}+\frac{\phi^{2}_1 p^{\star}_{1,j}}{\lambda_1}\bigg\}_{j=1}^{p}$\;
Compute $\bb{\beta}_{0}=\bb{D}_{0}^{-1}\frac{\phi_1}{\lambda_1}\bb{\beta_1}\odot\bb{p}_{1}^{\star}$\;
}
\end{algorithm}

\subsection{Particle Smoothing for Dynamic EMVS}

For the particle smoothing strategy implemented in the E-step we refer to the scheme proposed by @GDW_2004. In this section, we describe the latter with reference to model (\ref{eq:setup}). Therefore, a stochastic volatility model is assumed for the regression residuals $r_{t}=y_{t}-\bb{x}_{t}'\bb{\beta}_{t}$ for $t=1,\ldots,T$.\
The markovian structure of the stochastic volatility model allows the following factorization
\begin{equation}
\begin{aligned}\label{eq:psmooth}
\pi(h_{t}\mid h_{t+1:T},r_{1:T},\bb{\psi}) = & \pi(h_{t}\mid h_{t+1},r_{1:T},\bb{\psi}) \\ 
\propto & \pi(h_{t}\mid r_{1:T},\bb{\psi})\pi(h_{t+1}\mid h_{t},\bb{\psi})
\end{aligned}
\end{equation}
where $\bb{\psi}$ indicates a vector containing other unknown model's parameters. In equation (\ref{eq:psmooth}), $\pi(h_{t}\mid r_{1:T},\bb{\psi})$ can be approximated by the empirical distribution computed using a particle filter. In our implementation, for example, we use a Bootstrap Particle Filter [@CP_2020]. Consequently, $\pi(h_{t}\mid h_{t+1:T},r_{1:T},\bb{\psi})$ can be approximated by the empirical distribution
\[ \pi(h_{t}\mid h_{t+1:T},r_{1:T},\bb{\psi}) \approx \sum_{i=1}^{N}w_{t\mid t+1}^{(i)}\delta_{h_{t}^{(i)}}
\]
with modified weights
\[
w_{t\mid t+1}^{(i)}=\frac{w_{t}^{(i)}\pi(h_{t+1}\mid h_{t}^{(i)},\bb{\psi})}{\sum_{j=1}^{N}w_{t}^{(j)}\pi(h_{t+1}\mid h_{t}^{(j)},\bb{\psi})}.
\]

For practical reasons, we decided to implement a Bootstrap Particle Filter where the parameters $(\alpha_1,\alpha_2,\sigma^2_{\zeta})$ are known. The whole filtering and smoothing process used for the applications of this thesis is presented below. 
\begin{itemize}
\item Generate $N$ particle $(h_{0}^{(1)},...,h_{0}^{(N)})$ from $\mathcal{N}(\alpha_{0},\sigma_{\zeta}^{2}/(1-\alpha_{1}^{2}))$ and $N$ weights such that $w_{0}^{(i)}=N^{-1}$ for $i=1,...,N$
\item For $t=1,...,T$:
\begin{enumerate}
\item Draw $h_{t}^{(i)} \sim \mathcal{N}(\alpha_0+\alpha_1 h_{t-1}^{(i)},\sigma^2_\eta) \ \ i=1,...,N$
\item Set $\tilde{w}_{t}^{(i)} = w_{t-1}^{(i)}\;\mathcal{N}(r_{t};0,e^{h^{(i)}_{t}}) \ \ i=1,...,N$
\item Normalize the weights: $w_{t}^{(i)}=\frac{\tilde{w}_{t}^{(i)}}{\sum_{j=1}^{N}(\tilde{w}_{t}^{(j)})}$
\item Compute the effective sample size (ess): $ess=\Bigg(\sum_{i=1}^{N}(w_{t}^{(i)})^{2}\Bigg)^{-1}$
\item if $ess<N/2$ then
\begin{enumerate}
\item Draw a sample of size $N$, $(h_{t}^{(1)},...,h_{t}^{(N)})$, from the discrete distribution $\mathbb{P}(h_{t}=h_{t}^{(i)})=w_{t}^{(i)},\ \ i=1,...,N$
\item Reset the weights: $w_{t}^{(i)}=N^{-1}$, $i=1,...,N$.
\end{enumerate}
\end{enumerate}
\item For $t=T-1,...,0$
\begin{enumerate}
\item Set $w_{t|t+1}^{(i)}\propto w_{t}\;\mathcal{N}(\tilde{h}_{t+1}|h_{t}^{(i)}), \ \ i=1,...,N$
\item Draw a sample of size $N$, $(\tilde{h}_{t}^{(1)},...,\tilde{h}_{t}^{(N)})$, from the discrete distribution $\mathbb{P}(\tilde{h}_{t}=h_{t}^{(i)})=w_{t|t+1}^{(i)},\ \ i=1,...,N$
\end{enumerate}
\end{itemize}
The $ess$-based resampling step is meant to avoid the degeneracy of the particles that may occurs in a Sequential Importance Sampling. In addition, it allows to save computational time by resampling only whether necessary. In alternative to the Bootstrap Particle Filter we propose, it would be possible to adopt the Liu and West filter [@LW_2001] which is a modified version of the filtering strategy described above that provides estimates also for the unknown SV model's parameter. This is made possible by resampling them over time from a continuous distribution, that in our case coincides with a Normal distribution for $\alpha_0$ and $\alpha_1$ and an Inverse Gamma distribution for $\sigma_{\eta}^{2}$. In this way, at every time, the support of the parameters is not limited to the $N$ initially sampled values. Since the resampling of the parameters involves additional computational efforts we decide not to take into account the @LW_2001 strategy here.


  \section{Simulation Study}\label{SD}
  
Synthetic data are used in order to assess the validity of the algorithms illustrated in the previous sections. A sequence of $T=144$ observations has been generated from model (\ref{eq:eq211}) with $p=50$ explanatory variables and $\sigma^{2}_{\epsilon,t}=0.25$ for $t=1,...,T$. Each $x_{t,j}$ is drawn from a standard Normal distribution. Only the first four predictors contribute to explaining $y_{1:T}$, whereas the remaining forty-six are uncorrelated to the outcome. Hence, the actual coefficients associated to relevant predictors, namely $\beta_{1:T,j}^{0}$ for $j=1,...,4$, are simulated from the $AR(1)$ process defined in equation (\ref{eq:betaevol}) with hyperparameters $\lambda_{1}=0.1$, $\phi_{0}=0$ and $\phi_{1}=0.98$. The rest of the coefficients are instead forced to zero at every time: $\beta_{1:T,5}^{0}=...=\beta_{1:T,50}^{0}=\bb{0}$. As in the simulation study presented in @rockova_mcalinn_2021, the values are rescaled and thresholded to zero 
if the absolute value of the process falls below $0.5$, resulting in zero-valued periods.
Below we report the results obtained by fitting the data using the Dynamic SSVS and Dynamic EMVS strategies illustrated in the previous paragraphs. Posterior sampling in the Dynamic SSVS is performed via MCMC with $N=1000$ iterations of which $200$ are burn-in. We force $\sigma_{\epsilon,t}^{2}= 0.25$ for the first ten iterations since we empirically notice that this simple trick avoids the degeneracy of the Markov Chain. Similarly, $N=1000$ iterations are used for the Dynamic EMVS.

The large number of predictors and the dynamic evolution of their relevance over time make it extremely challenging for traditional techniques to extrapolate the actual source of the signal. Evidence of this is provided in Figure \ref{fig:myfig2}, where data are fitted using a standard DLM without shrinkage. The latter can be easily obtained by fixing $\Omega=1$, which is in practice equivalent to set $\gamma_{t,j}=1$ $\forall \ t \in \{0,\ldots,T\}$, $\forall \ j \in \{1,\ldots,p\}$. As in @rockova_mcalinn_2021, the volatility process is estimated using a discount factor model with hyperparameters $n_{0}=10$, $d_{0}=10$ and $\delta=0.9$. The plots in Figure \ref{fig:myfig2} show that the noise induced by unnecessary covariates produces biased point estimates with high uncertainty which results in huge credible intervals. The latter are computed as
\[ \bigg[\E(\beta_{t,j}|y_{1:T})-z_{1-\alpha/2}\sqrt{\V(\beta_{t,j}|y_{1:T})} \ ,\ \E(\beta_{t,j}|y_{1:T})+z_{1-\alpha/2}\sqrt{\V(\beta_{t,j}|y_{1:T})}\bigg].
\]
```{r myfig2, fig.cap = "Dynamic SSVS with $\\Omega=1$ (no shrinkage) and discount factor model for the residuals; true values of $\\beta_{1:T, j}$, $j=1, \\ldots, 6$, (black) and smoothing estimates with 95 percent credible intervals (yellow) of the first six regression coefficients.", fig.align='center', fig.height=3.5, fig.width=7.5, echo=F, results=F,fig.pos='H'}
for(i in 1:6){
  load(paste0("C:/Users/Edoardo/Dropbox/elementi salvati R/plotdlm_",i,".Rda"))
}
ggarrange(plotdlm_1,plotdlm_2,plotdlm_3,plotdlm_4,plotdlm_5,plotdlm_6)
```

Sparsity is induced by lowering the value of $\Omega$. Indeed, Dynamic SSVS with $\Omega=0.2$ seems to lead to improved results. As shown in Figure \ref{fig:myfig3}, the new model is able to capture more features of the data. The smoothed values follow quite closely the true ones and they present smaller credible intervals. Results may eventually improve by choosing appropriate values for the hyperparameters $(\Omega,\lambda_{1},\lambda_0,n_{0},d_{0},\delta)$. In this specific case, we chose $\lambda_{0}=0.01$ and $\lambda_{1}=0.1$ in order to maintain a good ratio between the spike and slab variances and hence to facilitate the recognition of the active coefficients. Moreover, by setting $n_0=40$ and $d_0=10$ we want to express our prior belief of a small residual variance.

```{r myfig3, fig.cap = "Dynamic SSVS with $\\Omega=0.2$ and discount factor model for the residuals; true values of $\\beta_{1:T, j}$, $j=1, \\ldots, 6$, (black) and smoothing estimates with 95 percent credible intervals (yellow) of the first six regression coefficients.", fig.align='center', fig.height=3.5, fig.width=7.5, echo=F, results=F,fig.pos='H'}
for(i in 1:6){
  load(paste0("C:/Users/Edoardo/Dropbox/elementi salvati R/plotdf_",i,".Rda"))
}
ggarrange(plotdf_1,plotdf_2,plotdf_3,plotdf_4,plotdf_5,plotdf_6)
```

As anticipated at the end of Section \ref{Dynamic Stochastic Search Variable Selection}, estimates can benefit from switching from a discount factor model to a stochastic volatility model for the residual variances. We prove this statement by comparing the Dynamic SSVS with these two alternative specifications.  
Therefore, we replace the third step of Algorithm \ref{alg:DSSVS-original} with the simulation strategy for stochastic volatility models discussed in Section \ref{A new proposal for the volatility process}. The parameters' priors are $\alpha_0\sim\mathcal{N}(-10,100)$, $\alpha_2\sim\mathcal{B}(20,1.5)$ and $\sigma^2_{\zeta}\sim\mathcal{IG}(0.5,0.5)$, and we set the initialization values $h_0^{(0)}=-2$, $\alpha_0^{(0)}=-2$, $\alpha_1^{(0)}=0.9$ and $\sigma_{\zeta}^{(0)}=0.1$. Figure \ref{fig:myfig4} shows that there is an actual improvement when using a stochastic volatility model for the residual variances. Indeed, this model is less sensible to the "spike trap" we discussed in Section \ref{Dynamic Stochastic Search Variable Selection},
as shown in Figure \ref{fig:myfig5}. 

```{r myfig4, fig.cap = "Dynamic SSVS with $\\Omega=0.2$ and stochastic volatility model for the residuals; true values of $\\beta_{1:T, j}$, $j=1, \\ldots, 6$, (black) and smoothing estimates with 95 percent credible intervals (yellow) of the first six regression coefficients.", fig.align='center', fig.height=3.5, fig.width=7.5, echo=F, results=F,fig.pos='H'}
for(i in 1:6){
  load(paste0("C:/Users/Edoardo/Dropbox/elementi salvati R/plotsv_",i,".Rda"))
}
ggarrange(plotsv_1,plotsv_2,plotsv_3,plotsv_4,plotsv_5,plotsv_6)
```

```{r myfig5, fig.cap = "MCMC variances' estimates of the volatility process with: a discount factor model (left panel), stochastic volatility model in a FFBS scheme (central panel) and a precision sampler scheme (right panel).", fig.align='center', fig.height=2, fig.width=8, echo=F, results=F,      fig.pos='H'}
  load("C:/Users/Edoardo/Dropbox/elementi salvati R/Varplotsv.Rda")
  load("C:/Users/Edoardo/Dropbox/elementi salvati R/Varplotdf.Rda")
  load("C:/Users/Edoardo/Dropbox/elementi salvati R/plot_chan_v.Rda")
ggarrange(varplot.df,varplot.sv,plot_chan_v,nrow=1)
``` 

```{r myfig12, fig.cap = "MCMC approximation of the posterior inclusion probabilities of a Dynamic SSVS with: a discount factor model (blue line), stochastic volatility model in a FFBS scheme (green line) and a precision sampler scheme (red line). Black dotted line represents the true values.", fig.pos='H',fig.align='center', fig.height=3.5, fig.width=7.5, echo=F, results=F}
for(i in 1:6){
  load(paste0("C:/Users/Edoardo/Dropbox/elementi salvati R/plot_ind_",i,".Rda"))
}
ggarrange(plot_ind_1,plot_ind_2,plot_ind_3,plot_ind_4,plot_ind_5,plot_ind_6,common.legend = TRUE, legend="bottom")
``` 
The running time of the simulation algorithms represents a limit when these strategies have to be reiterated multiple times. This happens for instance when one-step-ahead forecasts are generated for many periods or, simply, when multiple estimations are performed in order to compare results obtained with diverse specifications of the hypereparameters. 
By replacing the FFBS step with a precision sampler, the running time reduces while preserving performances as shown in Table \ref{tab:mytab1}. However, the greatest enhancement in terms of running time comes with the Dynamic EMVS. Therefore, here we present the results obtained by fitting the data using a Dynamic EMVS both with a discount factor model and with a stochastic volatility model. The initialization value $\bb{\beta}_{t}^{(0)}$ coincides with the least squares estimates. Even though the original approach of @rockova_mcalinn_2021 does not involve least squares estimation for the initial values of the regression coefficients, we notice that this simple trick can improve convergence. 

```{r myfig9, fig.cap = "Dynamic EMVS with discount factor model for the residual; true values of $\\beta_{1:T, j}$, $j=1, \\ldots, 6$, (black) and Maximum A Posteriori estimates (green) of the first six regression coefficients", fig.align='center', fig.height=3.5, fig.width=7.5, echo=F, results=F,      fig.pos='H'}
for(i in 1:6){
  load(paste0("C:/Users/Edoardo/Dropbox/elementi salvati R/plotdemvsdf_",i,".Rda"))
}
ggarrange(plotdemvsdf_1,plotdemvsdf_2,plotdemvsdf_3,plotdemvsdf_4,plotdemvsdf_5,plotdemvsdf_6)
```
 
```{r myfig10, fig.cap = "Dynamic EMVS with stochastic volatility model for the residual; true values of $\\beta_{1:T,j}$, $j=1, \\ldots, 6$, (black) and Maximum A Posteriori estimates (green) of the first six regression coefficients ", fig.pos='H',fig.align='center', fig.height=3.5, fig.width=7.5, echo=F, results=F}
for(i in 1:6){
  load(paste0("C:/Users/Edoardo/Dropbox/elementi salvati R/plotdemvssv_",i,".Rda"))
}
ggarrange(plotdemvssv_1,plotdemvssv_2,plotdemvssv_3,plotdemvssv_4,plotdemvssv_5,plotdemvssv_6)
``` 

```{r myfig13, fig.cap = "Estimates of the conditional inclusion probabilities using Dynamic EMVS with: a discount factor model (blue line) and  a stochastic volatility model (green line). Black dotted line represents the true values.", fig.align='center', fig.height=3.5, fig.width=7.5, fig.pos='H',echo=F, results=F}
for(i in 1:6){
  load(paste0("C:/Users/Edoardo/Dropbox/elementi salvati R/plot_ind2_",i,".Rda"))
}
ggarrange(plot_ind2_1,plot_ind2_2,plot_ind2_3,plot_ind2_4,plot_ind2_5,plot_ind2_6)
```   

```{r myfig11, fig.cap = "Dynamic EMVS: variances' estimates of a stochastic volatility model (left panel) and discount factor model (right panel)", fig.align='center', fig.pos='H',fig.height=1.5, fig.width=7.5, echo=F, results=F}
  load("C:/Users/Edoardo/Dropbox/elementi salvati R/Varplotemvssv.Rda")
  load("C:/Users/Edoardo/Dropbox/elementi salvati R/Varplotemvsdf.Rda")
ggarrange(varplot.emvssv,varplot.emvsdf)
```  

Running times along with other meaningful information is presented in Tables \ref{tab:mytab1} and \ref{tab:mytab2}. The metrics here used are: the Sum of Squared Errors (SSE) between the actual and the estimated state sequence, the Hamming distance (Ham) between the actual and the estimated indicator vectors, the amount of total false negative (FN) and false positive (FP), and the number of false detection (FD) and false non detection (FND). In details, the Hamming Distance is a metrics used in information theory to measure the number of substitutions necessary to convert a string into another one. We use it to measure the distance between the indicator vector estimated and the true one. More formally, let $\pi_{t,j}\equiv \mathbb{P}(\gamma_{t,j}=1|y_{1:T})$ (or $\pi_{t,j}=p^{\star}_{t,j}$ in the Dynamic EMVS case) and $\Pi=(\pi_{t,j})$, then
\[
Ham(\Pi,\bb{\beta}_{0:T}^{0})=\sum_{j=1}^{p}\sum_{t=1}^{T}|\mathbb{I}(\pi_{t,j} \geq 0.5)-\mathbb{I}(\beta_{t,j}^{0}\neq 0)|
\]
False positives and false negatives stands for the number of times in which a coefficient is classified as relevant while it is not and vice versa. On the other hand, false detection indicates the number of noise covariates that are classified as relevant at least once over the whole period whereas false non detection count the number of relevant variables that have been ignored by the algorithm. The conclusion we draw from Tables \ref{tab:mytab1} and \ref{tab:mytab2} are the following. Firstly, without shrinkage ($\Omega=1$) the SSE is huge and the model is clearly unable to fit the data. Therefore, the introduction of a sparsity inducing strategy is necessary. Just by lowering $\Omega$ to 0.2 leads to great improvements. The Dynamic SSVS with discount factor model provides in general the worst performances in terms of SSE and it generates many false positive as a consequence of the spike trap. On the other hand, the Dynamic SSVS with stochastic volatility shows better performances. Moreover, replacing the FFBS with a precision sampler implies minor accuracy of point estimates of the very first observations. The large share of false positive reported in Table \ref{tab:mytab1} can be explained from the fact that the latter strategy assigns all the coefficient to the slab in the first periods (see Figure \ref{fig:myfig13}). However, estimates improve for more iterations of the Markov Chain. Overall, modelling variances as a log-Normal AR(1) process proves to return better estimates of both regression coefficients and the variance, for this reason we will consider this specification as the baseline for developing further models in the next chapters. In addition, the Dynamic EMVS represents a valid alternative to the simulation algorithms. It provides indeed precise point estimates at a reduced computational effort. However, one should consider the fact that the deterministic nature of this algorithm may result unsatisfactory whenever quantifying the uncertainty around the estimates is of interest. Moreover, it shows a tendency to produce false positives, therefore choosing the hyperparameters such that shrinkage is more severe is recommended. 

```{r mytab1, message=FALSE, warning=FALSE, fig.align="center", echo=FALSE}
load("C:/Users/Edoardo/Dropbox/elementi salvati R/TABLE1.Rda")
load("C:/Users/Edoardo/Dropbox/elementi salvati R/SMM_MAT.Rda")
rownames(TABLE.1)<-NULL
TABLE.1.1 = rbind(TABLE.1[1:3,],SMM_MAT,TABLE.1[4:5,])
TABLE.1.1 = round(TABLE.1.1,digits=3)
col.omegas = c(1,0.2,0.2,0.2,0.2,0.2)
col.speed = c(745.56,570.09,502.03,251.27,59.72,149.75)
col.sv = c("DF","DF","SV","SV","DF","SV")
#col.alg = c("SSVS","SSVS","SSVS","SSVS","EMVS","EMVS")
sample.str = c("FFBS","FFBS","FFBS","Precision Sampler","","")
TABLE.1.1 = cbind(sample.str,col.sv,col.omegas,col.speed,TABLE.1.1)

collapse_rows_dt <- data.frame(Model = c(rep("SSVS", 4), rep("EMVS", 2)),TABLE.1.1)

kbl(collapse_rows_dt, align="c", longtable = F, booktabs = T, caption="Performance comparison",digits=3,escape=F,col.names =c("Dynamic","Sampling Strategy","Volatility","$\\Omega$","Speed","SSE","Ham.","FP","FN","FD","FND"))%>%
  kable_paper(full_width = F) %>%
  column_spec(1, bold = F) %>%
  collapse_rows(columns = 1:2, valign = "top")%>%
kable_styling(latex_options = c("HOLD_position"),font_size = 10)
```

```{r mytab2, echo=FALSE, , echo=FALSE, results='asis'}
load("C:/Users/Edoardo/Dropbox/elementi salvati R/TABLE2.Rda")
load("C:/Users/Edoardo/Dropbox/elementi salvati R/SMM_MAT_2.Rda")
rownames(TABLE.2)<-NULL
TABLE.2.1 = rbind(TABLE.2[1:3,],SMM_MAT_2,TABLE.2[4:5,])
TABLE.2.1 = round(TABLE.2.1,digits=3)
col.omegas = c(1,0.2,0.2,0.2,0.2,0.2)
col.omegas = c(1,0.2,0.2,0.2,0.2,0.2)
col.sv = c("DF","DF","SV","SV","DF","SV")
sample.str = c("FFBS","FFBS","FFBS","Precision Sampler","","")
TABLE.2.2 = cbind(sample.str,col.sv,col.omegas,TABLE.2.1)

collapse_rows_dt <- data.frame(Model = c(rep("SSVS", 4), rep("EMVS", 2)),TABLE.2.2)

kbl(collapse_rows_dt, align="c", longtable = F, booktabs = T, caption="Assessing the estimation bias with focus on the first four predictors and on the remaining forty-six.",digits=3,escape=F,col.names =c("Dynamic","Sampling Strategy","Volatility","$\\Omega$","SSE","Ham.","SSE","Ham."))%>%
  kable_paper(full_width = F) %>%
  column_spec(1, bold = F) %>%
  collapse_rows(columns = 1:2, valign = "top")%>%
add_header_above(c(" "," "," "," ","Predictors 1 to 4" = 2, "Predictors 5 to 50" = 2)) %>%
kable_styling(latex_options = c("repeat_header","HOLD_position"),font_size = 10)

```
  
  
\section{Macroeconomic Data}

Inflation forecasting is probably the hardest challenge monetary policy authorities face. What makes it a difficult tasks is that the price growth can be explained by several factors, some of whom are not directly observable. In general this factors are divided in some macro-classes. Briefly, there are factors affecting demand which are mainly due to fiscal and monetary policies and that are usually related to the money supply, consumption, disposable income, public expenditure, consumer expenditure, deficit financing, repayment of the public dept and exports. Then there are factors affecting supply such as shortage of important production factors, industrial disputes, natural calamities, artificial scarcities and other international factors. In addition a crucial role is played by agents' expectations which can be only measured using proxies. In the current exercise, we decide to include all the variables potentially affecting inflation in a unified Time-Varying Parameter regression model. The approach is therefore more data-driven and less theory-based. The purpose of this analysis (and the following ones) is purely illustrative and it contains several simplifications. Nonetheless, it also provides some good suggestions for further, more accurate, economic analysis. Data on the variables involved in this exercise are sourced from a large macroeconomic database maintained by @FREDMD. The latter is primarily based on Federal Reserve Economic Data (FRED) \footnote{url: https://fred.stlouisfed.org/} and it contains seasonally adjusted quarterly time series. Data are standardized and made stationary through log-difference, with the exception of interests rates. This transformations are in line with the instructions provided by the authors of the database. The list of the $40$ explanatory variables involved in the analysis is reported in Appendix.
The series we are interested to forecast is the quarterly Price Consumption Expenditure Price Index, which is commonly used as an indicator for inflation. The model we estimate is
\begin{align*}
y_{t+l}= & \mu_t+\bb{x}_{t}'\bb{\beta}_{t}+e^{\frac{h_t}{2}}\epsilon_{t}, \quad \epsilon_{t}\iid\mathcal{N}(0,1) \\
\mu_t = & \phi_{1}\mu_{t-1}+\eta_{t}, \quad \eta_{t}\iid\mathcal{N}(0,\lambda_1) \\
\boldsymbol{\beta}_{t} = & \Gamma_{t}\boldsymbol{\beta}_{t-1} + \boldsymbol{\xi}_{t}, \quad \boldsymbol{\xi}_{t}\sim\mathcal{N}(0,\Lambda_t) \\
h_{t}= & \alpha_0+\alpha_{1}(h_{t-1}-\alpha_{0})+\zeta_{t}, \quad \zeta_{t}\iid\mathcal{N}(0,\sigma_{\zeta})
\end{align*}
where $l$ is 4 in the current analysis, $\Gamma_t=diag\{\gamma_{t,j}\phi_{1}\}_{j=1}^{p}$ and $\Lambda_{t}=diag\{\gamma_{t,j}\lambda_{1}+(1-\gamma_{t,j})\lambda_{0}\}_{j=1}^{p}$. Moreover, $\gamma_{t,j}|\beta_{t-1,j}\ind Bernoulli(\omega(\beta_{t-1,j}))$ for $j=1,\ldots,40$. The initial states are $\bb{\beta}_{0}\sim\mathcal{N}(\bb{m}_{0},\bb{C}_{0})$ with $\bb{m}_{0}=\bb{0}$ and $\bb{C}_{0}=diag\{\gamma_{0,j}\lambda_{1}/(1-\phi_{1}^{2})+(1-\gamma_{0,j})\lambda_{0}\}_{j=1}^{p}$ and $\mu_0\sim\mathcal{N}(0,\lambda_1)$. The priors of the stochastic volatility model are the same of the simulation study. The model is trained from 1968-12-01 to 2003-03-01, whereas one-step-ahead forecasts are sequentially updated from 2003-06-01 to 2015-12-01. Out-of-sample forecasts for the Dynamic SSVS are obtained by sampling $\gamma_{j,T+l}$ form a $Bernoulli(\omega(\beta_{j,T}))$, and $h_{T+l}$ from $\mathcal{N}(\alpha_0+\alpha_1 h_{T},\sigma^{2}_{\zeta})$ and then computing the one-step-ahead forecast mean $f_{T+l}$ and the one-step-ahead forecast variance $q_{T+l}$ (using the notation of Algorithm \ref{alg:DSSVS-original}) via a Kalman filter recursion.  This methodology for obtaining forecasts is applied by default to every algorithm discussed in this thesis. However, for Dynamic EMVS the parameters' means are replaced by their mode. In addition, predictive performances are evaluated using a combination of four different metrics. Three of them 
evaluate the forecast errors, i.e. $e_{t}=y_{t}-f_{t}$, and they are: the Root Mean Squared Error (RMSE), the Mean Absolute Scaled Error (MASE) and the Weighted Mean Absolute Percentage Error (WMAPE). For the sake of clarity, the MASE is computed in this way,
\[
MASE=\frac{1}{n}\frac{\sum_{t=1}^{n}|y_{t}-f_{t}|}{\frac{1}{n-1}\sum_{t=2}^{n}|y_{t}-{y}_{t-1}|},
\]
and it measures how well our model performs compared to a naive forecast. Instead, the WMAPE is given by
\[
WMAPE=\frac{\sum_{t=1}^{n}|y_{t}-f_{t}|}{\sum_{i=1}^{n}|y_{t}|}
\]
and it represents an alternative to the classical MAPE which is robust to the presence of zero or almost zero values. The forth metric instead takes into account the entire predictive distribution and it is the the sum of log predictive likelihoods (SLPL) defined as
\[
SLPL = \sum_{t=1}^{n}\log \pi(y_{t}|f_{t},q_{t}).
\]
For this exercise we use the Dynamic SSVS strategy with precision sampler. The latter allows indeed for fast estimation without sacrificing uncertainty. The time series of inflation is plotted in the left panel of Figure \ref{fig:myfig001}. It clearly shows change points, that can be captured by a Time-Varying Parameter model. Overall, the residual standard deviations (central panel of Figure \ref{fig:myfig001}) are low, indicating that the model has succeed in selecting the right predictors, and it shows significant fluctuations during shocking events. The very first estimates however are the less reliable. The algorithm at first tends to assign all the coefficients to the slab, but after a dozen of observations the estimates became more precise and it starts to drop unimportant covariates from the model. In every periods, about one quarter of the coefficients are active which implies a significant improvements in terms of model interpret ability. Moreover, this number usually increases during shocks or change points, for then coming back to a value of around ten. 
```{r myfig001, fig.cap = "Left panel: Time series of quarterly inflation recentred and rescaled. Middle: Evolution of residual standard deviations. Right panel: Number of active covariates that contribute in forecasting inflation.", fig.align='center', fig.height=2, fig.width=10, echo=F, results=F, fig.pos='h'}
load("C:/Users/Edoardo/Dropbox/elementi salvati R/plot_series_inflation.Rda")
load("C:/Users/Edoardo/Dropbox/elementi salvati R/plot_vol_inflation.Rda")
load("C:/Users/Edoardo/Dropbox/elementi salvati R/plot_cov_inflation.Rda")

ggarrange(plot_series_inflation,plot_vol_inflation,plot_cov_inflation,nrow=1)
```
This in-sample assessment, that was useful to understand better the general features of the time series, is then followed by the forecasting exercise mentioned before. As shown in Figure \ref{fig:myfig002} dynamic shrinkage plays an important role in reducing the forecast uncertainty. The dynamic shrinkage model here used ($\Omega=0.1$) is able to preserve the accuracy of the accuracy of the full model ($\Omega=1$) but it returns smaller credible intervals and therefore its point estimates are more trustworthy from a statistical standpoint.

```{r myfig002, echo=F, fig.align='center', fig.cap="One-step-ahead forecasts (blue line) with credible intervals and realizations (black dashed line). For similar point estimates, the model withouth shrinkage ($\\Omega=1$) presents larger credible intervals than the model with actual dynamic shrinkage ($\\Omega=0.1$).",fig.height=2, fig.pos='h', fig.width=10, message=FALSE, warning=FALSE, results=F}
load("C:/Users/Edoardo/Dropbox/elementi salvati R/plot_inf_lag4_1.Rda")
load("C:/Users/Edoardo/Dropbox/elementi salvati R/plot_inf_lag4_01.Rda")

ggarrange(plot_inf_lag4_1,plot_inf_lag4_01,nrow=1)
```

This visual insight is accompanied by a quantitative comparison in Table \ref{tab:mytab001}. Here we compared several models characterized by diverse intensity of shrinkage expressed by varying hyperparameter $\Omega$. In addition, we also compared models with different specification of the AR(1) latent process. In details, while the metrics based on forecast errors are almost equal, the SLPL reduces according to $\Omega$. And for the same value of $\Omega$, the larger is $\phi_1$ (which must remain below 1 in absolute values) the better are the forecasts. This results was to be expected. We want to preserve a stationary structure of the latent process in order to maintain a stable evolution of the inclusion probabilities $\{w_t\}_{t=1}^{T}$, however we want to also conserve the information gained on the model's parameter from time to time. With an unjustified low value of $\phi_1$ the past history of the coefficient become uninformative to determine the value of the regression coefficient in $t$, making the estimation task harder.

```{r mytab001, message=FALSE, warning=FALSE, fig.align="center", echo=FALSE}
load("C:/Users/Edoardo/Dropbox/elementi salvati R/tab_vec.Rda")
load("C:/Users/Edoardo/Dropbox/elementi salvati R/tab_phi.Rda")
rownames(tab_vec)<-NULL
tab_vec = rbind(tab_vec,tab_phi[2:4,])
tab_vec= round(tab_vec,digits=3)
omega.col = c(1,0.9,0.7,0.5,0.3,0.1,0.1,0.1,0.1)
phi.col = c(0.98,0.98,0.98,0.98,0.98,0.98,0.8,0.4,0.1)
tab_vec.1 = cbind(omega.col,phi.col,tab_vec)
kbl(tab_vec.1, align="c", longtable = F, booktabs = T, caption="Performance comparison for varying intensities of shrinkage",digits=3,escape=F,col.names =c("$\\Omega$","$\\phi_1$","RMSE","WMAPE","MASE","SLPL"))%>%
  kable_styling(latex_options = "HOLD_position")%>%
  collapse_rows(columns = 1, valign = "top")

```


\chapter[Bayesian Structural Time Series]{Dynamic Shrinkage in Bayesian Structural Time Series}\label{Dynamic Shrinkage in Bayesian Structural Time Series}
  
  Building on the results of the previous chapter, we want to propose a new class of models for time series analysis. It consists of an extension of the Bayesian Structural Time Series models developed by @scott_varian_2013. Starting from this framework we develop a BSTS model with both Dynamic Spike-and-Slab process priors and stochastic volatility. This model can be also regarded as an extension of the TVP regression model defined in equation (\ref{eq:eq211}) that takes into account also structural time series components such as seasonality and trend. These features are indeed usually exhibited by many time series, especially in macroeconomics, and including them inside the model can improve forecasting performances.\
The model is defined by the following system of equations:
  \begin{equation}
   \begin{aligned}\label{eq:dssbsts1}
  y_{t} = & \mu_{t}+\tau_{t}+\boldsymbol{x}'_{t}\boldsymbol{\beta}_{t}+\epsilon_{t},\\
  \mu_{t} = & \mu_{t-1}+\delta_{t-1}+u_{t},\\
  \delta_{t} = & \delta_{t-1}+v_{t},\\
  \tau_{t} = & -\sum_{s=1}^{S-1}\tau_{t-s}+w_{t}, \\
  \boldsymbol{\beta}_{t} = & \Gamma_{t}\boldsymbol{\beta}_{t-1} + \boldsymbol{\xi}_{t} 
  \end{aligned}
  \end{equation}
where the $p$ coefficients are independent and identically distributed according to DSS priors: $\beta_{1:T,j}\overset{iid}{\sim}DSS(\Omega,\lambda_{0},\lambda_1,\phi_0,\phi_1)$ for $j=1,\ldots,p$. In addition, given the results illustrated in the previous chapter, that suggest improvements with an AR(1) stochastic volatility model, we model the residuals using following specification
$\epsilon_t\sim\mathcal{N}(0,e^{h_{t}})$ with 
\begin{equation}\label{eq:dssbsts2}
h_{t}=\alpha_{0}+\alpha_{1}(h_{t-1}-\alpha_{0})+\zeta_{t}
\end{equation}
The disturbances $u_{t}$, $v_{t}$, $w_{t}$ and $\zeta_{t}$ are i.i.d. normally distributed with mean zero and variances respectively $\sigma^{2}_{\mu}$, $\sigma^{2}_{\delta}$, $\sigma^{2}_{\tau}$ and $\sigma^{2}_{\zeta}$. Ideally, it would be possible to estimate these variances by maximum likelihood or assigning an Inverse-Gamma prior to them and proceeding with Bayesian inference. However, there are some identifiability issues in separating the residuals of the trend, the slope and the seasonality, and the MLE or MCMC may fail at converging towards a unique solution. We do not have this problem since we set a-priori $\sigma^{2}_{\mu}=\sigma^{2}_{\delta}=\sigma^{2}_{\tau}=\lambda_1$ in the applications discussed in this thesis, where $\lambda_1$ is a predetermined scalar. On the other hand, $\sigma^{2}_{\zeta}\sim\mathcal{IG}(0.5,0.5)$. Such choice is convenient since it is the default prior for $\sigma^{2}_{\zeta}$ used in the \code{stochvol} R-package. Moreover, in the state equation for $\bb{\beta}_t$, $\Gamma_t=diag\{\gamma_{t,j}\phi_{1}\}_{j=1}^{p}$ and 
\[ \boldsymbol{\xi}_{t} \iid \mathcal{N}(0,\Lambda_t)\; \mbox{with} \; \Lambda_{t}=diag\{\gamma_{t,j}\lambda_{1}+(1-\gamma_{t,j})\lambda_{0}\}_{j=1}^{p}. \]
Let us recall that $\gamma_{t,j}|\beta_{t-1,j}\ind Bernoulli(\omega(\beta_{t-1,j}))$ for $j=1,\ldots,p$. The values of $\lambda_{0}$ and $\lambda_{1}$ play a crucial role in ensuring that the estimation strategy is able to effectively distinguish the signal from the noise. Usually, some preliminary estimations have to be performed to understand which values of $\lambda_{0}$ and $\lambda_{1}$ provide the best fitting of the data. Overall, it is recommended to maintain the ratio $\lambda_1/\lambda_0=10$.\
The DLM representation of the Bayesian Structural Time Series is very useful since it allows to take advantage of the Kalman filter formulas, however some considerations are necessary. Firstly, the state vector is $\bb \theta_{t}=(\mu_{t},\delta_{t},\tau_{t},\tau_{t-1},...,\tau_{t-S+1},\bb{\beta}_{t})'$. Consequently, $\bb{F}_{t}=(1,0,1,0,...,0,\bb{x}_{t})$ and $\bb{G}_{t}=blockdiag(\bb{G}_{t}^{(trend)},\bb{G}_{t}^{(seasonal)},\Gamma_{t})$ where 

\[
\bb{G}_{t}^{(trend)}=\begin{pmatrix} 
1 & 1\\
0 & 1      
\end{pmatrix} \quad \text{and} \quad \bb{G}_{t}^{(seasonal)}=\begin{pmatrix}
-1 & -1 & ... & -1 \\
1 & 0 & ...& 0 \\
0 & 1 & ... & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & ... & 1
\end{pmatrix}.
\]
The specification of $\bb{G}_{t}^{(seasonal)}$ is a direct consequence of the identifiability constraint on the seasonal factors, i.e. $\sum_{s=1}^{S}\tau_{s}=0$. Similarly, $\Sigma_{\eta,t}=blockdiag(\Sigma_{\eta,t}^{(trend)},\bb{\Sigma}_{\eta,t}^{(seasonal)},\Lambda_{t})$ where 
\[
\Sigma_{\eta,t}^{(trend)}=\begin{pmatrix} 
\sigma^{2}_{\mu} & 0\\
0 & \sigma^{\delta}      
\end{pmatrix} \quad \text{and} \quad \Sigma_{\eta,t}^{(seasonal)}=\begin{pmatrix}
\sigma^{t}_{\tau} & 0 & ... & 0 \\
0 & 0 & ...& 0 \\
0 & 0 & ... & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & ... & 0
\end{pmatrix}.
\]
In addition, nothing prevents the model to accommodate also a static regression component $\bb{z}_t'\bb{\beta}$. This can be accomplished in several ways, however the most convenient from a computational point of view is the one envisaged by @scott_varian_2013 that consists in consists in appending a constant 1 to the state vector and $\bb{z}_{t}'\bb{\beta}$ to $\bb{F}_t$ in this way:
$\bb \theta_{t}=(1,\mu_{t},\delta_{t},\tau_{t},\tau_{t-1},...,\tau_{t-S+1},\bb{\beta}_{t})'$ and $\bb{F}_{t}=(\bb{z}_{t}'\bb{\beta},1,0,1,0,...,0,\bb{x}_{t})$.\
Efficient posterior sampling is provided by the Dynamic SSVS we introduced in the Section \ref{Dynamic Stochastic Search Variable Selection} with some modifications due to the introduction of structural components. All the steps are summarized in Algorithm \ref{alg:DSSBSTS} and we refer to Section \ref{Dynamic Stochastic Search Variable Selection} for further details. Note that the expansion of the state vector's size due to structural components leads inevitably to a slower running time since the Kalman filter's computational complexity is linear in data length but quadratic in the state vector dimension. Considering for instance quarterly data, $\bb{\theta}_{t}$ becomes a column vector of $p$ regression coefficients plus $3$ seasonal components and $2$ trend components. Therefore, in order to reduce computational efforts, we also propose an extension of the Dynamic EMVS seen in Section \ref{Dynamic EMVS} for this novel BSTS model. The latter results to be particularly useful for exploratory analysis, when multiple attempts are performed to find satisfying values of the hyperparameters.

\begin{algorithm}
\caption{Dynamic SSVS in BSTS with Stochastic Volatility} \label{alg:DSSBSTS}
\nl \textbf{Initialize} $\gamma_{j,t}$ and $\sigma_{\epsilon,t}$ for $0 \leq t \leq T$ and $1 \leq j \leq p$ and set $n_{0}$ and $d_{0}$\;
\begin{center}
\emph{Step 1: Sample Time Series Structural Components}\\
\end{center}
\nl \For{$1 \leq t \leq T$}{ 
Compute $\boldsymbol{a}_{t}=\boldsymbol{G}_{t}\boldsymbol{m}_{t-1}$\;
Compute $\boldsymbol{R}_{t}=\boldsymbol{G}_{t}\boldsymbol{C}_{t-1}\boldsymbol{G}'_{t}+\Sigma_{\eta,t}$\;
Compute $f_{t}=\boldsymbol{F}'_{t}\boldsymbol{a}_{t}$ and $e_{t}=y_{t}-f_{t}$\;
Compute $q_{t}=\boldsymbol{F}'_{t}\boldsymbol{R}_{t}\boldsymbol{F}_{t}+\sigma^{2}_{\epsilon,t}$\;
Compute $\boldsymbol{m}_{t}=\boldsymbol{a}_{t}+\boldsymbol{A}_{t}e_{t}$ and $\boldsymbol{C}_{t}=\boldsymbol{R}_{t}-\boldsymbol{A}_{t}\boldsymbol{A}'_{t}q_{t}$ with $\boldsymbol{A}_{t}=\boldsymbol{R}_{t}\boldsymbol{F}_{t}/q_{t}$
}
Draw $\boldsymbol{\theta}_{t}\sim \mathcal{N}(\boldsymbol{m}_{T},\boldsymbol{C}_{t})$\; \For{$t = T-1,...,0$}{  
Compute $\boldsymbol{a}_{T}(t-T)=\boldsymbol{m}_{t}+\boldsymbol{B}_{t}(\boldsymbol{\theta}_{t+1}-\boldsymbol{a}_{t+1})$\;
Compute $\boldsymbol{R}_{t}(t-T)=\boldsymbol{C}_{t}-\boldsymbol{B}_{t}\boldsymbol{R}_{t+1}\boldsymbol{B}'_{t}$ where $\boldsymbol{B}_{t}=\boldsymbol{C}_{t}\boldsymbol{G}'_{t+1}\boldsymbol{R}_{t+1}^{-1}$\;
Draw $\boldsymbol{\theta}_{t}\sim \mathcal{N}(\boldsymbol{a}_{T}(t-T),\boldsymbol{R}_{t}(t-T))$ \;
}
\begin{center}
\emph{Step 2: Sample Indicators}\\
\end{center}
\nl \For{$j=1,...,p$}{
\For{$1 \leq t \leq T$}{
Compute $\omega_{j,t}=\omega(\beta_{j,t-1})$ from \;
Compute $p^{\star}_{t,j}=p^{\star}_{t,j}(\beta_{j,t})$ from \;
Draw $\gamma_{j,t} \sim Bernoulli(p^{\star}_{j,t}(\beta_{j,t}))$\;
}
Compute $p^{\star}_{j,0}=\omega(\beta_{j,0})$\;
Draw $\gamma_{j,0} \sim Bernoulli(p^{\star}_{j,0}(\beta_{j,0}))$\;
}
\begin{center}
\emph{Step 3: Sample Volatility}\\
\end{center}
\nl Compute $r_{t}=y_{t}-\bb{F}_{t}'\bb{\theta}_{t}$ for $t=1,...,T$\;
Sample $(h_{0},...,h_{T})$ using the efficient sampling strategy described in Section \ref{A new proposal for the volatility process}\;
Compute $\sigma^{2}_{\epsilon,t}=e^{h_{t}}$\;
\end{algorithm}

\section{Dynamic Expectation-Maximization Variable Selection}

The huge computational saving in running time through the Dynamic EMVS encouraged us to develop a modified version of the strategy of @rockova_mcalinn_2021 that takes into account also structural time series components. Here, the MAP trajectory to be approximated is $\hat{\bb{\theta}}_{1:T}=\arg\max \pi(\bb{\beta}_{1:T},\mu_{1:T},\delta_{1:T},\tau_{1:T}\mid y_{1:T})$. Fortunately, the considerations done previously for the E-step in the previous chapter still hold. However, the M-step requires instead further computations. Here we describe how to implement the Dynamic EMVS for quarterly data, nonetheless the following steps can be generalized for other data frequencies.\
The joint density prior is factorized as it follows
\begin{align*}
 \pi(\mu_{0:T},\delta_{0:T},\tau_{0:T},&\bb{\beta}_{0:T},\bb{\gamma}_{0:T},\sigma^{2}_{\epsilon,0:T}) = \pi(\mu_{0})\pi(\delta_{0})\pi(\tau_{0})\pi(\bb{\beta}_{0}|\bb{\gamma}_{0})\pi(\bb{\gamma}_{0})... \\ 
...\prod_{t=1}^{T}\bigg\{ & \pi(\sigma^{2}_{\epsilon,t}\mid\sigma^{2}_{\epsilon,t-1})\pi(\mu_{t}\mid\mu_{t-1},\delta_{t-1})\pi(\delta_{t}\mid\delta_{t-1})\pi(\tau_{t}\mid\tau_{t-1},\tau_{t-2},\tau_{t-3})... \\
& ... \prod_{j=1}^{p}\bigg[\pi(\beta_{t,j}\mid\gamma_{t,j},\beta_{t-1,j})\pi(\gamma_{t,j}\mid\beta_{t-1,j})\bigg]\bigg\}
\end{align*}
whose posterior can be decomposed as
\begin{align*}
& \pi(\mu_{0:T},\delta_{0:T},\tau_{0:T},\bb{\beta}_{0:T},\bb{\gamma}_{0:T},\sigma^{2}_{\epsilon,0:T}\mid y_{1:T}) \propto \\ & \pi(y_{1:T}\mid \mu_{0:T},\delta_{0:T},\tau_{0:T},\bb{\beta}_{0:T},\bb{\gamma}_{0:T},\sigma^{2}_{\epsilon,0:T})\pi(\mu_{0:T},\delta_{0:T},\tau_{0:T},\bb{\beta}_{0:T},\bb{\gamma}_{0:T},\sigma^{2}_{\epsilon,0:T}).
\end{align*}
Therefore, in logs, we have
\begin{align*}
& \log\pi(\mu_{0:T},\delta_{0:T},\tau_{0:T},\bb{\beta}_{0:T},\bb{\gamma}_{0:T},\sigma^{2}_{\epsilon,0:T}\mid y_{1:T}) =\\
& C - \frac{(\mu_{0}-m_{0}^{(\mu)})^{2}}{2C_{0}^{(\mu)}}- \frac{(\delta_{0}-m_{0}^{(\delta)})^{2}}{2C_{0}^{(\delta)}}- \frac{(\tau_{0}-m_{0}^{(\tau)})^{2}}{2C_{0}^{(\tau)}} \\
& - \sum_{j=1}^{p}\bigg\{\gamma_{0,j}\frac{\beta_{0,j}^{2}(1-\phi_{1}^{2})}{2\lambda_{1}}+(1-\gamma_{0,j})\frac{\beta_{0,j}^{2}}{2\lambda_{0}}-\gamma_{0,j}\log\Theta-(1-\gamma_{0,j})\log(1-\Theta)\bigg\} \\
& +\sum_{t=1}^{T}\sum_{j=1}^{p}[\gamma_{t,j}\log \theta_{t,j}+(1-\gamma_{t,j})\log(1-\theta_{t,j})] \\
& - \sum_{t=1}^{T}\bigg\{\frac{(y_{t}-\bb{x}_{t}'\bb{\beta}_{t}-\mu_{t}-\tau_{t})^{2}}{2\sigma^{2}_{\epsilon,t}}+\sum_{j=1}^{p}\bigg[\gamma_{t,j}\frac{(\beta_{t,j}-\phi_{1}\beta_{t-1,j})^{2}}{2\lambda_{1}}+(1-\gamma_{t,j})\frac{\beta_{t,j}^{2}}{2\lambda_{0}}\bigg]+\\
& \log\pi(\sigma^2_{\epsilon,t}|\sigma^2_{\epsilon,t-1})+\frac{(\mu_{t}-\mu_{t-1}-\delta_{t-1})^{2}}{2\sigma^{2}_{\mu}}+\frac{(\delta_{t}-\delta_{t-1})^{2}}{2\sigma^{2}_{\delta}}+\frac{(\tau_{t}+\tau_{t-1}+\tau_{t-2}+\tau_{t-3})^{2}}{2\sigma^{2}_{\tau}} \bigg\}.
\end{align*}
Maximizing $\E(\mu_{0:T},\delta_{0:T},\tau_{0:T},\bb{\beta}_{0:T},\bb{\gamma}_{0:T},\sigma^{2}_{\epsilon,0:T}\mid y_{1:T})=Q(\bb{\Xi}\mid y_{1:T})$ with respect to $(\mu_{0:T},\delta_{0:T},\tau_{0:T},\bb{\beta}_{0:T})$, one obtains the following first order conditions:
\begin{align*}
\frac{\partial Q(\bb{\Xi}\mid y_{1:T})}{\partial \tau_{t}}  & = 0 \iff \\
\tau_{t} =\bigg(\frac{4}{\sigma^{2}_{\tau}}+ & \nu^{\star}\bigg)^{-1}\bigg\{\frac{1}{\sigma^{2}_{\tau}}(-3\tau_{t-1}-2\tau_{t-2}-\tau_{t-3}-3\tau_{t+1}-2\tau_{t+2}-\tau_{t+3})+\nu^{\star}(y_{t}-\bb{x}_{t}'\bb{\beta}_{t}-\mu_{t}))\bigg\} \\
 \frac{\partial Q(\bb{\Xi}\mid y_{1:T})}{\partial \tau_{0}} & = 0 \iff
 \tau_{0} = \bigg(\frac{1}{C_{0}^{(\tau)}}+\frac{1}{\sigma^{2}_{\tau}}\bigg)^{-1}\bigg\{\frac{m_{0}^{(\tau)}}{C_{0}^{(\tau)}}-\frac{1}{\sigma^{2}_{\tau}}(3\tau_{1}+2\tau_{2}+\tau_{3})\bigg\} \\
 \frac{\partial Q(\bb{\Xi}\mid y_{1:T})}{\partial \delta_{t}} & = 0 \iff  \delta_{t} =\bigg(\frac{1}{\sigma^{2}_{\mu}}+\frac{2}{\sigma^{2}_{\delta}}\bigg)^{-1}\bigg\{\frac{1}{\sigma^{2}_{\mu}}(\mu_{t+1}-\mu_{t})+\frac{1}{\sigma^{2}_{\delta}}(\delta_{t-1}+\delta_{t+1})\bigg\} \\
\frac{\partial Q(\bb{\Xi}\mid y_{1:T})}{\partial \delta_{0}} & = 0 \iff  \delta_{0}=\bigg(\frac{1}{C_{0}^{(\delta)}}+\frac{1}{\sigma^{2}_{\mu}}+\frac{1}{\sigma^{2}_{\delta}}\bigg)^{-1}\bigg\{\frac{m_{0}^{(\delta)}}{C_{0}^{(\delta)}}+\frac{1}{\sigma^{2}_{\mu}}(\mu_{1}-\mu_{0})+\frac{1}{\sigma^{2}_{\delta}}\delta_{1}\bigg\} \\
\frac{\partial Q(\bb{\Xi}\mid y_{1:T})}{\partial \mu_{t}} & = 0 \iff
\mu_{t} = \bigg(\frac{2}{\sigma^{2}_{\mu}}+\nu^{\star}\bigg)^{-1}\bigg\{\frac{1}{\sigma^{2}_{\mu}}(\mu_{t+1}-\mu_{t-1}+\delta_{t-1}-\delta_{t})+\nu^{\star}(y_{t}-\bb{x}_{t}'\bb{\beta}_{t}-\tau_{t})\bigg\} \\
\frac{\partial Q(\bb{\Xi}\mid y_{1:T})}{\partial \mu_{0}} & = 0 \iff 
\mu_{0} = \bigg(\frac{1}{\sigma^{2}_{\mu}}+\frac{1}{C_{0}^{(\mu)}}\bigg)^{-1}\bigg\{\frac{m_{0}^{(\mu)}}{C_{0}^{(\mu)}}+\frac{\mu_{1}}{\sigma^{2}_{\mu}}-\frac{\delta_{0}}{\sigma^{2}_{\mu}}\bigg\} \\
\frac{\partial Q(\bb{\Xi}\mid y_{1:T})}{\partial \bb{\beta}_{t}} & = 0 \iff \bb{\beta}_{t} = \bb{D}_{t}^{-1}\bigg\{\nu^{\star}(y_{t}-\mu_{t}-\tau_{t})\bb{x}_{t}+\frac{\phi_1}{\lambda_1}\bb{\beta}_{t-1}\odot\bb{p}^{\star}_{t}+\frac{\phi_1}{\lambda_{1}}\bb{\beta}_{t+1}\odot\bb{p}^{\star}_{t+1}\bigg\} \\
\frac{\partial Q(\bb{\Xi}\mid y_{1:T})}{\partial \bb{\beta}_{0}} & = 0 \iff
\bb{\beta}_{0}=\bb{D}_{0}^{-1}\frac{\phi_1}{\lambda_1}\bb{\beta_1}\odot\bb{p_{1}^{\star}}
\end{align*}
where
\[
\bb{D}_t=\frac{\bb{x}_{t}\bb{x}_{t}'}{\sigma^{2}_{\epsilon,t}}+diag\bigg\{\frac{p^{\star}_{t,j}}{\lambda_1}+\frac{1-p^{\star}_{t,j}}{\lambda_{0}}+\frac{\phi^{2}_{1}p^{\star}_{t+1,j}}{\lambda_{1}}\bigg\}_{j=1}^{p}
\]
and 
\[
\bb{D}_0=diag\bigg\{\frac{(1-\phi^2_1)p^{\star}_{0,j}}{\lambda_1}+\frac{1-p^{\star}_{0,j}}{\lambda_0}+\frac{\phi^{2}_1 p^{\star}_{1,j}}{\lambda_1}\bigg\}_{j=1}^{p}
\]
Again, the Woodburry formula simplifies calculations and reduces running time. 
The first order conditions listed above vary for $t=\{0,T\}$. For instance, the argmax with respect to $\tau_T$ is
\[\tau_{T}=\bigg(\frac{1}{\sigma^{2}_{\tau}}+\nu^{\star}\bigg)^{-1}\bigg\{\frac{1}{\sigma^{2}_{\tau}}(-\tau_{T-1}-\tau_{T-2}-\tau_{T-3})+\nu^{\star}(y_{T}-\bb{x}_{T}'\bb{\beta}_{T}-\mu_{T}))\bigg\}
\]
and so on. In the next section we explore the potential of the proposed model for structural time series in a simulation study. 

\section{Simulation Study}\label{SD2}
  
In order to illustrate of the validity of this novel approach, we propose a toy example on quasi-synthetic data. It is possible to generate a Bayesian Structural Time Series easily by summing up two or more processes. Therefore we generate synthetic data from a TVP regression model with $p=20$ explanatory variables of which the first four predictors affect the outcome whereas the remaining $16$ predictors are just disturbing factors following the same steps illustrated in Section \ref{SD}. For this simulation study, we want to also include a trend and a seasonal component. To this aim, rather than generating synthetic data, I used a time series already available in R, namely the series "AirPassengers" (the classic Box and Jenkins airline data consisting of monthly totals of international airline passengers from 1949 to 1960), which shows a trend and a multiplicative seasonality. The data used in the simulation study is obtained as the composition of the synthetis data plus the log of the series AirPassengers, as shown in Figure \ref{fig:myfig6}. Note, we rescale the data to emphasize its structural features.

```{r myfig6, fig.cap = "Left panel: Synthetic data generated from a TVP regression model. Middle: Rescaled Logarithmic AirPassengers. Right panel: Resulting time series used in the simulation.", fig.align='center', fig.height=2, fig.width=8, echo=F, results=F, fig.pos='H'}
  load("C:/Users/Edoardo/Dropbox/elementi salvati R/plot_compos.Rda")
  load("C:/Users/Edoardo/Dropbox/elementi salvati R/plot_real.Rda")
  load("C:/Users/Edoardo/Dropbox/elementi salvati R/plot_synthetic.y")
ggarrange(plot_synthetic.y,plot_real.y,plot_compos.y, nrow=1)
```

Clearly, fitting the resulting time series is anything but easy and this is also the reason why we restricted the number of (potential) regressors to 20 (rather than 50 as in Section \ref{SD}). Moreover, another reason fro limiting the simulation study to a moderate number of regressors is the size of the state vector, that here includes the structural components. For instance, using (dynamic) seasonal factors, $13$ additional latent states have to be estimated. Two of them are the time-varying trend with stochastic slope, whereas the remaining eleven represent the seasonality. Nevertheless, Dynamic SSVS and Dynamic EMVS prove to be able to capture the source of the signal. This is shown in Figure \ref{fig:myfig7} and \ref{fig:myfig8} and in particular in the table below. 

```{r myfig7, fig.cap = "Dynamic SSVS for BSTS model; true values of $\\beta_{1:T, j}$, $j=1, \\ldots, 6$, (black) and smoothed estimates with 95 percent credible intervals (yellow) of the first six regression coefficients.", fig.align='center', fig.height=4, fig.width=8, echo=F, fig.pos='ht', results=F}
for(i in 1:6){
  load(paste0("C:/Users/Edoardo/Dropbox/elementi salvati R/plotbstssv_",i,".Rda"))
}
ggarrange(plotbstssv_1,plotbstssv_2,plotbstssv_3,plotbstssv_4,plotbstssv_5,plotbstssv_6)
```  

```{r myfig8, fig.cap = "Dynamic EMVS for BSTS model; true values of $\\beta_{1:T, j}$, $j=1, \\ldots, 6$, (black) and MAP trajectory (green) of the first six regression coefficients.", fig.align='center', fig.height=4, fig.width=8, echo=F, results=F,fig.pos='h'}
for(i in 1:6){
  load(paste0("C:/Users/Edoardo/Dropbox/elementi salvati R/plotbstsem_",i,".Rda"))
}
ggarrange(plotbstsem_1,plotbstsem_2,plotbstsem_3,plotbstsem_4,plotbstsem_5,plotbstsem_6)
```  

In details, Table \ref{tab:mytab3} shows that a BSTS with trend and seasonality without dynamic shrinkage ($\Omega=1$) is not able to individuate the source of signal, therefore the estimates are biased and definitely not reliable. On the other hand, both the Dynamic SSVS and Dynamic EMVS performed well. We fix  $\sigma^{2}_{\tau}=\sigma^{2}_{\mu}=\sigma^{2}_{\delta}=0.1$. We acknowledge that the latter is a strong assumption, however we find it to work well for the estimation of the time series structural components. Figure \ref{fig:myfig14} shows smoothing estimates for the seasonal, the trend and the regression components.

```{r myfig14, fig.cap = "Structural time series components. Point estimates (black) with credible intervals (gray).", fig.align='center', fig.height=2, fig.width=8, echo=F, results=F,fig.pos='H'}
load("C:/Users/Edoardo/Dropbox/elementi salvati R/plot_str.Rda")
plot_str
``` 

For the sake of completeness, the analysis is carried out for $p=40$. Results are promising and they are shown in the last rows of Table \ref{tab:mytab3}. Overall, the Dynamic SSVS with $\Omega=0.2$ shows the best performance. In particular it is better than the Dynamic EMVS in terms of SSE and Hamming distance. In addition, the simulation algorithm did not produce any false detection or false non detection.

```{r mytab3, message=FALSE, warning=FALSE, fig.align="center", echo=FALSE}
load("C:/Users/Edoardo/Dropbox/elementi salvati R/TABLE3.Rda")
rownames(TABLE.3)<-NULL
TABLE.3 = round(TABLE.3,digits=3)
omega.col = c(1,0.2,0.2,0.2,0.2)
lab.col = c("SSVS","SSVS","EMVS","SSVS","EMVS")
TABLE.3.1 = cbind(lab.col,omega.col,TABLE.3)
kbl(TABLE.3.1, align="c", longtable = F, booktabs = T, caption="Performance comparison",digits=3,escape=F,col.names =c("Dynamic","$\\Omega$","p","SSE","Ham.","FP","FN","FD","FND"))%>%
  kable_styling(latex_options = "HOLD_position")
```

  \section{Macroeconomic Data}\label{Macro2}
  
Given the encouraging results obtained with synthetic data, here we provide an empirical macroeconomic application concerning unemployment forecasting. The unemployment time series is usually characterized by seasonal and trend-cycle components due to cyclical phenomena such as seasonal work and long-run economic fluctuations. However, in some specific circumstances, macroeconomic shocks may hit abruptly the series causing unexpected deviations from its natural path. For these reasons, unemployment forecasting represents a good exercise to test the strength of our model in capturing all these features. Therefore, the objective of this analysis is to fit the time series of unemployment rate not transformed and not seasonally adjusted. \
The BSTS model used in this exercise incorporates monthly seasonality, a stochastic trend with time-varying slope, and a large set of predictors whose role is to explain possible shocks. Predictors include: lagged values of the dependent variable, interest rates, financial and economics indicators, unemployment claims, and Google Trends data. The sources of the data are the Federal Reserve Economic Data (FRED)\footnote{url: https://fred.stlouisfed.org/}, Yahoo Finance\footnote{url: https://it.finance.yahoo.com/} and Google Trends\footnote{url: https://trends.google.com/}. The third one is a less conventional data source. In a nutshell, Google Trends is a free service provided by Google LLC which is meant to inform the user about how frequently a given search term is entered into Google’s research engine. However, Google Trends provides only relative frequencies of the data, such that a value equal to $100$ identifies the highest frequency of the searched term over a certain period, while the value $0$ indicates the lowest. \
The predictors are divided into two classes according to their frequency. For variables that present a monthly frequency we considered the lagged values, while for those having a daily frequency we considered the contemporaneous values. The list of predictors used in this study is provided in Appendix. More formally, the observation equation of the model here proposed is
\begin{equation}
y_{t}=\mu_{t}+\tau_{t}+\bb{w}_{t}'\bb{\beta}_{1,t}+\bb{z}_{t-1}'\bb{\beta}_{2,t-1}+\epsilon_{t}
\end{equation}
which is the same as for model (3.1) (\ref{eq:dssbsts1}), but with $\bb{x}_{t}=(\bb{w}_{t},\bb{z}_{t-1})$ and $\bb{\beta}_{t}=(\bb{\beta}_{1,t},\bb{\beta}_{2,t-1})$. \
The problem of forecasting using contemporaneous data is often referred to as "nowcasting". Such a term was originally coined in metereology to indicate the prediction of the present or the very near future of an economic
or business indicator. What makes nowcasting appealing is the fact that official statistics are usually published with a time lag, whereas other type of data, such as financial data or web data, are available at lower frequencies. For example, @CV_2009 had the idea of using Google Trends data for anticipating the release of official statistics. Official statistics are indeed released one or even two weeks after the end of the month while web data related to them are updated every day. This allows to gather meaningful information about the statistics of interest before the release. For example, observing Figure \ref{fig:myfig17}, where the time series of unemployment rate is presented together with some correlated Google searches, it would be evident that a certain correlation exists among them. Indeed, Google searches show a similar seasonality and synchronous spikes in proximity of shocking events. For instance, searches for unemployment depression, unemployment insurance and unemployment agencies drastically increased at the beginning of pandemic crisis. And while the official statistic for unemployment rate in April 2020 was released only on 8th May 2020 by the US Bureau of Labor Statistics, Google Trends data were already available in April.  Therefore, we could say that these Google Trends anticipated the observed peak in unemployment rate.
```{r myfig17, fig.cap = "Scaled time series of unemployment rate (black) and Google searches in different colors.", fig.align='center', fig.height=2.5, fig.width=6.5, echo=F, results=F, fig.pos='H'}
  load("C:/Users/Edoardo/Dropbox/elementi salvati R/unemp_totalplot.Rda")
unemp_totalplot
```
Before proceeding to forecasting, we carry out a analysis we carry out an explorative analysis. The hyperparameters are set in this way: $\Omega=0.2$, $\phi_0=0$, $\phi_1=0.98$, $\lambda_0=0.01$ and $\lambda_1=0.1$. This setting prove good results in the quasi-synthetic study and we maintain these choices here. After some preliminary analysis performed with a large set of 61 predictors (40 of which Google Trends), 12 seasonal factors, and a stochastic trend with a time-varying slope we decide to manually select only 15 Google Trends since we notice some redundancy in the information provided by some of them. In particular, we remove the US states-specific ones, e.g. "ny unemployment" and "florida unemployment", and those not showing any sort of correlation with the dependent variable. In addition, removing collinear predictors allows for better identification and it reduces the running time of the algorithms too. \
As shown in Figure \ref{fig:myfig18}, the BSTS model succeed in identifying the trend and seasonal components of the time series. Instead, Google Trends data intervene principally in proximity of the pandemic crisis, as reported also in Figure \ref{fig:myfig19} which shows that six predictors are particularly important for anticipating the event. The latter are: "searches for unemployment", "federal unemployment", "unemployment check", "unemployment depression", "unemployment office" and "unemployment pa". 

```{r myfig18, fig.cap = "Structural time series components. MCMC approximation of the expected values and their credible intervals. Data are rescaled to better visualize the main features.", fig.align='center', fig.height=2.5, fig.width=10, echo=F, results=F, fig.pos='H'}
  load("C:/Users/Edoardo/Dropbox/elementi salvati R/STRplot_bsts.Rda")
STRplot_bsts
```

```{r myfig19, fig.cap = "Number of active TVP regression coefficients that contribute to unemployment rate nowcasting.", fig.align='center', fig.height=1.5, fig.width=4, echo=F, results=F, fig.pos='H'}
#load("C:/Users/Edoardo/Dropbox/elementi salvati R/VP_bsts.Rda")
load("C:/Users/Edoardo/Dropbox/elementi salvati R/ACP_bsts.Rda")

ACP_bsts
```

We wondered why many Google Trends are insignificant and, in particular, why some of them can anticipate the 2020 pandemic crises but none of them can anticipate the 2008 financial crises. The answers we found are the following. Firstly, Google economists used to rely on another and maybe more powerful tool called Google Correlate. The difference between the two services is that in Google Correlate the user uploaded a time series and the system returned directly the most correlated queries. On the other hand, in Google Trends correlated series have to be selected manually. This makes it more difficult to include interesting predictors inside the model. Secondly, Google Trends data can assume only discrete values that depend on the time frame considered since they are rescaled according to the rule mentioned before. This fact may explain why Google Trends seem to have an important role in anticipating the pandemic crisis while they are insignificant for predicting the financial crises of 2008. We notice indeed that the enormous increase in Google searches due to the pandemic flatten the series to zero at every other time. \
Once we admit that there is a difference between the two crisis also under the aspect of Google searches, then it is then natural to wonder why this occurs.
In our opinion, the huge increase in Google searches during the pandemic crisis can be due to two possible causes. The lockdown, which forced people to stay at home, and the recent possibility to use web tools to search a job and carry out administrative procedure such as unemployment benefits applications. This can explain why the use of Google has been more intense during the pandemic crisis compared to the financial crisis.\
In any case, the model we developed is enough flexible to perform well even in the absence of relevant predictors. This is shown in Figure \ref{fig:myfig501} in which we compare the volatility process of two BSTS models with dynamic shrinkage, one including all the predictors we mentioned, i.e. economic variables and Google Trends, and the other including only one predictor, i.e. the one period lag of the dependent variable. In the first case (left panel) the model is able to capture the predictable dependence through the (flexible) conditional mean. In the second case (right panel), instead, the shock is captured by the residuals' variance, which increase during the pandemic crisis.
```{r myfig501, fig.cap = "Left panel: MCMC approximation of the expected values of $\\sigma_{\\epsilon,t}=\\exp(h_t/2)$ of a BSTS model with 36 predictors, including economic indicators and Google Trends data. Right panel: MCMC approximation of the expected values of $\\sigma_{\\epsilon,t}=\\exp(h_t/2)$ of a BSTS model with one predictors, i.e. the one period lag of unemployment rate.", fig.align='center', fig.height=2, fig.width=10, echo=F, results=F, fig.pos='h'}
load("C:/Users/Edoardo/Dropbox/elementi salvati R/VP_bsts_1.Rda")
load("C:/Users/Edoardo/Dropbox/elementi salvati R/VP_bsts_2.Rda")

ggarrange(VP_bsts_1,VP_bsts_2)
```
Let us focus on the forecasting performances. Because of the shock due to the pandemic crisis, that induces structural changes, we decided to split the time series in two periods: from 2004-01-01, day in which Google Trends was released by Google Inc., to 2013-08-01 and from 2011-06-01 to 2020-12-01. The last 20 observations of each period form the testing set. The idea is to evaluate the model's performances both under standard conditions and when shocks occur. The objective is to show that BSTS with dynamic shrinkage and stochastic volatility represents a very flexible approach to time series modeling and forecasting. In Tables \ref{tab:mytab31} and \ref{tab:mytab32}, forecasting performances of diverse specifications of Bayesian Structural Time Series models are compared. Under standard conditions, the model endowed with a seasonal component, which we call "seasonal model", provides better results. On the other hand, contrary to expectations, the full ($\Omega=1$) non-seasonal model performs curiously better than the shrunk ($\Omega=0.2$) non-seasonal model. This effect is due to the seasonality shown by some predictors that may help the model with more predictors to approximate somehow the seasonal component of the dependent variable.  Note, however, that seasonal fluctuations observed in the explanatory variables are usually caused by weather effects, administrative measures or other events that may not affect significantly the dependent variable. This explains the better accuracy obtained using a seasonal component rather than extracting the seasonality from the predictors. On the other hand, regarding the quantification of the uncertainty, the similar SLPL between the models with and without seasonality when $\Omega=1$ (as shown in Table \ref{tab:mytab31}) can be explained by the larger credible intervals due to the bigger size of the state vector in the seasonal model which tends to inflate the state variance and, consequently, the forecast variance.\
On the other hand, by seasonally adjusting the predictors the resulting picture becomes much more consistent with the expectations. Indeed non-seasonal models are now completely unable to capture seasonal fluctuations. The adjustment of the series has been carried out using the X-13ARIMA-SEATS Seasonal Adjustment strategy of the Census Bureau.\
So far we explained why a seasonal model should be preferred over a non-seasonal one. Regarding the comparison dynamic shrinkage versus no shrinkage, the results of this forecasting exercise are the following. Even though the predictive means are rather similar in both models, the reduction of the noise induced by shrinkage priors translates to narrower credible intervals and hence into an higher reliability of the point estimates. This peculiarity makes the penalized model preferable under standard conditions. On the other hand, when big shocks occur, the larger credible intervals produced by the full model allow to cover most of the fluctuation observed in the time series of unemployment rate, whereas the penalized model seems unable to 
properly represent the uncertainty around the point forecasts. This happens because the conditional inclusion probabilities evolve smoothly over time and they are not adequate for sudden changes. Therefore, under regular conditions, BSTS models with dynamic shrinkage are very promising for time series analysis of high-dimensional non-stationary and not seasonally adjusted time series. In order to make them more suitable also for predicting shocking events, $\Omega$ could be modeled as a random variable with support $(0,1]$, whose hyperparameters are set as a function of $\sigma^{2}_{\epsilon,t}$. Developments in this direction are not dealt with here. 

```{r mytab31, message=FALSE, warning=FALSE, fig.align="center", echo=FALSE}
load("C:/Users/Edoardo/Dropbox/elementi salvati R/MATRIX.Rda")
load("C:/Users/Edoardo/Dropbox/elementi salvati R/SASMAT.Rda")

MATRIX = round(MATRIX,digits=3)
omega.col1 = c(0.2,1,0.2,1,0.2,1,0.2,1)
seas.col=c("Yes","Yes","","","Yes","Yes","","")
MATRIX1 = cbind(seas.col,omega.col1,MATRIX)
MATRIX1[is.na(MATRIX1)]="-"

collapse_rows_dt <- data.frame(Model = c(rep("SSVS", 4), rep("EMVS", 4)),MATRIX1)

kbl(collapse_rows_dt, align="c", longtable = F, booktabs = T, caption="Not seasonally adjusted predictors",digits=3,escape=F,col.names = c("Dynamic","Seasonality","$\\Omega$","RMSE","WMAPE","MASE","SLPL","RMSE","WMAPE","MASE","SLPL"))%>%
  kable_paper(full_width = F) %>%
  column_spec(1, bold = F) %>%
  collapse_rows(columns = 1, valign = "top")%>%
add_header_above(c(" "=3, "From 01/12/2011 to 01/08/2013" = 4, "From 01/04/2019 to 01/12/2020" = 4)) %>%
kable_styling(latex_options = c("repeat_header","HOLD_position"),font_size = 8)
```
  
```{r mytab32, echo=FALSE, fig.align="center", message=FALSE, warning=FALSE}
load("C:/Users/Edoardo/Dropbox/elementi salvati R/SASMAT.Rda")

SASMAT = round(SASMAT,digits=3)
seas.col=c("Yes","Yes","","","Yes","Yes","","")
omega.col1 = c(0.2,1,0.2,1,0.2,1,0.2,1)
SASMAT1 = cbind(seas.col,omega.col1 ,SASMAT)
SASMAT1[is.na(SASMAT1)]="-"

collapse_rows_dt <- data.frame(Model = c(rep("SSVS", 4), rep("EMVS", 4)),SASMAT1)

kbl(collapse_rows_dt, align="c", longtable = F, booktabs = T, caption="Seasonally adjusted predictors",digits=3,escape=F,col.names = c("Dynamic","Seasonality","$\\Omega$","RMSE","WMAPE","MASE","SLPL","RMSE","WMAPE","MASE","SLPL"))%>%
  kable_paper(full_width = F) %>%
  column_spec(1, bold = F) %>%
  collapse_rows(columns = 1, valign = "top")%>%
add_header_above(c(" "=3, "From 01/12/2011 to 01/08/2013" = 4, "From 01/04/2019 to 01/12/2020" = 4)) %>%
kable_styling(latex_options = c("repeat_header","HOLD_position"),font_size = 8)
```

```{r myfig117, echo=F, fig.align='center', fig.cap="Not seasonally adjusted predictors. One-step-ahead forecasts (blue) with their credible intervals and the true series (black). Upper-left panel: Seasonal BSTS model with $\\Omega=0.2$. Bottom-left panel: Not seasonal BSTS model with $\\Omega=0.2$. Upper-right panel: Seasonal BSTS model with $\\Omega=1$. Bottom-right panel: Not seasonal BSTS model with $\\Omega=1$. These plots shows how the model behaviour under regular conditions.", fig.height=4, fig.pos='H', fig.width=10, message=FALSE, warning=FALSE, results=F}
load("C:/Users/Edoardo/Dropbox/elementi salvati R/fcst_plot_1.Rda")
load("C:/Users/Edoardo/Dropbox/elementi salvati R/fcst_plot_2.Rda")
load("C:/Users/Edoardo/Dropbox/elementi salvati R/fcst_plot_5.Rda")
load("C:/Users/Edoardo/Dropbox/elementi salvati R/fcst_plot_6.Rda")
load("C:/Users/Edoardo/Dropbox/elementi salvati R/fcst_plot_11.Rda")
load("C:/Users/Edoardo/Dropbox/elementi salvati R/fcst_plot_12.Rda")
load("C:/Users/Edoardo/Dropbox/elementi salvati R/fcst_plot_15.Rda")
load("C:/Users/Edoardo/Dropbox/elementi salvati R/fcst_plot_16.Rda")

ggarrange(fcst_plot_1,fcst_plot_2,
          fcst_plot_5,fcst_plot_6,
      ncol=2,nrow=2)
```

```{r myfig118, echo=F, fig.align='center', fig.cap="Not seasonally adjusted predictors. One-step-ahead forecasts (blue) with their credible intervals and the true series (black). Upper-left panel: Seasonal BSTS model with $\\Omega=0.2$. Bottom-left panel: Not seasonal BSTS model with $\\Omega=0.2$. Upper-right panel: Seasonal BSTS model with $\\Omega=1$. Bottom-right panel: Not seasonal BSTS model with $\\Omega=1$. These plots shows how the model behaviour when shocks occur.", fig.height=4, fig.pos='H', fig.width=10, message=FALSE, warning=FALSE, results=F}
load("C:/Users/Edoardo/Dropbox/elementi salvati R/fcst_plot_1.Rda")
load("C:/Users/Edoardo/Dropbox/elementi salvati R/fcst_plot_2.Rda")
load("C:/Users/Edoardo/Dropbox/elementi salvati R/fcst_plot_5.Rda")
load("C:/Users/Edoardo/Dropbox/elementi salvati R/fcst_plot_6.Rda")
load("C:/Users/Edoardo/Dropbox/elementi salvati R/fcst_plot_11.Rda")
load("C:/Users/Edoardo/Dropbox/elementi salvati R/fcst_plot_12.Rda")
load("C:/Users/Edoardo/Dropbox/elementi salvati R/fcst_plot_15.Rda")
load("C:/Users/Edoardo/Dropbox/elementi salvati R/fcst_plot_16.Rda")

ggarrange(fcst_plot_11,fcst_plot_12
          ,fcst_plot_15,fcst_plot_16,ncol=2,nrow=2)
```


```{r myfig119, echo=F, fig.align='center', fig.cap="Seasonally adjusted predictors. One-step-ahead forecasts (blue) with their credible intervals and the true series (black). Upper-left panel: Seasonal BSTS model with $\\Omega=0.2$. Bottom-left panel: Not seasonal BSTS model with $\\Omega=0.2$. Upper-right panel: Seasonal BSTS model with $\\Omega=1$. Bottom-right panel: Not seasonal BSTS model with $\\Omega=1$. These plots shows how the model behaviour under regular conditions.", fig.height=4, fig.pos='H', fig.width=10, message=FALSE, warning=FALSE, results=F}
load("C:/Users/Edoardo/Dropbox/elementi salvati R/sas_plot_1.Rda")
load("C:/Users/Edoardo/Dropbox/elementi salvati R/sas_plot_2.Rda")
load("C:/Users/Edoardo/Dropbox/elementi salvati R/sas_plot_5.Rda")
load("C:/Users/Edoardo/Dropbox/elementi salvati R/sas_plot_6.Rda")
load("C:/Users/Edoardo/Dropbox/elementi salvati R/sas_plot_11.Rda")
load("C:/Users/Edoardo/Dropbox/elementi salvati R/sas_plot_12.Rda")
load("C:/Users/Edoardo/Dropbox/elementi salvati R/sas_plot_15.Rda")
load("C:/Users/Edoardo/Dropbox/elementi salvati R/sas_plot_16.Rda")

ggarrange(sas_plot_1,sas_plot_2,
          sas_plot_5,sas_plot_6,
          ncol=2,nrow=2)
```


```{r myfig120, echo=F, fig.align='center', fig.cap="Seasonally adjusted predictors. One-step-ahead forecasts (blue) with their credible intervals and the true series (black). Upper-left panel: Seasonal BSTS model with $\\Omega=0.2$. Bottom-left panel: Not seasonal BSTS model with $\\Omega=0.2$. Upper-right panel: Seasonal BSTS model with $\\Omega=1$. Bottom-right panel: Not seasonal BSTS model with $\\Omega=1$. These plots shows how the model behaviour when shocks occur.", fig.height=4, fig.pos='H', fig.width=10, message=FALSE, warning=FALSE, results=F}
load("C:/Users/Edoardo/Dropbox/elementi salvati R/sas_plot_1.Rda")
load("C:/Users/Edoardo/Dropbox/elementi salvati R/sas_plot_2.Rda")
load("C:/Users/Edoardo/Dropbox/elementi salvati R/sas_plot_5.Rda")
load("C:/Users/Edoardo/Dropbox/elementi salvati R/sas_plot_6.Rda")
load("C:/Users/Edoardo/Dropbox/elementi salvati R/sas_plot_11.Rda")
load("C:/Users/Edoardo/Dropbox/elementi salvati R/sas_plot_12.Rda")
load("C:/Users/Edoardo/Dropbox/elementi salvati R/sas_plot_15.Rda")
load("C:/Users/Edoardo/Dropbox/elementi salvati R/sas_plot_16.Rda")

ggarrange(sas_plot_11,sas_plot_12
          ,sas_plot_15,sas_plot_16,ncol=2,nrow=2)
```

\section{Discussion}

Bayesian Structural Time Series models with Dynamic Spike-and-Slab regression and stochastic volatility represent the natural evolution of the model proposed by @scott_varian_2013. For the same reasons we allow for stochastic fluctuations in the trend, slope, and seasonality, it is logical to model the remaining parameters as time-varying too. The assumption of constant regression coefficients may hold when it found justification on physical models or when the time series evolves over few years. On the other hand, this assumption is likely to be too limiting for most applications, particularly in the economic and social sciences. The analysis presented in the preceding paragraph demonstrates this fact by proving that some predictors, in that case Google Trends data, play an important role when the time series exhibits shocks that cause it to deviate from its natural pattern. Otherwise, the time series can be satisfactorily modeled using a simple BSTS model with trend and seasonality. This raise two considerations. To begin with, simple models can occasionally operate admirably. In this scenario, adding predictors is not only pointless, but it also diminishes the model's interpretability, which is critical for economists. As a result, our novel class of BSTS models fits for the purpose since it is particularly parsimony oriented, permitting the intervention of additional predictors only when absolutely essential. Second, while time-varying coefficients can capture both permanent and transient phenomena like the pandemic crisis, time-varying residual variances hedge the researcher against biases that may arise as a consequence of the exclusion of some important predictors. The model indeed would still work without the need to adjust parameter estimates also in the latter case.
Overall, the results obtained using both simulated and real data are consistent with our goal of developing a highly flexible model that can be used to analyze long macroeconomic time series under a variety of scenarios. On the other hand, the analysis brings out some limitations of our approach too. The most crucial ones have concern how to deal with the model's hyperparameters. In particular, $\Omega$, which drives shrinkage globally, may be recalibrated in proximity of structural breaks. This would make the model even more flexible. Therefore, further developments on this issue will follow. A possible solution we envisage is to consider $\Omega$ as a random variable and letting its values changing according to the magnitude of shocking events such as the one exhibited by the unemployment rate in April 2020. 

  \chapter[Multivariate Time Series Models]{Dynamic Shrinkage in Multivariate Time Series Models}\label{Dynamic Shrinkage in Multivariate Time Series Models}
  
Introducing dynamic shrinkage methods in multivariate time series models is undoubtedly interesting and challenging at the same time. Multivariate time series models are widely used and they are often necessary to study macroeconomic phenomena phenomena, that are characterized by the interactions of many economic variables so that univariate models can hardly capture all the dynamics of interest. A multivariate model allows to better describe past and contemporaneous relationships between the key variables. This however comes with some costs. When the number of dependent variables increases, the curse of dimensionality becomes more severe and it may eventually be unsustainable. For this reason, many authors have addressed their researches to the development of variable selection methods in this area. Nevertheless, dynamic shrinkage in multivariate time series analysis is still an open problem. With this chapter we want to provide a small contribution in this direction by embedding the Dynamic Spike-and-Slab process priors within the framework of Time-Varying Parameter Vector Autoregressive (TVP-VAR) models. 
 
  \section{Time-Varying Parameter VAR Models}

The introduction of TVP-VAR models into the literature was prompted by a wide range of phenomena observed in time series analysis between the 1960s and the 1990s. For instance, strong evidence suggests that unemployment and inflation in the United States were greater and more volatile in the 1960s and 1970s than in the 1980s and 1990s. Such changes in the model parameters are difficult to be modeled within the classical frequentist framework of VAR models and this encouraged researcher to adopt a Bayesian approach. In the last decades, many authors developed even more flexible and sophisticated models in which the regression parameters or the residual variances are subject to stochastic variations. Some interesting contributions in this direction come from @CANOVA_1993, @CS_2001 and @COGLEY2005262, to mention a few.\
The assumption of time-varying parameters should be natural. For example recent structural macroeconomic shocks such as the 2008 financial crisis and the pandemic crisis, would suggest time varying models, that are likely to become the standard in macroeconomics. However, the lack of a variable selection mechanism in these models has limited their potential for many interesting applications. This limitation has however stimulated a growing research in this topic.
Here we decided to focus on the Time-Varying Parameter VAR model of @Primicieri_2005 that had a certain impact in the econometric literature. Our contribution is to extend this model by allowing for dynamic variable selection through a Dynamic Spike-and-Slab Process Priors. With respect to previous approaches which focused on heteroschedasticity or time-varying regression coefficients, the model of @Primicieri_2005 includes both of them. This model is particularly flexible since it provides a data-driven guidance in establishing whether the overall time variation of the linear structure is explained by a change in the size of the shocks, and hence in the volatility process, or a change in the propagation mechanism, and hence the coefficients. Formally, let $\bb{y}_{t}$ be an $n \times 1$ vector of observations, for $t=1,...,T$, the TVP-VAR is 
\begin{equation}\label{eq:TVPVAR1}
\bb{y}_{t}=\bb{c}_{t}+\sum_{h=1}^{H}\bb{B}_{h,t}\bb{y}_{t-h}+\bb{\epsilon}_{t}, \ \ \ \bb{\epsilon}_{t}\sim\mathcal{N}(\bb{0},\Omega_{t})
\end{equation}
where $\bb{c}_{t}$ is a $n \times 1$ vector of time-varying coefficients associated to a constant term, $\bb{B}_{h,t}$ (for $h=1,...,H$ number of lags) is a $n \times n$ matrix of time varying coefficients and $\epsilon_{t}$ is a vector of $n \times 1$ residuals. For identificability reasons we follow the proposal of @SIMS_1980 and we assume a lower triangular matrix $\bb{A}_{t}$ of contemporaneous relationship such that
\begin{equation*}
\bb{A}_{t}\Omega_{t}\bb{A}_{t}'=\Sigma_{t}^{\frac{1}{2}}\Sigma_{t}'^{\frac{1}{2}}
\end{equation*}
and 
\[ \boldsymbol A_{t} = \begin{pmatrix}
  1 & 0 & ... & 0 \\
  \alpha_{2 1,t} & 1 & ... & 0 \\
  \vdots  & \vdots  & \ddots & \vdots \\
  \alpha_{n1,t} & ... & \alpha_{n n-1,t} & 1
  \end{pmatrix}\]
while 
\[ \Sigma_{t} = \begin{pmatrix}
  \sigma^{2}_{1,t} & 0 & ... & 0 \\
  0 & \sigma^{2}_{2,t} & ... & 0 \\
  \vdots  & \vdots  & \ddots & \vdots \\
  0 & ... & 0 & \sigma^{2}_{n,t}
  \end{pmatrix}\]
A more convenient way to write equation (\ref{eq:TVPVAR1}) is
\begin{equation*}
\bb{y}_{t}=\bb{c}_{t}+\sum_{h=1}^{H}\bb{B}_{h,t}\bb{y}_{t-h}+\bb{A}^{-1}_{t}\Sigma_{t}^{\frac{1}{2}}\bb{\epsilon}_{t}, \ \ \ \bb{\epsilon}_{t}\sim\mathcal{N}(\bb{0},\bb{I}_{n})
\end{equation*}
This equation can be re-written more compactly in state space form. Let $\bb{X}'_{t}\equiv\bb{I}_{n}\otimes (1,\bb{y}_{t-1}',...,\bb{y}_{t-H}')$ and $\bb{\beta}_{t}\equiv vec([\bb{c}_t,\bb{B}_{1,t},...,\bb{B}_{H,t}]')$, then
\begin{equation}\label{eq:ssmtvp}
\bb{y}_{t}=\bb{X}_{t}'\bb{\beta}_{t}+\bb{A}_{t}^{-1}\Sigma_{t}^{\frac{1}{2}}\bb{\epsilon}_{t}, \ \ \ \bb{\epsilon}\sim\mathcal{N}(\bb{0},\bb{I}_{n})
\end{equation}
@Primicieri_2005 considers the regression coefficients $\bb{\beta}_{t}$, the elements $\alpha_{ij,t}$ of the matrix $\bb{A}_{t}$, and the logarithm of the diagonal elements of $\Sigma_{t}$ to behave like random walks. Even though this specification has the advantage of reducing the number of unknown parameters which is already large, it also entails undesirable implications from a theoretical point of view since random walk processes are intrinsically unstable. Moreover, although @rockova_mcalinn_2021 observe that it would be possible to extend the DSS approach to a random walk slab process by reformulating opportunely the transition weights, one must be careful to avoid the series $\omega_{1:T}$ being too unstable. This would indeed lead to erratic transitions from the spike to slab and vice versa. Therefore, we will stick to the considerations made in Chapter \ref{Dynamic Shrinkage} and assume the regression coefficients to follow independent stationary Gaussian AR(1) processes. The dynamics of the model is described by the following equations:
\begin{equation}
\begin{aligned}\label{EQ:dmodel}
\bb{y}_{t} = & \bb{X}_{t}'\bb{\beta}_{t}+\bb{A}_{t}\Sigma^{\frac{1}{2}} \bb{\epsilon}_{t},\\
\bb{\beta}_{t} = & \bb{G}_{t}^{(\beta)}\bb{\beta}_{t-1}+\bb{\xi}_{t},\\
\bb{\alpha}_{t} = & \bb{G}_{t}^{(\alpha)}\bb{\alpha}_{t-1}+\bb{\eta}_{t},\\
\bb{h}_{t} = & \bb{\mu} + \bb{R}(\bb{h}_{t-1}-\bb{\mu})+\bb{\zeta}_{t}
\end{aligned}
\end{equation}
where $\bb{G}_{t}^{(\beta)}=diag\{\gamma_{t,j}^{(\beta)}\phi_{1}\}_{j=1}^{p_\beta}$, $\bb{G}_{t}^{(\alpha)}=diag\{\gamma_{t,j}^{(\alpha)}\phi_{1}\}_{j=1}^{p_\alpha}$, $\bb{\mu}=(\mu_{1},...,\mu_{n})'$ and $\bb{R}=diag(\rho_1,...,\rho_n)$. The parameter $\phi_1$ will be set equal to 0.98 in the applications that will follow. We acknowledge that such assumption may seem too strong, however empirical analysis proved that fixing $\phi_1$ close to one leads to satisfying results while preserving stationary. On the other hand, it would be possible to assign a Beta distribution to this parameter and update its posterior; however, this would inevitably increase the amount of unknown parameters and add further steps into the MCMC scheme with the consequent increase in the running time (which is already long). Regarding the choice of $\bb{\beta}_{0}$ and $\bb{\alpha}_{0}$, one may assume a Gaussian distribution centered on the least squares estimates computed on a subsample of the data or on the whole sample with large variance. Alternatively, another strategy is to model $\bb{\beta}_{0}\sim\mathcal{N}(\bb{m}_0,\bb{C}_0)$ where $\bb{m}_0=\phi_1\bb{\gamma}_{0}^{(\beta)}$ and $\bb{C}_0=diag\{\gamma_{0,j}^{(\beta)}\lambda_1/(1+\phi_1)+(1-\gamma_{0,j}^{(\beta)})\lambda_0\}_{j=1}^{p_\beta}$ and same for $\bb{\alpha}_0$.\
In this model we distinguish between two types of auxiliary variables: $\gamma_{t,j}^{(\beta)}$ and $\gamma_{t,j}^{(\alpha)}$. The reason behind this separation is that we consider relationships between contemporaneous variables more meaningful with respect to the ones with lagged values of the variables. Therefore we would set $\Omega_{\alpha}$ close to one and $\Omega_\beta$ commensurate to the number of lags of our model such that models with more lags are shrunk with greater severity.
Our proposal is to assume that both $\beta_{1:T,j}$ and $\alpha_{1:T,k}$ for $j=1,...,p_\beta$ and $k=1,...,p_\alpha$ are i.i.d. distributed with DSS priors. That is, we assume that the generic $\beta_t$ has a mixture density prior of the kind 
\begin{equation*}
\pi(\beta_{t}|\gamma_{t},\beta_{t-1})=(1-\gamma_{t})\psi_{0}(\beta_{t}|\lambda_{0})+\gamma_{t}\psi_{1}(\beta_{t}|\mu_{t},\lambda_{1})
\end{equation*}
where
\begin{equation*}
\mu_{t}=\phi_{0}+\phi_{1}(\beta_{t-1}-\phi_{0}) \ \ \ \text{with} \ \ \ |\phi_{1}|<1
\end{equation*}
and
\begin{equation*}
P(\gamma_{t}=1|\beta_{t-1})=\omega_{t}^{(\beta)}
\end{equation*}
The inclusion probabilities $\omega_{t}^{(\beta)}$ are definite exactly as in Chapter \ref{Dynamic Shrinkage} and they depend on $\Omega_\beta$. Analogously we assign DSS priors on $\alpha_t$.
Finally, we assume that
\[ 
( \bb{\xi}_{t}, \bb{\eta}_{t}, \bb{\zeta}_{t})' \sim \mathcal{N}(\bb{0},\bb{W}_{t})
\]
where
\[ \bb{W}_{t} = \begin{pmatrix}
  \Lambda_{\beta} & 0 &  0 \\
  0 & \Lambda_{\alpha} & 0 \\
  0 & 0 & \Sigma_{\zeta} 
  \end{pmatrix}\]
with $\Lambda_{\beta}=\{\gamma_{t,j}^{(\beta)}\lambda_{1}+(1-\gamma_{t,j}^{(\beta)})\lambda_0\}_{j=1}^{p_{\beta}}$, $\Lambda_{\alpha}=\{\gamma_{t,j}^{(\alpha)}\lambda_{1}+(1-\gamma_{t,j}^{(\alpha)})\lambda_0\}_{j=1}^{p_{\alpha}}$ and $\Sigma_{\zeta}=diag(\sigma^{2}_{\zeta,1},...,\sigma^{2}_{\zeta,n})$. The hyperparameters $\lambda_1$ and $\lambda_0$ must be fixed in such a way that allows a sufficiently large ratio between spike and slab variances [@GM_1993]. For example, in the empirical study of Section \ref{MD3}, we notice that fixing $\lambda_1=0.1$ and $\lambda_0=0.01$ provides satisfying results. In general, we recommend to repeat the study for different values of $\lambda_0$ and $\lambda_1$ and pick the ones that produce the best predictive performances. The parameters $\sigma_{\zeta,i}^{2}$ for $i=1,...,n$ are independently estimated using the sampling strategy described in Section \ref{A new proposal for the volatility process}.


  \subsection{Dynamic Stochastic Search Variable Selection}
  
  Efficient posterior sampling for the TVP-VAR model with dynamic shrinkage can be performed using a Dynamic SSVS strategy. A possible sampling scheme is the one proposed by @Primicieri_2005 that we extend in order to deal with the dynamic shrinkage components, as reported below. The overall strategy, resumed in Algorithm \ref{alg:dstvpvar}. We here detail the main steps. Let $\bb{\gamma}=(\bb{\gamma}^{(\beta)},\bb{\gamma}^{(\alpha)})$ then
 
\begingroup
\LinesNumbered
\begin{algorithm}[t]
\caption{Dynamic Shrinkage in Time-Varying Parameter VAR models} \label{alg:dstvpvar}
 Draw $\boldsymbol{\beta}$ from $\pi(\boldsymbol{\beta}|\boldsymbol{y},\bb{\Sigma},\bb{A},\boldsymbol{\gamma},\bb{W})$ \;
 Draw $\bb{A}$ from $\pi(\boldsymbol{A}|\boldsymbol{y},\boldsymbol{\beta},\boldsymbol{\gamma},\boldsymbol{\Sigma},\boldsymbol{W})$ \;
 Draw $\gamma_{j,t}^{(\beta)}$ individually and independently from $\pi(\gamma_{j,t}^{(\beta)}|\boldsymbol{y},\boldsymbol{\beta},\bb{A},\boldsymbol{\Sigma},\boldsymbol{W},\bb{\gamma}_{-j,t}^{(\beta)})$ and $\gamma_{j,t}^{(\alpha)}$ individually and independently from $\pi(\gamma_{j,t}^{(\alpha)}|\boldsymbol{y},\boldsymbol{\beta},\bb{A},\boldsymbol{\Sigma},\boldsymbol{W},\bb{\gamma}_{-j,t}^{(\alpha)})$ \;
 Compute $\Lambda_{\beta}$ and $\Lambda_{\alpha}$\;
 Draw $\boldsymbol{\Sigma}$ from $\pi(\boldsymbol{\Sigma}|\boldsymbol{y},\boldsymbol{\beta},\bb{A},\boldsymbol{\gamma},\bb{W},\bb{\mu},\bb{R},\Sigma_{\zeta})$ \;
 Draw $\bb{\mu},\bb{R},\Sigma_{\zeta}$ independently from each time series using the passages illustrated in section \ref{A new proposal for the volatility process} \;
\end{algorithm}


\begin{itemize}
\item Step 1; Conditionally on $(\boldsymbol{y},\bb{\Sigma},\bb{A},\boldsymbol{\gamma},\bb{W})$, the space state model is linear and Gaussian with known variance, therefore draws from $\pi(\boldsymbol{\beta}|\boldsymbol{y},\bb{\Sigma},\bb{A},\boldsymbol{\gamma},\bb{W})$ can be obtained through FFBS algorithm.
\item Step 2; Let $\bb{y}_{t}-\bb{X}_{t}'\bb{\beta}_{t}=\hat{\bb{y}}_{t}$, which is observable once $\bb{\beta}_{t}$ is sampled, and note that equation (\ref{eq:ssmtvp}) can be rewritten as
\begin{equation}\label{eq:A1}
\bb{A}_{t}\hat{\bb{y}}_{t}=\Sigma^{\frac{1}{2}}_{t}\bb{\epsilon}_{t}
\end{equation}
Given the lower triangular shape of $\bb{A}_{t}$ with ones on its diagonal, then equation (\ref{eq:A1}) can be seen as a State-Space Model
\begin{equation}
\hat{\bb{y}}_{t}=\bb{Z}_{t}\bb{\alpha}_{t}+\Sigma^{\frac{1}{2}}_{t}\bb{\epsilon}_{t}
\end{equation}
where 
\[
\bb{Z}_{t} = \begin{pmatrix}
  0 & ... &  ... & 0 \\
  -\hat{y}_{1,t} & 0 & ... \\
  0 & -\hat{y}_{[1,2],t} & ... & 0 \\
  \vdots & \ddots & \ddots & 0\\
  0 & \dots &  0 & -\hat{y}_{[1,...,n-1],t}
  \end{pmatrix}
\]
where $\hat{y}_{[1,...,n-1],t}=(\hat{y}_{1,t},...,\hat{y}_{n-1,t})$. 
It is evident that, even in Step 2, FFBS algorithm can be used to sample from the full conditional distribution of $\bb{A}_{t}$.
\item Step 3; Samples of the indicators $\gamma_{t,j}$ are obtained individually and independently from their full conditional distribution as in Step 2 of Algorithm 3.
\item Step 4; Compute $\Lambda_{\beta}=\{\gamma_{t,j}^{(\beta)}\lambda_{1}+(1-\gamma_{t,j}^{(\beta)})\lambda_0\}_{j=1}^{p_{\beta}}$ and $\Lambda_{\alpha}=\{\gamma_{t,j}^{(\alpha)}\lambda_{1}+(1-\gamma_{t,j}^{(\alpha)})\lambda_0\}_{j=1}^{p_{\alpha}}$.
\item Step 5; Let $\bb{A}_{t}(\bb{y}_{t}-\bb{X}_{t}'\bb{\beta}_{t})=\bb{y}_{t}^{*}$ then 
\[
\bb{y}^{*}_{t}=\Sigma_{t}^{\frac{1}{2}}\bb{\epsilon}_{t},
\]
with $\Sigma_{t}=diag(\exp(h_{1,t}),...,\exp(h_{n,t}))$, where
\[
h_{i,t} = \mu_i+\rho_i(h_{i,t-1}-\mu_i)+\zeta_{i,t}, \quad \zeta_{i,t}\iid\mathcal{N}(0,\sigma^{2}_{i,\zeta})
\]
for $i \in \{1,...,n\}$ and $t \in \{1,...,T\}$. The latter represents a stochastic volatility model from which samples can be drawn using the sampling strategy of Kastner (2016) independently for each univariate time series, which is possible thanks to the diagonal structure of $\Sigma_{t}$.
\item Step 6; Draw $\bb{\mu},\bb{R},\Sigma_{\zeta}$ independently from each time series using the passages illustrated in section \ref{A new proposal for the volatility process}
\end{itemize}

The sampling strategy described above is very intuitive, however it has an important limit: the computational cost. Given that the Kalman filter's computational complexity is linear in data length but quadratic in the state vector dimension, it's easy to see how the FFBS strategy would be incredibly slow in large TVP-VAR models. Therefore, following @EJC_2016 and @Chan_2018, we replace the FFBS steps with a precision sampler. Moreover, the author propose the following model specification which is shown to greatly increase the sampler's efficiency in Structural VAR models:
\[
\bb{y}_t = \bb{X}\bb{\beta}_{t}+\bb{W}\bb{\alpha}_{t}+\Sigma_{t}^{\frac{1}{2}}\bb{\epsilon}_{t}
\]
where $\bb{X}_{t}\equiv\bb{I}_{n}\otimes (1,\bb{y}_{t-1}',...,\bb{y}_{t-H}')$ and $\bb{W}$ is a $n\times n(n-1)/2$ matrix such that
\[
\bb{W}=\begin{pmatrix}
  0 & ... &  ... & 0 \\
  -y_{1,t} & 0 & ... \\
  0 & -y_{[1,2],t} & ... & 0 \\
  \vdots & \ddots & \ddots & 0\\
  0 & \dots &  0 & -y_{[1,...,n-1],t}
  \end{pmatrix},
\]

that allows to simultaneously draw $\bb{\beta}$ and $\bb{\alpha}$. Clearly, its State Space Model form is 
\[
\bb{y}_{t} = \tilde{\bb{X}}_t\bb{\theta}_t+\bb{\epsilon}_t
\]
with $\tilde{\bb{X}}_t=(\bb{X}_t,\bb{W}_t)$ and $\bb{\theta}_t=(\bb{\beta}_t',\bb{\alpha}_t')'$.
Details on the precision sampler have been already provided in section \ref{Sparse Matrix}, here we provide a brief recap.  

\begin{itemize}
\item Step 1; Let us recall the State Space representation of a TVP-VAR model 
\[\underset{(T \times n)\times1}{\boldsymbol{y}}=\underset{(T\times n)\times p}{\tilde{\boldsymbol{X}}}\ \ \underset{p\times 1}{\boldsymbol{\theta}} + \underset{(T \times n)\times1}{\boldsymbol{\epsilon}}, \ \ \ \boldsymbol{\epsilon} \sim \mathcal{N}\bigg(\underset{(T \times n)\times1}{\boldsymbol{0}},\underset{(T \times n)\times (T\times n)}{\boldsymbol{\Sigma}_{\epsilon}}\bigg)\]
where $\boldsymbol{\epsilon} = (\boldsymbol{\epsilon}_{1}',...,\boldsymbol{\epsilon}_{T}')'$, $\boldsymbol{\Sigma}_{\epsilon} = diag(\Sigma_{\epsilon,1},...,\Sigma_{\epsilon,T})$ and $\tilde{\boldsymbol{X}} = diag(\tilde{\boldsymbol{X}}_{1},...,\tilde{\boldsymbol{X}}_{T})$. Remember that the latent process of the stochastic coefficients evolves as 
\[\boldsymbol{\theta}_{t}=\boldsymbol{G}_{t}\boldsymbol{\theta}_{t-1}+\boldsymbol{\eta}_{t}, \ \ \ \boldsymbol{\eta}_{t} \sim \mathcal{N}(\boldsymbol{0},\Lambda_{t})\]
where in the DSS scheme $\boldsymbol{G}_{t}=diag\{\gamma_{j,t}\phi_{1}\}_{j=1}^{p}$ and $\Lambda_{t}=diag\{\gamma_{j,t}\lambda_{1}+(1-\gamma_{j,t})\lambda_{0}\}_{j=1}^{p}$. \
Define the matrix 
\begin{equation} 
\boldsymbol{D} = \begin{pmatrix}
\boldsymbol{I}_{k} & 0 & ... & 0\\
-\boldsymbol{G}_{1} & \boldsymbol{I}_{k} & ... & 0\\
... & ... & ... & 0 \\
0 & ... & -\boldsymbol{G}_{T} & \boldsymbol{I}_{k}
\end{pmatrix}
\end{equation}
Therefore we can write
\[  \boldsymbol{D}\boldsymbol{\theta} = \boldsymbol{\tilde{\alpha}}_{0}+\boldsymbol{\xi}, \ \ \ \boldsymbol{\xi} \sim \mathcal{N}(\boldsymbol{0},\boldsymbol{S_\theta}) \]
where $\boldsymbol{\tilde{\alpha}}_{0}=(\boldsymbol{\theta}'_{0},\boldsymbol{0},...\boldsymbol{0})'$ and $\boldsymbol{S_\theta}=diag(\Lambda_{1},...,\Lambda_{T})$.
Equivalently we write
\[\boldsymbol{\theta} | \boldsymbol{\theta}_{0},\boldsymbol{\gamma} \sim \mathcal{N}(\boldsymbol{D}^{-1}\boldsymbol{\tilde{\alpha}}_{0},(\boldsymbol{D}'\boldsymbol{S_\theta}^{-1}\boldsymbol{D})^{-1})\]
and we label $\bb{\alpha}_{0}=\boldsymbol{D}^{-1}\boldsymbol{\tilde{\alpha}}_{0}$.
Thanks to Corollary 8.1 of Theorem 8.1 of Kroese and Chan. (2014), that we mentioned in Section \ref{Bayesian Inference in Linear Regression}, we can sample from 
\[\boldsymbol{\theta} | \boldsymbol{y},\boldsymbol{h},\boldsymbol{\gamma},\boldsymbol{\theta}_{0},\boldsymbol{h}_{0} \sim \mathcal{N}(\hat{\boldsymbol{\theta}},\boldsymbol{K}^{-1}_{\boldsymbol{\theta}})\]
where $\hat{\boldsymbol{\theta}}=\boldsymbol{K}^{-1}_{\boldsymbol{\theta}}\boldsymbol{d}_{\boldsymbol{\theta}}$, $\boldsymbol{K_\theta}=\boldsymbol{D}'\boldsymbol{S_\theta}^{-1}\boldsymbol{D}+\tilde{\boldsymbol{X}}'\boldsymbol{\Sigma_\epsilon}^{-1}\tilde{\boldsymbol{X}}$ and $\boldsymbol{d_\theta}=\boldsymbol{D}'\boldsymbol{S_\theta}^{-1}\boldsymbol{D}\boldsymbol{\alpha}_{0}+\tilde{\boldsymbol{X}}'\boldsymbol{\Sigma_{\epsilon}}^{-1}\boldsymbol{y}$.
\item Step 2; Sample $\boldsymbol{\theta}_{0}$ from the full conditional distribution 
\[
(\boldsymbol{\theta}_{0}|\boldsymbol{y},\boldsymbol{\theta},\boldsymbol{h},\boldsymbol{\Lambda}_{0})\sim\mathcal{N}(\hat{\boldsymbol{\theta}_{0}},\boldsymbol{K_{\theta_{0}}}^{-1})
\]
where $\boldsymbol{K_{\theta_{0}}}=\bb{C}_{0}^{-1}+\Lambda^{-1}_{0}$ and $\hat{\boldsymbol{\theta}_{0}}=\boldsymbol{K_{\theta_{0}}}^{-1}(\bb{C}_{0}^{-1}\bb{m}_{0}+\Lambda_{0}^{-1}\boldsymbol{\theta}_{1})$, with $\bb{m}_{0}$ and $C_{0}$ respectively the prior mean and the prior variance of the state vector.
\item Step 3; Sample individually and independently the indicators $\gamma_{j,t}$ from their full conditional distribution as in Step 2 of Algorithm \ref{alg:DSSVS-original}.
\item Step 4; Compute $\Lambda_{t}=diag\{\gamma_{j,t}\lambda_{1}+(1-\gamma_{j,t})\lambda_{0}\}_{j=1}^{p}$.
\item Step 5; Compute $\bb{D}$ as in equation (2.17).
\item Step 6; Compute $\bb{r}=\bb{y}-\tilde{\boldsymbol{X}}\bb{\theta}$ and use the residuals $\bb{r}_{i}=(r_{1,i},...,r_{T,i})'$ (for $i=1,...,n$) to perform the AWOL-ASIS strategy of Kastner (2016).
\end{itemize}
In the following section we provide an empirical example on macroeconomic data that will be useful to illustrate the performance of dynamic shrinkage in VAR models and to compare these two alternative estimation strategies.


\section{Macroeconomic Data}\label{MD3}
  
The dataset used for this empirical exercise is included in the package \code{bvarsv} and it contains quarterly time series data of inflation rate, unemployment rate and treasury bill interest rate for the US. Data are standardized and made stationary using log-differences. Overall, the data set covers the period from March 1953 to June 2015. In the current exercise the focus is on forecasting, therefore identification plays a secondary role in this analysis. However, the Structural VAR is built in such a way to allow for identification and, possibly, to assess Granger causality or to compute impulse response functions. The TVP-VAR we consider for this analysis is characterized by the three variables just mentioned, i.e. inflation, unemployment and interest rate, with eight lags for each of them. In other words, the model comprises a total of 75 regression coefficients to be estimated, plus the time-varying parameters included in the covariance matrix. As mentioned before, identification is made possible by a Cholesky scheme (or triangular scheme). 
Therefore, the order of the variables entering the vector $\bb{Y}_{t}$ must be chosen accurately. This issue is actually less important when we are interested in forecasting, however we decided to follow the strategy used by @Primicieri_2005 for monetary policy shock identification. Clearly, the interest rate is ordered last since we can assume that the monetary policy authority responds to a monetary policy shock according to the Taylor rule
\[i_{t}=\pi _{t}+i_{t}^{*}+a_{\pi }(\pi _{t}-\pi _{t}^{*})+a_{y}(y_{t}-{\bar  y}_{t}),
\]
where $\pi _{t}$ is the current inflation rate, $i_{t}^{*}$ is the natural interest rate, and $(\pi _{t}-\pi _{t}^{*})$ and $(y_{t}-{ y}_{t}^{*})$ are respectively the deviations of inflation and output from their natural level. Unemployment is ordered second and finally inflation is ordered first. According to the author, this is more a normalization rather than an identification condition. This scheme implies that unemployment and inflation do not respond at a time $t$ to a contemporaneous monetary shock, represented by the innovation term of the interest rate, but the response occurs only after some periods. Figures \ref{fig:myfig72} -- \ref{fig:myfig74}, show heat maps comparing a model without shrinkage ($\Omega_{\beta}=1$) to a model with a quite severe degree of shrinkage ($\Omega_{\beta}=0.2$). The remaining model's hyperparameters are set accordingly to previous applications: $\lambda_0=0.01$, $\lambda_1=0.1$ and $\phi_1=0.98$. The stochastic volatility process has parameters' priors: $\alpha_0\sim\mathcal{N}(-10,100)$, $\alpha_2\sim\mathcal{B}(20,1.5)$ and $\sigma^2_{\zeta}\sim\mathcal{IG}(0.5,0.5)$. As the heat maps show, the introduction of dynamic shrinkage priors produces a drastic change in the coefficients values. Before shrinkage, the signs and the magnitude of the regression coefficients estimated is consistent with economic theory. The interest rate for example is negatively correlated with unemployment rate and its one period lag and positively correlated with inflation rate and its one period lag, while this relationships became less clear for further lags. The relationship between unemployment and inflation is less stable and it depends on the lag considered. Overall, we would expect an overall negative relationship which is more evident in the equation of inflation than in the one of unemployment. In any case, what we want to highlight is the drastic change of these relationships once dynamic shrinkage occurs. Indeed, when $\Omega_\beta=0.2$ the heat maps show that for each response variable the most significant predictor is its lagged values. These results are surprising, but not too much. In fact, it is reasonable to think that the most important coefficients are the most recent ones and that the signal dissipates as lags increase. In addition, because of the instability of the relationships among lagged variables, a dynamic shrinkage priors is likely to make a safe choice, entrusting the highest predictive power to those predictors that show a consistent behavior over time.

```{r myfig72, echo=F, fig.align='center', fig.cap="Heat map representing estimated time-varying coefficients of the lagged VAR variables affecting inflation. In the upper panel, a VAR(8) model without shrinkage ($\\Omega_\\beta=1$) is presented. In this case, the signal is redistributed across all the predictors. In the lower panel, the same VAR(8) model is estimated when a severe shrinkage ($\\Omega_\\beta=0.2$) applies. Here, the signal is concentrated on the first lag of the dependent variable.", fig.height=5.5, fig.pos='H', fig.width=5.5, message=FALSE, warning=FALSE, results=F}

load("C:/Users/Edoardo/Dropbox/elementi salvati R/plot_8lags_02_inf.Rda")
load("C:/Users/Edoardo/Dropbox/elementi salvati R/plot_8lags_1_inf.Rda")

ggarrange(plot_8lags_1_inf,
plot_8lags_02_inf, ncol=1)
``` 

```{r myfig73, echo=F, fig.align='center', fig.cap="Heat map representing estimated time-varying coefficients of the lagged VAR variables affecting unemployment rate. In the upper panel, a VAR(8) model without shrinkage ($\\Omega_\\beta=1$) is presented. In this case, the signal is redistributed across all the predictors. In the lower panel, the same VAR(8) model is estimated when a severe shrinkage ($\\Omega_\\beta=0.2$) applies. Here, the signal is concentrated on the first lag of the dependent variable.", fig.height=5.5, fig.pos='H', fig.width=5.5, message=FALSE, warning=FALSE, results=F}

load("C:/Users/Edoardo/Dropbox/elementi salvati R/plot_8lags_02_une.Rda")
load("C:/Users/Edoardo/Dropbox/elementi salvati R/plot_8lags_1_une.Rda")

ggarrange(plot_8lags_1_une,
plot_8lags_02_une, ncol=1)
``` 

```{r myfig74, echo=F, fig.align='center', fig.cap="Heat map representing estimated time-varying coefficients of the lagged VAR variables affecting interest rate. In the upper panel, a VAR(8) model without shrinkage ($\\Omega_\\beta=1$) is presented. In this case, the signal is redistributed across all the predictors. In the lower panel, the same VAR(8) model is estimated when a severe shrinkage ($\\Omega_\\beta=0.2$) applies. Here, the signal is concentrated on the first lag of the dependent variable.", fig.height=5.5, fig.pos='H', fig.width=5.5, message=FALSE, warning=FALSE, results=F}

load("C:/Users/Edoardo/Dropbox/elementi salvati R/plot_8lags_02_int.Rda")
load("C:/Users/Edoardo/Dropbox/elementi salvati R/plot_8lags_1_int.Rda")

ggarrange(plot_8lags_1_int,
plot_8lags_02_int, ncol=1)

``` 


```{r eval=FALSE, include=FALSE}
#load("C:/Users/Edoardo/Dropbox/elementi salvati R/plot_alpha_02.Rda")
#plot_alpha_02
```


```{r myfig502, fig.cap = "MCMC approximation of the expected value of the residual standard deviations using precision sampler of inflation rate (left panel), unemployment rate (central panel), interest rate (right panel).", fig.align='center', fig.height=2, fig.width=8, echo=F, results=F,fig.pos='H'}
  load("C:/Users/Edoardo/Dropbox/elementi salvati R/plot_std1.Rda")
  load("C:/Users/Edoardo/Dropbox/elementi salvati R/plot_std2.Rda")
  load("C:/Users/Edoardo/Dropbox/elementi salvati R/plot_std3.Rda")
ggarrange(plot_std1,plot_std2,plot_std3,nrow=1)
```

We now test the out-of-sample performances of the models. Therefore, one-step-ahead forecast distributions are approximated via MCMC for the last ten periods of the series.
The results are reported in Table \ref{tab:mytab51} and they show that point forecasts improve for some variables (interest rate and unemployment rate) when shrinkage applies. Gains can be rather minor as in the case of inflation and unemployment or very large as for the interest rate. Values of WMAPE and MASE are in general acceptable. The huge values of MASE for interest rate are motivated by the fact that interest rates have been kept close to the zero lower bound in the effort of stimulating the economy after the 2008 financial crisis. Therefore the denominator of MASE is almost near zero and the metric tends to be large by construction. The most impressive gain lies in the SLPL. Indeed, noise reduction due to shrinkage of unimportant coefficients toward zero lead to smaller credible intervals and, thus, more precise estimates. From a visual standpoint, this effect is shown in Figure \ref{fig:myfig71}. \
Using the precision sampler we obtain very similar results but with a significant gain in running time. For 1000 iterations the Dynamic SSVS with a precision sampler scheme takes 170.76 seconds against the 1835.97 seconds of the Dynamic SSVS with double FFBS scheme. This fact leads to new opportunities in the field of large TVP-VAR since it allows to deal with a vast amount of coefficients in a reasonable amount of time and, at the same time, it avoids overfitting thanks to dynamic shriankage.

```{r mytab51, message=FALSE, warning=FALSE, fig.align="center", echo=FALSE}
load("C:/Users/Edoardo/Dropbox/elementi salvati R/TVPVAR_MAT.Rda")
TVPVAR_MAT = round(TVPVAR_MAT,digits=3)
omega.col = c(0.2,1,0.2,1,0.2,1,0.2,1)
TVPVAR_MAT1 =cbind(omega.col,TVPVAR_MAT)
TVPVAR_MAT1[is.na(TVPVAR_MAT1)]=" "
collapse_rows_dt <- data.frame(Model = c(rep("Inflation rate", 2), rep("Unemployment rate", 2),rep("Interest rate", 2),rep("Total",2)),TVPVAR_MAT1)

kbl(collapse_rows_dt, align="c", longtable = F, booktabs = T, caption="Dynamic SSVS with FFBS: performance comparison.",digits=3,escape=F,col.names = c(" ","$\\Omega$","RMSE","WMAPE","MASE","SLPL"))%>%
  kable_paper(full_width = F) %>%
  column_spec(1, bold = F) %>%
  collapse_rows(columns = 1, valign = "top")%>%
  kable_styling(latex_options = "HOLD_position")
```

```{r mytab151, message=FALSE, warning=FALSE, fig.align="center", echo=FALSE}
load("C:/Users/Edoardo/Dropbox/elementi salvati R/TVPVAR_MAT_2.Rda")
TVPVAR_MAT_2 = round(TVPVAR_MAT_2,digits=3)
omega.col = c(0.2,1,0.2,1,0.2,1,0.2,1)
TVPVAR_MAT2 =cbind(omega.col,TVPVAR_MAT_2)
TVPVAR_MAT2[is.na(TVPVAR_MAT2)]=" "
collapse_rows_dt <- data.frame(Model = c(rep("Inflation rate", 2), rep("Unemployment rate", 2),rep("Interest rate", 2),rep("Total",2)),TVPVAR_MAT2)

kbl(collapse_rows_dt, align="c", longtable = F, booktabs = T, caption="Dynamic SSVS with precision sampler: performance comparison.",digits=3,escape=F,col.names = c(" ","$\\Omega$","RMSE","WMAPE","MASE","SLPL"))%>%
  kable_paper(full_width = F) %>%
  column_spec(1, bold = F) %>%
  collapse_rows(columns = 1, valign = "top")%>%
  kable_styling(latex_options = "HOLD_position")
```

 
```{r myfig71, echo=F, fig.align='center', fig.cap="TVP-VAR model comprising US inflation rate, unemployment rate and federal funds rate with 8 lags. One-step-ahead forecasts with credible intervals (blue) estimated using the precision sampler strategy and the true time series (black). On the left column, the graphs show forecasts generated by a TVP-VAR model with no shrinkage ($\\Omega_\\beta=1$), while on the right column forecasts are generated by a TVP-VAR model with severe shrinkage ($\\Omega_\\beta=0.2$). The latter presents smaller credible intervals.", fig.height=6, fig.pos='H', fig.width=6, message=FALSE, warning=FALSE, results=F}

load("C:/Users/Edoardo/Dropbox/elementi salvati R/plot_inf_var02_pt2.Rda")
load("C:/Users/Edoardo/Dropbox/elementi salvati R/plot_inf_var1_pt2.Rda")
load("C:/Users/Edoardo/Dropbox/elementi salvati R/plot_int_var02_pt2.Rda")
load("C:/Users/Edoardo/Dropbox/elementi salvati R/plot_int_var1_pt2.Rda")
load("C:/Users/Edoardo/Dropbox/elementi salvati R/plot_une_var02_pt2.Rda")
load("C:/Users/Edoardo/Dropbox/elementi salvati R/plot_une_var1_pt2.Rda")


ggarrange(plot_inf_var1,plot_inf_var02,plot_une_var1,plot_une_var02,plot_int_var1,plot_int_var02,ncol=2,nrow=3)
``` 

```{r myfig722, echo=F, fig.align='center', fig.cap="TVP-VAR model comprising US inflation rate, unemployment rate and federal funds rate with 8 lags. One-step-ahead forecasts with credible intervals (blue) estimated using the precision sampler strategy and the true time series (black). On the left column, the graphs show forecasts generated by a TVP-VAR model with no shrinkage ($\\Omega_\\beta=1$), while on the right column forecasts are generated by a TVP-VAR model with severe shrinkage ($\\Omega_\\beta=0.2$). The latter presents smaller credible intervals.", fig.height=6, fig.pos='H', fig.width=6, message=FALSE, warning=FALSE, results=F}

load("C:/Users/Edoardo/Dropbox/elementi salvati R/plot_ps_inf02.Rda")
load("C:/Users/Edoardo/Dropbox/elementi salvati R/plot_ps_inf1.Rda")
load("C:/Users/Edoardo/Dropbox/elementi salvati R/plot_ps_une02.Rda")
load("C:/Users/Edoardo/Dropbox/elementi salvati R/plot_ps_une1.Rda")
load("C:/Users/Edoardo/Dropbox/elementi salvati R/plot_ps_int02.Rda")
load("C:/Users/Edoardo/Dropbox/elementi salvati R/plot_ps_int1.Rda")


ggarrange(plot_ps_inf1,plot_ps_inf02,plot_ps_une1,plot_ps_une02,plot_ps_int1,plot_ps_int02,ncol=2,nrow=3)
``` 


\section{Discussion}

With time-varying parameters, the risk of overfitting in VAR models is round the corner. This prevents TVP-VAR model from being used in high dimensions, limiting interesting modelling opportunities for applied research. With the example described in the previous section, we show the potentiality of Dynamic Spike-and-Slab process priors in mitigating this problem. Our aim is to provide an approach to estimate large TVP-VAR and Structural TVP-VAR that can be simultaneously fast and precise. The combination of the precision sampler with the Dynamic SSVS proved to be a valid strategy to achieve both goals. Thanks to dynamic shrinkage priors it is possible to obtain a simpler model and achieve important benefits in terms of interpretability. For what concern forecasting, we shown that by removing noisy variables it is still possible to preserve accuracy while reducing uncertainty. In addition, here we decided to focus on forecasting as a way to validate model performances, we do not exclude to expand this approach on causality and impulse responses assessment features in further researches. Nevertheless, the output of the \code{DSSTVPVAR\_SV} function in our \code{dynamicshrink} package can be easily converted and used to compute impulse response functions using the existing \code{bvarsv} package. Another interesting fact we notice through the previous analysis, which is true in principle, is that $\bb{Y}_t$ is less correlated with $\bb{Y}_{t-h}$ when $h$ increases. In order to emphasize this phenomenon, dynamic shrinkage priors that induce a greater penalization for $h$ large could be developed. The work of @legramanti2020bayesian presents a proposal in this direction for static problems. It would interesting to translate it into a dynamic environment.

  
  \chapter{My dynamicshrink R-package}\label{The dynamicshrink package}
  
The absence in literature of a unified \code{R}-package for dynamic variable selection constitutes an important limit for empirical research. With this thesis, we fill this gap by making available a set of \code{R} functions that implement the estimation strategies described in the previous chapters. Therefore, here we briefly introduce our 
\code{dynamicshrink} package. This sort of experimental package allows firstly to replicate the results obtained in previous sections and, eventually, to perform further analysis on high dimensional time series. The adjective "experimental" used here refers to the fact that, even though the great efforts to make these functions high performing, we acknowledge that there is room for improvements in terms of computational time. The code, which is entirely written in \code{R}, recalls for some specific tasks high-performing functions provided by other popular packages. Reference \code{R} packages are: \code{dlm}, \code{stochvol} and \code{matlib}, \code{MASS}, \code{Rcpp} and the \code{tidyverse}. Further developments in \code{C} will eventually follow. \
Below we present the list of the most useful functions complemented by a brief description of them. In addition, a little helper is provided in Appendix. Further details on these functions (and on the other functions) can be found at the personal Github page of the author, along with the rest of the code. 

\newpage
\begin{table}[h!]
\begin{tabularx}{\textwidth}{|l|X|}
\code{DSSBSTS\_SV}          & Dynamic Spike-and-Slab Bayesian Structural Time Series with log-normal AR(1) stochastic volatility \\
\code{DSSBSTS\_DF}          & Dynamic Spike-and-Slab Bayesian Structural Time Series with discount factor model for time-varying variances \\
\code{MCMC.out} & Compute meaningful statistics from the output of \code{DSSBSTS\_SV} and \code{DSSBSTS\_DF} \\
\code{DEMVS} & Dynamic Expectation-Maximization Variable Selection with discount factor model for time-varying variances \\
\code{DEMVS\_PS} & Dynamic Expectation-Maximization Variable Selection with log-normal AR(1) stochastic volatility estimated via Particle Smoothing (PS)\\
\code{DEMVS.Quarterly} \\ \code{DEMVS.Monthly} & Dynamic Expectation-Maximization Variable Selection for Bayesian Structural Time Series with log-normal AR(1) stochastic volatility estimated via Particle Smoothing (PF). The functions are respectively built for quarterly and monthly data.\\
\code{DSSTVPVAR\_SV} & Dynamic Stochastic Search Variable Selection for Time-Varying Parameter Vector Autoregressive models with Stochastic Volatility.\\
\code{MCMC.TVPVAR.out} & Compute meaningful statistics from the output of \code{DSSTVPVAR\_SV}\\
\end{tabularx}
\end{table}

\newpage
\begin{table}[h!]
\begin{tabularx}{\textwidth}{|l|X|}
\code{sparse.data.sim} & Generates dynamic sparse time series data.\\
\code{varplot} & Generates the plot of the volatility process \\
\code{Indicatorplot} & Generates the plot of the indicator variables \\
\code{Coef.compare.plot} & Generates the plot the regression coefficients. It can be used for comparisons \\
\code{SeasTrendSlope.plot} & Generates three plots concerning the time series seasonality, trend and trend's slope. \\
\code{SeasTrendReg.plot} & Generates three plots concerning the time series regression component, seasonal component and trend component estimated using Dynamic SSVS.\\
\code{SeasTrendReg.plot.1} &  Generates three plots concerning the time series regression component, seasonal component and trend component estimated using Dynamic EMVS.\\
\code{Active.coef.plot} & Generates a plot indicating how many coefficients result active at each period in time. \\
\code{Forecastplot} & Compares forecasts with the actual series. Developed for BSTS models. \\
\code{Forecastplot.TVP.VAR} & Compares forecasts with the actual series. Developed for TVP-VAR models. \\
\code{DSS\_PRECISIONSAMPLER} & Dynamic Stochastic Search Variable Selection for Time-Varying Parameter Vector Autoregressive models with Stochastic Volatility. The FFBS step is replaced by a precision sampler.\\
\code{DSSTVP\_PRECISIONSAMPLER} & Dynamic Stochastic Search Variable Selection for Time-Varying Parameter Vector Autoregressive models with Stochastic Volatility. The FFBS step is replaced by a precision sampler.
\end{tabularx}
\end{table}

\newpage
Here we discuss briefly how the package works presenting the code used for the simulations study on quasi-synthetic data. Let's thus simulate a TVP regression model with 20 predictors of which only the first four are truly relevant using the function \code{sparse.data.sim}. We label data generated through this function as \code{synthetic.y} and then we add to then Airpassengers data in lags and rescaled by ten. We rescaled real data to adapt them to the scale of simulated data, note that results would not be affected if data are not rescaled. The true model's parameter are labeled \code{true\_b} and \code{true\_ind}.

```{r echo=TRUE, eval=FALSE}
sim = sparse.data.sim(TIME=144,P=4,FP=16,
                     phi0=0,phi1=0.98,lambda1=0.1,v=0,seed=100)
synthetic.y = sim$y
X = sim$X
real.y = log(as.numeric(AirPassengers))
y = real.y*10+synthetic.y 
true_b = sim$true_b
true_ind = sim$true_ind
```

The estimation of the models parameters can be carried out using one of the following functions: \code{DSSBSTS\_SV}, \code{DSSBSTS\_DF} or \code{DEMVS.Monthly} whose general functionality is introduced in the tables above. The positive note here is that, even though they implements different strategies their usage is very similar, therefore we focus on the first function mentioned before to provide an overall guidance. The arguments of the function are self-explanatory. The model's hyperparameters are  \code{phi1}, \code{phi0}, \code{lambda1},  \code{lambda0} and \code{OMEGA}, the starting values of the indicators, \code{gamma0}, and the volatility, \code{v0}, and the structural components, \code{S} and \code{U}. Moreover, \code{sig2u}, \code{sig2d}, \code{sig2tau} are the variances of, respectively, the trend, the slope and the seasonal factor. Note that the value of \code{S} is equal to the number of seasonal factors to be estimated i.e. $S-1$ where $S$ is equal for example to 12 in monthly time series. On the other hand, \code{U} may be equal to 2 if our base model is a linear growth model or equal to 1 if it is a local level model. Alternatively to \code{U}=1, one can add a constant to the matrix of predictors and set \code{ACTIVATE=TRUE} which means that the indicator of the first predictor will be always set equal to one. This latter option is useful also when we do not want to shrinkage an important predictor. In the latter case that predictors has to stay in the first column of \code{X}. If present, \code{new.X} contains future values of the predictors, $\bb{x}_{t+1}$, which are used for one-step-ahead forecasting. Finally \code{start\_params} and \code{start\_latent} are two objects defined in the \code{stochvol} package and they refer to the starting parameters and starting latent values of a stochastic volatility model.

```{r echo=TRUE, eval=FALSE}
set.seed(1)
ssvs.bsts = DSSBSTS_SV(y=y,X=X,N=1000,S=11,U=2,
                     phi0=0,phi1=0.98,gamma0=0.5,v0=0.25,
                     sig2u=0.1,sig2d=0.1,sig2tau=0.1,
                     lambda0=0.01,lambda1=0.1,OMEGA=0.2,
                     start_params,start_latent,new.X,activate=F)
```

This function returns a huge list of objects of which the most important are the samples generate by the Markov Chain. These samples are indeed stored in arrays whose interpretation is not immediate, therefore we need to convert them into some meaningful statistics. This is done by the function \code{MCMC.out} which returns the sample means and the sample variances. Here we just plug the name of the function used for model estimation and the number of samples to burn-in. Note that this function is already endowed in many other functions such as plot functions. Moreover, the functions carrying out the precision sampler do not need this passage, but they directly return the mean and the variance of the MCMC estimates.

```{r echo=TRUE, eval=FALSE}
mcmc = MCMC.out(fun=ssvs.bsts,burn=200)
```

Once the model parameters are estimated, there are several useful tool for visualization. For example, \code{Coef.compare.plot} provides the plot of the estimated regression coefficients and, if \code{trueb} is not missing, it compares them to their true values or, if \code{trueb} is equal to coefficients estimated with other model's specifications, it compares the two. \code{column} indicates the column of interest of the regression matrix and if for example \code{column=1} the plot of $\beta_{1:T,1}$ will be generated. Furthermore, when we use the Dynamic EMVS instead of the Dynamic SSVS, then \code{EMVS} should be explicated. If we want dates on the x-axis then \code{time} must be equal to 
a date vector of the same length of \code{y}, otherwise if \code{time} is missing the x-axis will be a vector of natural numbers. Figure \ref{fig:myfig7} is generated from the following command for $i=1,...,6$.

```{r echo=TRUE, eval=FALSE}
Coef.compare.plot(trueb=true_b,column=i,fun=ssvs.bsts,burn=200,time,EMVS)
```

Similarly, \code{Indicatorplot}, \code{varplot} provides plots of the MCMC approximation of the expected values of the indicators and the variance. \code{Active.coef.plot} instead shows how many variables are relevant at any moment in time. If structural time series components are estimated, they can be represented by the two following function which plots respectively the trend, the seasonality and the regression component. The following generates Figure \ref{fig:myfig14}

```{r echo=TRUE, eval=FALSE}
SeasTrendReg.plot(ssvs.bsts,S=11,U=2,burn=200)
```

Forecasts one-step-ahead, two-step-ahead and so on are visualized via the function \code{Forecastplot}, for univariate time series the latter provides a comparison between realizations and point forecasts along with their credible intervals. Below we present an example. The time series \code{y} is plotted from \code{from} to \code{to}, whereas the forecasts are plotted from \code{start\_fc} to \code{to}. This allows to show the path of the dependent variable before forecasting starts.

```{r echo=TRUE, eval=FALSE}
Forecastplot(y,from,to,start_fc,fc.f,fc.Q,ub,lb,timeframe)
```

In addition, the package provides all the metrics relevant for comparisons such as SSE, Hamming Distance, MASE, MAFE, RMSE, LPDS, FP, FN etc.  The case in which the time series does not show evident time series structural component can be estimated by the same functions with missing \code{S} and \code{U} or, according to need, by \code{DSS\_PRECISIONSAMPLER}, \code{DEMVS} and \code{DEMVS\_PS}. In general, as already mentioned, the usage is similar among these functions. \
For the multivariate case, instead, another set of functions is envisaged. The relevant functions here are: \code{DSSTVPVAR\_SV} which estimates the Time-Varying Parameter Structural VAR model of @Primicieri_2005 whit shrinkage, \code{DSSTVP\_PRECISIONSAMPLER\_VAR} and \code{DSSTVP\_PRECISIONSAMPLER\_SVAR} that estimates respectively the Time-Varying Parameter VAR and Structural VAR models with the precision sampler. As we can see from the code below, the overall structure of the command is similar to the one mentioned before. \code{X} is the regression matrix with the variables of the VAR model. If \code{constant=TRUE} then a constant will be added to the model. Now we have two hyperparameters \code{OMEGA} which are respectively $\Omega_\alpha$ and $\Omega_\beta$. In this experimental version of the package the function are built in order to provide just one-step-ahead forecasts, thus \code{step\_ahead=1}, however we do not exclude further developments to extend the number of steps ahead. Finally, together with the starting value of the variances, \code{v0}, the starting value of the covariances, \code{cov0}, should be set. We decided to keep it simple and let all the covariances to have the same starting values.

```{r echo=TRUE, eval=FALSE}
DSSTVPVAR_SV(X,constant,N,lags,
             step_ahead=1,gamma0,phi0,v0,cov0,phi1,
             OMEGA.A,OMEGA.B,lambda0,lambda1,
             start_params,start_latent)
```

Ad-hoc functions are created to deal with the output of \code{DSSTVPVAR\_SV}, these are very similar to the ones mentioned above and they are \code{MCMC.TVP.out} for extracting the meaningful statistics, \code{Forecastplot.TVP.VAR} to plot forecasts and \code{lpds\_multi} which is the multivariate version of the \code{lpds} command for Log-Predicitive Density Sum comparison. Again \code{DSSTVP\_PRECISIONSAMPLER\_VAR} and \code{DSSTVP\_PRECISIONSAMPLER\_SVAR} return directly the mean and the variances of the estimates.


  \backmatter
  \chapter{Bibliography}
  
  <div id="refs"></div>
  
  \chapter{Acknowledgement}
  
 A conclusione di questo elaborato, desidero ringraziare tutte le persone che mi hanno supportato nella stesura di questa tesi e, più in generale, nel mio percorso accademico. 
 
In particolar modo, voglio ringraziare la Professoressa Sonia Petrone che ha saputo guidarmi, con suggerimenti pratici, nelle ricerche e nella stesura dell’elaborato. Il suo lavoro meticoloso di revisione di questa tesi ha contribuito immensamente a migliorarne l'esposizione. 

Ringrazio inoltre il Professor Marco Bonetti che è stato mio mentore presso il BIDSA per avermi trasmesso la sua passione per la statistica e la ricerca.

Ringrazio infinitamente mia madre e mio padre, senza il loro supporto, questo lavoro di tesi non esisterebbe nemmeno.

Infine voglio ringraziare i miei amici storici e quelli conosciuti in Bocconi per tutti i momenti passati insieme. Un riconoscimento speciale va al mio amico e coinquilino Yuri, per essere riuscito a strapparmi una risata anche nei momenti più ardui vissuti durante la magistrale, ed alla mia ragazza Lilia, per essermi stata a fianco in questi mesi.

 

  \appendix
  \chapter{Appendix}
  
  \section{Dynamic Spike-and-Slab with Laplace priors}\label{Appendix A.1}
As already mentioned in the text, using a Laplace spike instead of a Gaussian one does not involves new implementation challenges. Moreover, under Laplace spike, the series $\{\beta_{t}\}$ can be shown to be stationary and identically and independently distributed. On the other hand, the conditional Laplace density does not imply marginal Laplace distribution. However, building an autoregressive path with Laplace marginals is still possible. For example, @rockova_mcalinn_2021 define the so called Laplace autoregressive (LAR) process:
\[ \beta_{t}=\sqrt{\frac{\psi_{t}}{\psi_{t-1}}}\phi_{1}\beta_{t-1}+\xi_{t}, \ \ \ \xi_{t}\sim\mathcal{N}(0,(1-\phi_{1}^{2})\psi_{t})
\]
with $\{\psi_{t}\}_{t=1}^{T}$ being an exponential autoregressive process specified by $\psi_{t}|\kappa_{t-1}\sim\mathcal{G}(1+\kappa_{t-1},\lambda_{1}^{2}/[2(1-\rho)])$ and $\kappa_{t-1}|\psi_{t-1}\sim Poisson (\lambda_{1}^{2}\psi_{t-1}\rho/[2(1-\rho)])$ with marginal distribution $Exp(\lambda_{1}^{2}/2)$. The Laplace process implies Laplace marginals $\beta_{t}\sim \psi^{ST}_{1}(\beta_{t}|\lambda_{1})\equiv Laplace(\lambda_{1})$.  \


\section{More details about the dynamicshrink package}


\hrule
\vspace{1em}
\code{DSSBSTS\_SV} Dynamic Spike-and-Slab Bayesian Structural Time Series with log-normal AR(1) stochastic volatility
\vspace{1em}
\hrule
\vspace{1em}
\textbf{Usage}\
\code{DSSBSTS\_SV(y,X,N,S,U,gamma0,phi0,v0,phi1,OMEGA,lambda0,lambda1,\\ sig2u=0.1,sig2d=0.1,sig2tau=0.1,start\_params, start\_latent,new.X)}\
\textbf{Arguments}
\begin{small}
\begin{longtable}{ c l }
\code{y} &  the response variable. It can be a vector or a $T\times1$ matrix object. \\
\code{X} &  $T \times p$ matrix of explanatory variables.\\
\code{N} &  number of MCMC iterations. \\
\code{S} & number of seasonal factors. \\
 & Note if data is quarterly $S=3$, whereas if data is annually $S=11$. \\
 & If $S$ is missing then no seasonal component\\
 & is added to the the regression ($S=0$). \\
\code{U} &  the trend component. \\
 & If $U=2$ then a linear growth model is implemented.\\
 & If $U=1$ then the $\delta_{t}$ component is missing. \\
 & If $U$ is missing, no trend is considered.\\
\code{gamma0} &  initial matrix of indicators $\bb{\gamma}^{(0)}_{0:T}$. Recommended choice $\bb{\gamma}_{0:T}=0.5$.  \\  
\code{phi0} & $\phi_{0}$ in equation (\ref{eq:dssbsts1})\\
\code{phi1} &  $\phi_1$ in equation (\ref{eq:dssbsts1})\\
\code{v0} &  initial vector of volatilities $v^{(0)}_{0:T}$. \\
 & It is recommended to assign an initial small value for them. \\
\code{OMEGA} &  $\Omega$ in equation (\ref{eq:dssbsts1})\\
\code{lambda0} &  $\lambda_0$ in equation (\ref{eq:dssbsts1})\\
\code{lambda1} &  $\lambda_1$ in equation (\ref{eq:dssbsts1})\\
\code{sig2u} & $\sigma^{2}_{\mu}$ in equation (\ref{eq:dssbsts1})\\
\code{sig2d} & $\sigma^{2}_{\delta}$ in equation (\ref{eq:dssbsts1})\\
\code{sig2tau} & $\sigma^{2}_{\tau}$ in equation (\ref{eq:dssbsts1})\\
\code{start\_params} &  list of starting values for $\alpha_0$, $\alpha_1$, $\sigma_\zeta$ in equation (\ref{eq:dssbsts2}) and $h_0$.\\
\code{start\_latent} &  vector of starting values for the latent log-Normal volatility process $\bb{h}^{(0)}_{0:T}$.\\
\code{new.X} &  vector of explanatory variables at time $t+1$ used for Kalman forecasting.
\end{longtable}
\end{small}
\textbf{Value}
\begin{small}
\begin{longtable}{ c l }
\code{theta} & MCMC draws from the posterior distribution of the latent process $\bb{\theta}_{0:T}$  \\
\code{gamma} &  MCMC draws from the posterior distribution of the auxiliary variable $\bb{\gamma}_{0:T}$\\
\code{v} &  MCMC draws from the posterior distribution of the volatility process $\sigma^{2}_{\epsilon,0:T}$ \\
\code{fc.f} & mean of the one-step-ahead posterior predictive distribution.\\
\code{fc.Q} & variance of the one-step-ahead posterior predictive distribution.\\
\code{...} & 
\end{longtable}
\end{small}
\hrule
\vspace{1em}
\code{DSSBSTS\_DF} Dynamic Spike-and-Slab Bayesian Structural Time Series with discount factor model for time-varying variances
\vspace{1em}
\hrule
\vspace{1em}
\textbf{Usage}\
\code{DSSBSTS\_DF(y,X,N,S,U,n0,d0,v0,gamma0,phi0,phi1,OMEGA,lambda0,lambda1,\\ delta,sig2u=0.1,sig2d=0.1,sig2tau=0.1,new.X)}\
\textbf{Arguments}\
\begin{small}
\begin{longtable}{ c l }
\code{y} &  the response variable. It can be a vector or a $T\times1$ matrix object. \\
\code{X} &  $T \times p$ matrix of explanatory variables.\\
\code{N} &  number of MCMC iterations. \\
\code{S} &  number of seasonal factors. \\
 & Note if data is quarterly $S=3$, whereas if data is annually $S=11$. \\
 & If $S$ is missing then no seasonal component\\
 & is added to the the regression ($S=0$). \\
\code{U} &  the trend component. \\
 & If $U=2$ then a linear growth model is implemented.\\
 & If $U=1$ then the $\delta_{t}$ component is missing. \\
 & If $U$ is missing, no trend is considered.\\
\code{n0,d0} & hyperparameters of equation (\ref{eq:factmod}) at time $t=0$.\\
\code{v0} &  initial vector of volatilities $v^{(0)}_{0:T}$. \\
 & It is recommended to assign an initial small value for them. \\
\code{gamma0} &  initial matrix of indicators $\bb{\gamma}^{(0)}_{0:T}$. Recommended choice $\bb{\gamma}_{0:T}=0.5$.  \\  
\code{phi0} & $\phi_{0}$ in equation (\ref{eq:dssbsts1})\\
\code{phi1} &  $\phi_1$ in equation (\ref{eq:dssbsts1})\\
\code{OMEGA} &  $\Omega$ in equation (\ref{eq:dssbsts1})\\
\code{lambda0} &  $\lambda_0$ in equation (\ref{eq:dssbsts1})\\
\code{lambda1} &  $\lambda_1$ in equation (\ref{eq:dssbsts1})\\
\code{delta} & the discount factor of the variance.\\
\code{sig2u} & $\sigma^{2}_{\mu}$ in equation (\ref{eq:dssbsts1})\\
\code{sig2d} & $\sigma^{2}_{\delta}$ in equation (\ref{eq:dssbsts1})\\
\code{sig2tau} & $\sigma^{2}_{\tau}$ in equation (\ref{eq:dssbsts1})\\
\code{new.X} &  vector of explanatory variables at time $t+1$ used for Kalman forecasting.
\end{longtable}
\end{small}  
\textbf{Value}
\begin{small}
\begin{longtable}{ c l }
\code{theta} & MCMC draws from the posterior distribution of the latent process $\bb{\theta}_{0:T}$  \\
\code{gamma} &  MCMC draws from the posterior distribution of the auxiliary variables $\bb{\gamma}_{0:T}$\\
\code{v} &  MCMC draws from the posterior distribution of the volatility process $\sigma^{2}_{\epsilon,0:T}$ \\
\code{fc.f} & mean of the one-step-ahead posterior predictive distribution.\\
\code{fc.Q} & variance of the one-step-ahead posterior predictive distribution.\\
\code{...} & 
\end{longtable}
\end{small}
\hrule
\vspace{1em}
\code{MCMC.out} Compute meaningful statistics from the output of \code{DSSBSTS\_SV} and \code{DSSBSTS\_DF}
\vspace{1em}
\hrule
\vspace{1em}
\textbf{Usage}\
\code{MCMC.out(fun,burn)}\
\textbf{Arguments}
\begin{small}
\begin{longtable}{ c l }
\code{fun} &  an objects of the type \code{DSSBSTS\_SV} or \code{DSSBSTS\_DF}. \\
\code{burn} &  size of burnin sample.
\end{longtable}
\end{small}
\textbf{Value}
\begin{small}
\begin{longtable}{ c l }
\code{mean} & mean of the posterior distribution of the latent process.  \\
\code{var} &  var of the posterior distribution of the latent process.\\
\code{ind} &  mean of the posterior distribution of the auxiliary variables.\\
\code{v} & mean of the posterior distribution of the volatility process.\\
\code{var.v} & var of the posterior distribution of the volatility process.\\
\code{fit.reg} & mean of the time series regression component.\\
\code{var.fit.reg} & var of the time series regression component.
\end{longtable}
\end{small}
\hrule
\vspace{0.5em}
\code{DEMVS} Dynamic Expectation-Maximization Variable Selection with discount factor model for time-varying variances
\vspace{1em}
\hrule
\vspace{1em}
\textbf{Usage}\
\code{DEMVS(y,X,N,n0,d0,phi0,phi1,OMEGA,lambda0,lambda1,delta,new.X)}\
\textbf{Arguments}
\begin{small}
\begin{longtable}{ c l }
\code{y} &  the response variable. It can be a vector or a $T\times1$ matrix object. \\
\code{X} &  $T \times p$ matrix of explanatory variables.\\
\code{N} &  number of iterations of the EM algorithm. \\
\code{n0,d0} & hyperparameters of equation (\ref{eq:factmod}) at time $t=0$.\\
\code{phi0} & $\phi_{0}$ in equation (\ref{eq:dssbsts1})\\
\code{phi1} &  $\phi_1$ in equation (\ref{eq:dssbsts1})\\
\code{OMEGA} &  $\Omega$ in equation (\ref{eq:dssbsts1})\\
\code{lambda0} &  $\lambda_0$ in equation (\ref{eq:dssbsts1})\\
\code{lambda1} &  $\lambda_1$ in equation (\ref{eq:dssbsts1})\\
\code{delta} & the discount factor of the variance.\\
\code{new.X} &  vector of explanatory variables at time $t+1$ used for forecasting.
\end{longtable}
\end{small}
\textbf{Value}
\begin{small}
\begin{longtable}{ c l }
\code{beta} & Maximum a Posteriori estimates of the latent state process. \\
\code{gamma} &  conditional inclusion probabilities.\\
\code{v} &  residual variances.\\
\code{fc.f} & one-step-ahead forecast.
\end{longtable}
\end{small}
\hrule
\vspace{0.5em}
\code{DEMVS\_PS} Dynamic Expectation-Maximization Variable Selection with log-normal AR(1) stochastic volatility estimated via Particle Smoothing (PS)
\vspace{1em}
\hrule
\vspace{1em}
\textbf{Usage}\
\code{DEMVS\_PS(y,X,N,phi0,phi1,OMEGA,lambda0,lambda1,new.X)}\
\textbf{Arguments}
\begin{small}
\begin{longtable}{ c l }
\code{y} &  the response variable. It can be a vector or a $T\times1$ matrix object. \\
\code{X} &  $T \times p$ matrix of explanatory variables.\\
\code{N} &  number of iterations of the EM algorithm. \\
\code{n0,d0} & hyperparameters of equation (\ref{eq:factmod}) at time $t=0$.\\
\code{phi0} & $\phi_{0}$ in equation (\ref{eq:dssbsts1})\\
\code{phi1} &  $\phi_1$ in equation (\ref{eq:dssbsts1})\\
\code{OMEGA} &  $\Omega$ in equation (\ref{eq:dssbsts1})\\
\code{lambda0} &  $\lambda_0$ in equation (\ref{eq:dssbsts1})\\
\code{lambda1} &  $\lambda_1$ in equation (\ref{eq:dssbsts1})\\
\code{new.X} &  vector of explanatory variables at time $t+1$ used for forecasting.
\end{longtable}
\end{small}
\textbf{Value}
\begin{small}
\begin{longtable}{ c l }
\code{beta} & Maximum a Posteriori estimates of the latent state process. \\
\code{gamma} &  conditional inclusion probabilities.\\
\code{v} &  residual variances.\\
\code{fc.f} & one-step-ahead forecast.
\end{longtable}
\end{small}
\hrule
\vspace{0.5em}
\code{DEMVS.Quarterly} & \code{DEMVS.Monthly} Dynamic Expectation-Maximization Variable Selection for Bayesian Structural Time Series with log-normal AR(1) stochastic volatility estimated via Particle Smoothing (PF). The functions are respectively built for quarterly and monthly data.
\vspace{1em}
\hrule
\vspace{1em}
\textbf{Usage}\
\code{DEMVS.Monthly(y,X,N,phi0,phi1,OMEGA,lambda0,lambda1,  \\                sig2u=0.1,sig2d=0.1,sig2tau=0.1,new.X)}\
\textbf{Arguments}
\begin{small}
\begin{longtable}{ c l }
\code{y} &  the response variable. It can be a vector or a $T\times1$ matrix object. \\
\code{X} &  $T \times p$ matrix of explanatory variables.\\
\code{N} &  number of iterations of the EM algorithm. \\
\code{phi0} & $\phi_{0}$ in equation (\ref{eq:dssbsts1})\\
\code{phi1} &  $\phi_1$ in equation (\ref{eq:dssbsts1})\\
\code{OMEGA} &  $\Omega$ in equation (\ref{eq:dssbsts1})\\
\code{lambda0} &  $\lambda_0$ in equation (\ref{eq:dssbsts1})\\
\code{lambda1} &  $\lambda_1$ in equation (\ref{eq:dssbsts1})\\
\code{sig2u} & $\sigma^{2}_{\mu}$ in equation (\ref{eq:dssbsts1})\\
\code{sig2d} & $\sigma^{2}_{\delta}$ in equation (\ref{eq:dssbsts1})\\
\code{sig2tau} & $\sigma^{2}_{\tau}$ in equation (\ref{eq:dssbsts1})\\
\code{new.X} &  vector of explanatory variables at time $t+1$ used for forecasting.
\end{longtable}
\end{small}
\textbf{Value}
\begin{small}
\begin{longtable}{ c l }
\code{beta} & Maximum a Posteriori estimates of the regression coefficients. \\
\code{u} & Maximum a Posteriori estimates of the stochastic trend. \\
\code{d} & Maximum a Posteriori estimates of the stochastic trend's slope. \\
\code{tau} & Maximum a Posteriori estimates of the seasonality. \\
\code{gamma} &  conditional inclusion probabilities.\\
\code{v} &  residual variances.\\
\code{fc.f} & one-step-ahead forecast.\\
\code{...} &
\end{longtable}
\end{small}
\hrule
\vspace{0.5em}
\code{DSSTVPVAR\_SV} Dynamic Stochastic Search Variable Selection for Time-Varying Parameter Vector Autoregressive models with Stochastic Volatility.
\vspace{1em}
\hrule
\vspace{1em}
\textbf{Usage}\
\code{DSSTVPVAR\_SV(X,constant,N,lags,step\_ahead,gamma0,phi0,\\
v0,cov0,phi1,OMEGA.A,OMEGA.B,lambda0,lambda1,start\_params,start\_latent)}\
\textbf{Arguments}
\begin{small}
\begin{longtable}{ c l }
\code{X} &  $T \times n$ matrix of regressors.\\
\code{constant} & if TRUE then a constant is added to the model.\\
\code{N} &  number of iterations of the MCMC. \\
\code{lags} & number of lags.\\
\code{step\_ahead} & number of step ahead for forecasting.\\
\code{gamma0} &  initial matrix of indicators $\bb{\gamma}^{(0)}_{0:T}$. Recommended choice $\bb{\gamma}_{0:T}=0.5$\\
\code{phi0} & $\phi_{0}$ in equation (\ref{eq:dssbsts1})\\
\code{phi1} &  $\phi_1$ in equation (\ref{eq:dssbsts1})\\
\code{v0} & diagonal elements of the starting covariance matrix.\\
\code{cov0} & non-diagonal elements of the starting covariance matrix.\\
\code{OMEGA} &  $\Omega$ in equation (\ref{eq:dssbsts1})\\
\code{lambda0} &  $\lambda_0$ in equation (\ref{eq:dssbsts1})\\
\code{lambda1} &  $\lambda_1$ in equation (\ref{eq:dssbsts1})\\
\code{start\_params} &  list of starting values for $\alpha_0$, $\alpha_1$, $\sigma_\zeta$ in equation (\ref{eq:dssbsts2}) and $h_0$.\\
\code{start\_latent} &  vector of starting values for the latent log-Normal volatility process $\bb{h}^{(0)}_{0:T}$.
\end{longtable}
\end{small}
\textbf{Value}
\begin{small}
\begin{longtable}{ c l }
\code{beta} & MCMC draws from the posterior distribution of $\bb{\beta}_{0:T}$. \\
\code{alpha} & MCMC draws from the posterior distribution of  $\bb{\alpha}_{0:T}$. \\
\code{gamma\_beta} & MCMC draws from the posterior distribution of $\bb{\gamma}_{0:T}$ related to $\bb{\beta}_{0:T}$. \\
\code{gamma\_alpha} & MCMC draws from the posterior distribution of $\bb{\gamma}_{0:T}$ related to $\bb{\alpha}_{0:T}$. \\
\code{v} &  MCMC draws from the posterior distribution of the volatility process.\\
\code{fc.m} & one-step-ahead forecast mean generated at each MCMC iteration.\\
\code{fc.v} & one-step-ahead forecast variance generated at each MCMC iteration.\\
\code{fc.y} & MCMC draws from the one-step-ahead forecast density.\\
\code{...} &
\end{longtable}
\end{small}
\hrule
\vspace{1em}
\code{DSS\_PRECISIONSAMPLER} Dynamic SSVS for Time-Varying Parameter regression models with stochastic volatility. Posterior sampling is performed using a precision sampler
\vspace{1em}
\hrule
\vspace{1em}
\textbf{Usage}\
\code{DSSBSTS\_DF(y,X,N,burn,OMEGA,lambda1,lambda0,\\
phi1,phi0,gamma0,v0,start\_params,start\_latent,new.X,activate=F)}\
\textbf{Arguments}\
\begin{small}
\begin{longtable}{ c l }
\code{y} &  the response variable. It can be a vector or a $T\times1$ matrix object. \\
\code{X} &  $T \times p$ matrix of explanatory variables.\\
\code{N} &  number of MCMC iterations. \\
\code{v0} &  initial vector of volatilities $v^{(0)}_{0:T}$. \\
 & It is recommended to assign an initial small value for them. \\
\code{gamma0} &  initial matrix of indicators $\bb{\gamma}^{(0)}_{0:T}$. Recommended choice $\bb{\gamma}_{0:T}=0.5$.  \\  
\code{phi0} & $\phi_{0}$ in equation (\ref{eq:dssbsts1})\\
\code{phi1} &  $\phi_1$ in equation (\ref{eq:dssbsts1})\\
\code{OMEGA} &  $\Omega$ in equation (\ref{eq:dssbsts1})\\
\code{lambda0} &  $\lambda_0$ in equation (\ref{eq:dssbsts1})\\
\code{lambda1} &  $\lambda_1$ in equation (\ref{eq:dssbsts1})\\
\code{start\_params} &  list of starting values for $\alpha_0$, $\alpha_1$, $\sigma_\zeta$ in equation (\ref{eq:dssbsts2}) and $h_0$.\\
\code{start\_latent} &  vector of starting values for the latent log-Normal volatility process $\bb{h}^{(0)}_{0:T}$.\\
\code{new.X} &  vector of explanatory variables at time $t+1$ used for Kalman forecasting.\\
\code{activate} & If \code{TRUE} no shrinkage applies to the first variable of the regression matrix. Useful for example if the latter is constant term.
\end{longtable}
\end{small}  
\textbf{Value}
\begin{small}
\begin{longtable}{ c l }
\code{beta} & Expected value of the posterior distribution of the latent process $\bb{\beta}_{1:T}$\\
\code{beta0} & Expected value of the posterior distribution of the latent process $\bb{\beta}_{0}$\\
\code{ind} & Expected value of the posterior distribution of the auxiliary variables $\bb{\gamma}_{1:T}$ \\
\code{ind0} & Expected value of the posterior distribution of the auxiliary variables $\bb{\gamma}_{0}$ \\
\code{h} & Median of the posterior distribution of the volatility process $h_{0:T}$ \\
\code{beta.sample} & MCMC draws from the posterior distribution of the latent process $\bb{\beta}_{1:T}$  \\
\code{gamma.sample} &  MCMC draws from the posterior distribution of the auxiliary variables $\bb{\gamma}_{1:T}$\\
\code{beta0.sample} & MCMC draws from the posterior distribution of the latent process $\bb{\beta}_{0}$  \\
\code{gamma0.sample} &  MCMC draws from the posterior distribution of the auxiliary variables $\bb{\gamma}_{0}$\\
\code{h.sample} &  MCMC draws from the posterior distribution of the volatility process $h_{0:T}$ \\
\code{fc.f} & mean of the one-step-ahead posterior predictive distribution.\\
\code{fc.Q} & variance of the one-step-ahead posterior predictive distribution.\\
\code{...} & 
\end{longtable}
\end{small}




\newpage
\section{Inflation forecasting: list of predictors}

```{r mytab102, echo=FALSE, fig.align="center", message=FALSE, warning=FALSE}
load("C:/Users/Edoardo/Dropbox/elementi salvati R/WORD_MAT.Rda")

kbl(WORD.mat, align="c", longtable = F, booktabs = T, caption="List of Predictors",digits=3,escape=F)%>%
kable_styling(latex_options = c("HOLD_position"),font_size = 8)

```

For details on the labels please see @FREDQD

\section{Inflation forecasting: plots of some regression coefficients }

```{r , fig.cap = "Dynamic SSVS for BSTS model; smoothed estimates (yellow) with 95 percent credible intervals of some interesting regression coefficients.", fig.align='center', fig.height=6, fig.width=8, echo=F, fig.pos='H', results=F}
for(i in 1:40){
  load(paste0("C:/Users/Edoardo/Dropbox/elementi salvati R/plotappendix001_",i,".Rda"))
}
ggarrange(plotappendix001_3,plotappendix001_4,plotappendix001_8,
          plotappendix001_10,plotappendix001_14,plotappendix001_15,
          plotappendix001_22,plotappendix001_25,
    plotappendix001_26)
``` 


\newpage
\section{\small Unemployment rate nowcasting: list of predictors}

```{r mytab101, echo=FALSE, fig.align="center", message=FALSE, warning=FALSE}
load("C:/Users/Edoardo/Dropbox/elementi salvati R/List_predictors.Rda")

source.col=c(rep("Yahoo",1),rep("FRED",13),rep("Google Trends",14))
type.col=c(rep("Contemporary", 4), rep("One period lag", 10),rep("Contemporary", 14))
List.Predictors1 = cbind(source.col,type.col,List.Predictors)

kbl(List.Predictors1, align="c", longtable = F, booktabs = T, caption="List of Predictors",digits=3,escape=F,col.names = c("Source","Type","Label"))%>%
kable_styling(latex_options = c("HOLD_position"),font_size = 8)

```

\newpage
\section{BSTS model for Airpassengers data}

Here, a BSTS model with DSS priors is used to fit AirPassengers data. The coefficients are correctly shrunk to zero and the model succeed in capturing trend and seasonality.

```{r , fig.cap = "Structural time series components. Point estimates (black) with credible intervals (gray).", fig.align='center', fig.height=2, fig.width=8, echo=F, results=F,fig.pos='H'}
load("C:/Users/Edoardo/Dropbox/elementi salvati R/plot_airpstr.Rda")
plot_airpstr
``` 

```{r , fig.cap = "Dynamic SSVS for BSTS model; true values of $\\beta_{1:T, j}$, $j=1, \\ldots, 6$, (black) and smoothed estimates with 95 percent credible intervals (yellow) of the first six regression coefficients. The coefficients are correctly shrunk to zero at every time.", fig.align='center', fig.height=3, fig.width=8, echo=F, fig.pos='H', results=F}
for(i in 1:20){
  load(paste0("C:/Users/Edoardo/Dropbox/elementi salvati R/plotairp_",i,".Rda"))
}
ggarrange(plotairp_17,plotairp_12,plotairp_13,plotairp_15,plotairp_5,plotairp_6)
```  

```{r , fig.cap = "Left panel: Airpassengers time series. Middle: fitted values with 95 credible intervals. Right panel: Airpassengers time series (black) and fitted values (red) with 95 credible intervals.", fig.align='center', fig.height=2, fig.width=8, echo=F, fig.pos='H', results=F}
load("C:/Users/Edoardo/Dropbox/elementi salvati R/plotst501.Rda")
load("C:/Users/Edoardo/Dropbox/elementi salvati R/plotst502.Rda")
load("C:/Users/Edoardo/Dropbox/elementi salvati R/plotst503.Rda")
ggarrange(plotst501,plotst502,plotst503,nrow=1)
``` 


```{r myfig104, eval=FALSE, fig.align='center', fig.cap="Dynamic SSVS with $\\Omega=0.2$ and stochastic volatility model for the residuals; true values of $\\beta_{1:T, j}$, fig.height=3.5, fig.width=7.5, $j=1, \\ldots, 6$, (black) and smoothing estimates with 95 percent credible intervals (yellow) of the first six regression coefficients. Here FFBS is replaced by a precision sampler.",fig.pos='H', include=FALSE, results=F}
load("C:/Users/Edoardo/Dropbox/elementi salvati R/plot_chan_1.Rda")
load("C:/Users/Edoardo/Dropbox/elementi salvati R/plot_chan_2.Rda")
load("C:/Users/Edoardo/Dropbox/elementi salvati R/plot_chan_3.Rda")
load("C:/Users/Edoardo/Dropbox/elementi salvati R/plot_chan_4.Rda")
load("C:/Users/Edoardo/Dropbox/elementi salvati R/plot_chan_5.Rda")
load("C:/Users/Edoardo/Dropbox/elementi salvati R/plot_chan_6.Rda")
ggarrange(plot_chan_1,plot_chan_2,plot_chan_3,plot_chan_4,plot_chan_5,plot_chan_6)
# The latter is performed by function \code{svsample\_fast\_cpp} included in the # \code{stochvol} package. 

```